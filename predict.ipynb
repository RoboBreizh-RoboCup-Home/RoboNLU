{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from utils import init_logger, load_tokenizer, get_intent_labels, get_slot_labels, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "# from utils import init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP, get_intent_labels, get_slot_labels\n",
    "\n",
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(pred_config):\n",
    "    return \"cuda\" if torch.cuda.is_available() and not pred_config.no_cuda else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(pred_config):\n",
    "    return torch.load(os.path.join(pred_config.model_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pred_config, args, device):\n",
    "    # Check whether model exists\n",
    "    if not os.path.exists(pred_config.model_dir):\n",
    "        raise Exception(\"Model doesn't exists! Train first!\")\n",
    "    try:\n",
    "        # model = torch.load(os.path.join(args.model_dir, 'pytorch_model.bin'))\n",
    "\n",
    "        model = MODEL_CLASSES[args.model_type][1].from_pretrained(args.model_dir,\n",
    "                                                                  args=args,\n",
    "                                                                  intent_label_lst=get_intent_labels(args),\n",
    "                                                                  slot_label_lst=get_slot_labels(args))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(\"***** Model Loaded *****\")\n",
    "    except:\n",
    "        raise Exception(\"Some model files might be missing...\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_file(pred_config):\n",
    "    lines = []\n",
    "    with open(pred_config.input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            words = line.split()\n",
    "            lines.append(words)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input_file_to_tensor_dataset(lines,\n",
    "                                         pred_config,\n",
    "                                         args,\n",
    "                                         tokenizer,\n",
    "                                         pad_token_label_id,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "    # Setting based on the current model type\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    all_slot_label_mask = []\n",
    "    for words in lines:\n",
    "        tokens = []\n",
    "        slot_label_mask = []\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_label_mask.extend([pad_token_label_id + 1] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        print(tokens)\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > args.max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (args.max_seq_len - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(args.max_seq_len - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [pad_token_label_id]\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = args.max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "        all_slot_label_mask.append(slot_label_mask)\n",
    "\n",
    "        # print('padding_length: \\n',padding_length,'\\n')\n",
    "        # print('input_ids: \\n',input_ids,'\\n')\n",
    "        # print('slot_label_mask: \\n',slot_label_mask,'\\n')\n",
    "        # print(f'attention_mask: \\n{attention_mask}\\n')\n",
    "\n",
    "    # Change to Tensor\n",
    "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "    all_slot_label_mask = torch.tensor(all_slot_label_mask, dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_slot_label_mask)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def predict(pred_config, dataset):\n",
    "    # load model and args\n",
    "    args = get_args(pred_config)\n",
    "    device = get_device(pred_config)\n",
    "    model = load_model(pred_config, args, device)\n",
    "    logger.info(args)\n",
    "    intent_label_lst = get_intent_labels(args)\n",
    "    slot_label_lst = get_slot_labels(args)\n",
    "\n",
    "    # Convert input file to TensorDataset\n",
    "    pad_token_label_id = args.ignore_index\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    lines = read_input_file(pred_config)\n",
    "    #dataset = convert_input_file_to_tensor_dataset(lines, pred_config, args, tokenizer, pad_token_label_id)\n",
    "\n",
    "    # Predict\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    data_loader = DataLoader(dataset, sampler=sampler, batch_size=pred_config.batch_size)\n",
    "    all_slot_label_mask = None\n",
    "\n",
    "    intent_preds = None\n",
    "    slot_preds = None\n",
    "    intent_token_preds = None\n",
    "    out_intent_label_ids = None\n",
    "    out_slot_labels_ids = None\n",
    "    out_intent_token_ids = None\n",
    "\n",
    "    tag_intent_preds = None\n",
    "    out_tag_intent_ids = None\n",
    "\n",
    "    referee_preds = None #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    out_referee_labels_ids = None #!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    all_referee_preds = None #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    all_out_referee_labels_ids = None #!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'intent_label_ids': batch[3],\n",
    "                      'slot_labels_ids': batch[4],\n",
    "                      'intent_token_ids': batch[5],\n",
    "                      'B_tag_mask': batch[6],\n",
    "                      'BI_tag_mask': batch[7],\n",
    "                      'tag_intent_label': batch[8],\n",
    "                       'referee_labels_ids' : batch[9],\n",
    "                       'pro_labels_ids' : batch[10]}\n",
    "\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            if args.pro and args.intent_seq and args.tag_intent: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits, referee_token_logits,all_referee_token_logits) = outputs[:2] #!!!!!!!!!!!!!\n",
    "            elif args.intent_seq and args.tag_intent:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits) = outputs[:2]\n",
    "            elif args.intent_seq:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits) = outputs[:2]\n",
    "            elif args.tag_intent:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits, tag_intent_logits) = outputs[:2]\n",
    "            else:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "        # ============================ Intent prediction =============================\n",
    "        if intent_preds is None:\n",
    "            intent_preds = intent_logits.detach().cpu().numpy()\n",
    "            out_intent_label_ids = inputs['intent_label_ids'].detach().cpu().numpy()\n",
    "        else:\n",
    "            intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
    "            out_intent_label_ids = np.append(\n",
    "                out_intent_label_ids, inputs['intent_label_ids'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        # ============================= Slot prediction ==============================\n",
    "        if slot_preds is None:\n",
    "            if args.use_crf:\n",
    "                # decode() in `torchcrf` returns list with best index directly\n",
    "                slot_preds = np.array(model.crf.decode(slot_logits))\n",
    "            else:\n",
    "                slot_preds = slot_logits.detach().cpu().numpy()\n",
    "\n",
    "            out_slot_labels_ids = inputs[\"slot_labels_ids\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            if args.use_crf:\n",
    "                slot_preds = np.append(slot_preds, np.array(model.crf.decode(slot_logits)), axis=0)\n",
    "            else:\n",
    "                slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            out_slot_labels_ids = np.append(out_slot_labels_ids, inputs[\"slot_labels_ids\"].detach().cpu().numpy(),\n",
    "                                            axis=0)\n",
    "        # print('slot_preds shape:     ',slot_preds.shape)\n",
    "        # ============================= Pronoun referee prediction ==============================\n",
    "        if args.pro:\n",
    "            if all_referee_preds is None:\n",
    "                all_referee_preds = all_referee_token_logits.detach().cpu().numpy()\n",
    "                referee_preds = referee_token_logits.detach().cpu().numpy()\n",
    "\n",
    "                pro_sample_mask_np = (torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] ==1 ).detach().cpu().numpy()\n",
    "\n",
    "                all_out_referee_labels_ids = inputs[\"referee_labels_ids\"].detach().cpu().numpy()\n",
    "                # out_referee_labels_ids = np.array([ele for i,ele in enumerate(all_out_referee_labels_ids) if pro_sample_mask_np[i] != False])\n",
    "                out_referee_labels_ids = all_out_referee_labels_ids[pro_sample_mask_np]\n",
    "\n",
    "\n",
    "            else:\n",
    "                all_referee_preds = np.append(all_referee_preds,all_referee_token_logits.detach().cpu().numpy(), axis = 0)\n",
    "                referee_preds = np.append(referee_preds, referee_token_logits.detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "                pro_sample_mask_np = (torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] == 1).detach().cpu().numpy()\n",
    "                new_all_out_referee_labels_ids = inputs[\"referee_labels_ids\"].detach().cpu().numpy()\n",
    "                all_out_referee_labels_ids = np.append(all_out_referee_labels_ids,new_all_out_referee_labels_ids,axis = 0)\n",
    "                # small_new_out_referee_labels_ids = np.array([ele for i,ele in enumerate(new_all_out_referee_labels_ids) if pro_sample_mask_np[i] != False])\n",
    "                small_new_out_referee_labels_ids = new_all_out_referee_labels_ids[pro_sample_mask_np]\n",
    "                out_referee_labels_ids = np.append(out_referee_labels_ids, small_new_out_referee_labels_ids, axis = 0)\n",
    "\n",
    "        # print('all_referee_token_logits shape: ',all_referee_token_logits.shape)\n",
    "        # print('all_referee_preds shape: ',all_referee_preds.shape)\n",
    "        # ============================== Intent Token Seq =============================\n",
    "        if args.intent_seq:\n",
    "            if intent_token_preds is None:\n",
    "                if args.use_crf:\n",
    "                    intent_token_preds = np.array(model.crf.decode(intent_token_logits))\n",
    "                else:\n",
    "                    intent_token_preds = intent_token_logits.detach().cpu().numpy()\n",
    "\n",
    "                out_intent_token_ids = inputs[\"intent_token_ids\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                if args.use_crf:\n",
    "                    intent_token_preds = np.append(intent_token_preds,\n",
    "                                                   np.array(model.crf.decode(intent_token_logits)), axis=0)\n",
    "                else:\n",
    "                    intent_token_preds = np.append(intent_token_preds, intent_token_logits.detach().cpu().numpy(),\n",
    "                                                   axis=0)\n",
    "\n",
    "                out_intent_token_ids = np.append(out_intent_token_ids,\n",
    "                                                 inputs[\"intent_token_ids\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        # print('intent_token_preds shape: ',intent_token_preds.shape)\n",
    "\n",
    "    # Intent result\n",
    "    # (batch_size, )\n",
    "    # intent_preds = np.argmax(intent_preds, axis=1)\n",
    "    # (batch_size, num_intents)\n",
    "    # we set the threshold to 0.5\n",
    "    intent_preds = torch.as_tensor(intent_preds > 0.5, dtype=torch.int32)\n",
    "\n",
    "    # Slot result\n",
    "    # (batch_size, seq_len)\n",
    "    if not args.use_crf:\n",
    "        slot_preds = np.argmax(slot_preds, axis=2)\n",
    "    slot_label_map = {i: label for i, label in enumerate(slot_label_lst)}\n",
    "    out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "    slot_preds_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "\n",
    "    B_tag_mask_pred = []\n",
    "    BI_tag_mask_pred = []\n",
    "\n",
    "    # generate mask\n",
    "    for i in range(out_slot_labels_ids.shape[0]):\n",
    "        # record the padding position\n",
    "        pos_offset = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "        pos_cnt = 0\n",
    "        padding_recording = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "\n",
    "        for j in range(out_slot_labels_ids.shape[1]):\n",
    "            if out_slot_labels_ids[i, j] != pad_token_label_id:\n",
    "                out_slot_label_list[i].append(slot_label_map[out_slot_labels_ids[i][j]])\n",
    "                slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
    "                pos_offset[pos_cnt + 1] = pos_offset[pos_cnt]\n",
    "                pos_cnt += 1\n",
    "            else:\n",
    "                pos_offset[pos_cnt] = pos_offset[pos_cnt] + 1\n",
    "                padding_recording[j] = 1\n",
    "\n",
    "        entities = get_entities(slot_preds_list[i])\n",
    "        entities = [tag for entity_idx, tag in enumerate(entities) if slot_preds_list[i][tag[1]].startswith('B')]\n",
    "\n",
    "        if len(entities) > args.num_mask:\n",
    "            entities = entities[:args.num_mask]\n",
    "\n",
    "        entity_masks = []\n",
    "\n",
    "        for entity_idx, entity in enumerate(entities):\n",
    "            entity_mask = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "            start_idx = entity[1] + pos_offset[entity[1]]\n",
    "            end_idx = entity[2] + pos_offset[entity[2]] + 1\n",
    "            if args.BI_tag:\n",
    "                entity_mask[start_idx:end_idx] = [1] * (end_idx - start_idx)\n",
    "                for padding_idx in range(start_idx, end_idx):\n",
    "                    if padding_recording[padding_idx]:\n",
    "                        entity_mask[padding_idx] = 0\n",
    "            else:\n",
    "                entity_mask[start_idx] = 1\n",
    "\n",
    "            entity_masks.append(entity_mask)\n",
    "\n",
    "        for extra_idx in range(args.num_mask - len(entity_masks)):\n",
    "            entity_masks.append([\n",
    "                0 for _ in range(out_slot_labels_ids.shape[1])\n",
    "            ])\n",
    "\n",
    "        if args.BI_tag:\n",
    "            BI_tag_mask_pred.append(entity_masks)\n",
    "        else:\n",
    "            B_tag_mask_pred.append(entity_masks)\n",
    "\n",
    "    if args.BI_tag:\n",
    "        BI_tag_mask_pred_tensor = torch.FloatTensor(BI_tag_mask_pred)\n",
    "    else:\n",
    "        B_tag_mask_pred_tensor = torch.FloatTensor(B_tag_mask_pred)\n",
    "\n",
    "    BI_tag_mask_pred_input = None\n",
    "    B_tag_mask_pred_input = None\n",
    "\n",
    "    for eval_idx, batch in tqdm(enumerate(data_loader), desc=\"Predicting\", disable=False):\n",
    "        if args.BI_tag:\n",
    "            BI_tag_mask_pred_input = BI_tag_mask_pred_tensor[\n",
    "                                     eval_idx * (pred_config.batch_size):(eval_idx + 1) * pred_config.batch_size]\n",
    "        else:\n",
    "            B_tag_mask_pred_input = B_tag_mask_pred_tensor[\n",
    "                                    eval_idx * (pred_config.batch_size):(eval_idx + 1) * pred_config.batch_size]\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'intent_label_ids': batch[3],\n",
    "                      'slot_labels_ids': batch[4],\n",
    "                      'intent_token_ids': batch[5],\n",
    "                      'B_tag_mask': B_tag_mask_pred_input,\n",
    "                      'BI_tag_mask': BI_tag_mask_pred_input,\n",
    "                      'tag_intent_label': batch[8],\n",
    "                      'referee_labels_ids' : batch[9],\n",
    "                      'pro_labels_ids' : batch[10]}\n",
    "\n",
    "            if args.model_type != 'distilbert':\n",
    "                inputs['token_type_ids'] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            if args.pro and args.intent_seq and args.tag_intent: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    # print('len: ',len(outputs[:2][1]))\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits, referee_token_logits,all_referee_token_logits) = outputs[:2] #!!!!!!!!!!!!!\n",
    "            elif args.intent_seq and args.tag_intent:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits) = outputs[:2]\n",
    "            elif args.intent_seq:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits) = outputs[:2]\n",
    "            elif args.tag_intent:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits, tag_intent_logits) = outputs[:2]\n",
    "            else:\n",
    "                tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "        if args.tag_intent:\n",
    "            size_1 = inputs['tag_intent_label'].size(0)\n",
    "            size_2 = inputs['tag_intent_label'].size(1)\n",
    "\n",
    "            if tag_intent_preds is None:\n",
    "                tag_intent_preds = tag_intent_logits.view(size_1, size_2, -1).detach().cpu().numpy()\n",
    "                out_tag_intent_ids = inputs['tag_intent_label'].detach().cpu().numpy()\n",
    "            else:\n",
    "                tag_intent_preds = np.append(tag_intent_preds,\n",
    "                                             tag_intent_logits.view(size_1, size_2, -1).detach().cpu().numpy(),\n",
    "                                             axis=0)\n",
    "                out_tag_intent_ids = np.append(\n",
    "                    out_tag_intent_ids, inputs['tag_intent_label'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "\n",
    "    # ============================= Pronoun Referee Prediction ============================ !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    print('referee_preds shape: ',referee_preds.shape)\n",
    "    print('all_referee_preds shape: ', all_referee_preds.shape)\n",
    "    print('out_referee_labels_ids shape: ',out_referee_labels_ids.shape)\n",
    "    print('all_out_referee_labels_ids shape: ', all_out_referee_labels_ids.shape)\n",
    "    print('out_slot_labels_ids shape: ',out_slot_labels_ids.shape)\n",
    "    print('out_intent_token_ids shape: ',out_intent_token_ids.shape)\n",
    "\n",
    "    print('out_referee_labels_ids[0]:     ',out_referee_labels_ids[0])\n",
    "    print('out_slot_labels_ids[0]:        ',out_slot_labels_ids[0])\n",
    "\n",
    "    referee_token_map = {0:'PAD', 1:'O' ,2: 'B-referee'} # All referee are just one word in EGPSR\n",
    "\n",
    "\n",
    "    if args.pro:\n",
    "        referee_preds = np.argmax(referee_preds, axis=2)\n",
    "        all_referee_preds = np.argmax(all_referee_preds, axis=2)\n",
    "\n",
    "        print('referee_preds shape:       ',referee_preds.shape)\n",
    "        print('referee_preds [0]:         ',referee_preds[0])\n",
    "        print('all_referee_preds shape: ',all_referee_preds.shape)\n",
    "\n",
    "\n",
    "        referee_preds_list = [[] for _ in range(out_referee_labels_ids.shape[0])]\n",
    "        out_referee_label_list = [[] for _ in range(out_referee_labels_ids.shape[0])]\n",
    "        all_referee_preds_list = [[] for _ in range(all_out_referee_labels_ids.shape[0])]\n",
    "        all_out_referee_label_list = [[] for _ in range(all_out_referee_labels_ids.shape[0])]\n",
    "\n",
    "\n",
    "        for i in range(out_referee_labels_ids.shape[0]):\n",
    "            for j in range(out_referee_labels_ids.shape[1]):\n",
    "                if out_referee_labels_ids[i, j] != pad_token_label_id: #out_slot_labels_ids,out_referee_labels_ids\n",
    "                    out_referee_label_list[i].append(referee_token_map[out_referee_labels_ids[i][j]])\n",
    "                    referee_preds_list[i].append(referee_token_map[referee_preds[i][j]])\n",
    "\n",
    "        for i in range(all_out_referee_labels_ids.shape[0]):\n",
    "            for j in range(all_out_referee_labels_ids.shape[1]):\n",
    "                if all_out_referee_labels_ids[i, j] != pad_token_label_id: #all_out_referee_labels_ids\n",
    "                    all_out_referee_label_list[i].append(referee_token_map[all_out_referee_labels_ids[i][j]])\n",
    "                    all_referee_preds_list[i].append(referee_token_map[all_referee_preds[i][j]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    intent_token_map = {i: label for i, label in enumerate(intent_label_lst)}\n",
    "    out_intent_token_list = None\n",
    "    intent_token_preds_list = None\n",
    "    # ============================= Intent Seq Prediction ============================\n",
    "    if args.intent_seq:\n",
    "        if not args.use_crf:\n",
    "            intent_token_preds = np.argmax(intent_token_preds, axis=2)\n",
    "        out_intent_token_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "        intent_token_preds_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "\n",
    "        for i in range(out_intent_token_ids.shape[0]):\n",
    "            for j in range(out_intent_token_ids.shape[1]):\n",
    "                if out_intent_token_ids[i, j] != pad_token_label_id:\n",
    "                    out_intent_token_list[i].append(intent_token_map[out_intent_token_ids[i][j]])\n",
    "                    intent_token_preds_list[i].append(intent_token_map[intent_token_preds[i][j]])\n",
    "\n",
    "    out_tag_intent_list = None\n",
    "    tag_intent_preds_list = None\n",
    "    # ============================ Tag Intent Prediction ==============================\n",
    "    if args.tag_intent:\n",
    "        tag_intent_preds = np.argmax(tag_intent_preds, axis=2)\n",
    "        out_tag_intent_list = [[] for _ in range(out_tag_intent_ids.shape[0])]\n",
    "        tag_intent_preds_list = [[] for _ in range(out_tag_intent_ids.shape[0])]\n",
    "\n",
    "        for i in range(out_tag_intent_ids.shape[0]):\n",
    "            for j in range(out_tag_intent_ids.shape[1]):\n",
    "                if out_tag_intent_ids[i, j] != pad_token_label_id:\n",
    "                    out_tag_intent_list[i].append(intent_token_map[out_tag_intent_ids[i][j]])\n",
    "                    tag_intent_preds_list[i].append(intent_token_map[tag_intent_preds[i][j]])\n",
    "\n",
    "    # Write to output file\n",
    "    pronouns = ['him','her','it','its']\n",
    "    p_count = 0\n",
    "    with open(pred_config.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx,(words, slot_preds, intent_preds,referee_preds,true_referee_preds) in enumerate(zip(lines, slot_preds_list, intent_token_preds_list,all_referee_preds_list,all_out_referee_label_list)):\n",
    "            if idx <= 10:\n",
    "                print('words:              ',words, len(words))\n",
    "                print('slot_preds:         ',slot_preds, len(slot_preds))\n",
    "                print('referee_preds:      ',referee_preds, len(referee_preds))\n",
    "                print('true_referee_preds: ',true_referee_preds, len(true_referee_preds))\n",
    "                print('intent_preds:       ',intent_preds, len(intent_preds))\n",
    "                print('=====================================')\n",
    "            line = \"\"\n",
    "            if 'B-referee' not in referee_preds:#all([word not in pronouns for word in words]):\n",
    "                for word, i_pred, s_pred in zip(words, intent_preds, slot_preds):\n",
    "                    if s_pred == 'O' and i_pred == 'O':\n",
    "                        line = line + word + \" \"\n",
    "                    else:\n",
    "                        line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "                #f.write(\"<{}> -> {}\\n\".format(intent_label_lst[intent_pred], line.strip()))\n",
    "                f.write(line.strip()+'\\n')\n",
    "            else:\n",
    "                r_idx = referee_preds.index('B-referee')\n",
    "                for word, i_pred, s_pred, r_pred in zip(words, intent_preds, slot_preds,referee_preds):\n",
    "                    if s_pred == 'O' and i_pred == 'O':\n",
    "                        line = line + word + \" \"\n",
    "                    else:\n",
    "                        if word not in pronouns:\n",
    "                            line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "                            if r_pred == 'B-referee':\n",
    "                                ref = word\n",
    "                        else:\n",
    "                            if r_idx >= len(words):\n",
    "                                print('sample: ',idx)\n",
    "                                print(words)\n",
    "                                print('len(words): ',len(words))\n",
    "                                print('len(intent_preds): ',len(intent_preds))\n",
    "                                print('len(slot_preds): ',len(slot_preds))\n",
    "                                print(slot_preds)\n",
    "                                print('len(referee_preds): ',len(referee_preds))\n",
    "                                print(referee_preds)\n",
    "\n",
    "                                print(r_idx)\n",
    "                                print('--------------------------')\n",
    "\n",
    "\n",
    "                                line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "                            else:\n",
    "                                line = line + \"[{}:{}:{}:{}] \".format(word,words[r_idx], i_pred,s_pred)\n",
    "                                # if idx <= 100:\n",
    "                                #     print('good idx: ',idx)\n",
    "                                #     print(line)\n",
    "                                #     print('len(slot_preds): ',len(slot_preds))\n",
    "                                #     print(slot_preds)\n",
    "                                #     print('len(referee_preds): ',len(referee_preds))\n",
    "                                #     print(referee_preds)\n",
    "                                #     print(all_referee_preds_list[idx])\n",
    "\n",
    "                #f.write(\"<{}> -> {}\\n\".format(intent_label_lst[intent_pred], line.strip()))\n",
    "                f.write('\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n')\n",
    "                f.write('* Pro Case: \\n')\n",
    "                f.write(line.strip()+'\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n \\n')\n",
    "\n",
    "\n",
    "    logger.info(\"Prediction Done!\")\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/69 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'intent_label_ids'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m dev_dataset \u001B[38;5;241m=\u001B[39m load_and_cache_examples(pred_config, tokenizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     43\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m load_and_cache_examples(pred_config, tokenizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 45\u001B[0m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_config\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 52\u001B[0m, in \u001B[0;36mpredict\u001B[0;34m(pred_config, dataset)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mmodel_type \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistilbert\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     51\u001B[0m     inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m---> 52\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mpro \u001B[38;5;129;01mand\u001B[39;00m args\u001B[38;5;241m.\u001B[39mintent_seq \u001B[38;5;129;01mand\u001B[39;00m args\u001B[38;5;241m.\u001B[39mtag_intent: \u001B[38;5;66;03m#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\u001B[39;00m\n\u001B[1;32m     54\u001B[0m         tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits, referee_token_logits,all_referee_token_logits) \u001B[38;5;241m=\u001B[39m outputs[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;66;03m#!!!!!!!!!!!!!\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() got an unexpected keyword argument 'intent_label_ids'"
     ]
    }
   ],
   "source": [
    "from data_loader import load_and_cache_examples\n",
    "import argparse\n",
    "\n",
    "# train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--input_file\", default=\"sample_pred_in.txt\", type=str, help=\"Input file for prediction\")\n",
    "    parser.add_argument(\"--input_file\", default=\"data/gpsr_pro_instance/test/seq.in\", type=str, help=\"Input file for prediction\")\n",
    "\n",
    "    parser.add_argument(\"--task\", default='gpsr_pro_instance', type=str, help=\"The name of the task to train\")\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str,\n",
    "                        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--pro\", type=int, default=1, help=\"support pronoun disambiguition\")#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "\n",
    "    parser.add_argument(\"--output_file\", default=\"sample_pred_out_pro_instance.txt\", type=str, help=\"Output file for prediction\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"./gpsr_pro_instance_model_02-10-02:24:43\", type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--max_seq_len\", default=32, type=int,\n",
    "                        help=\"The maximum total input sequence length after tokenization.\")\n",
    "    parser.add_argument(\"--batch_size\", default=128, type=int, help=\"Batch size for prediction\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument('-f')  #########################\n",
    "\n",
    "    pred_config = parser.parse_args()\n",
    "\n",
    "    pred_config.model_name_or_path = MODEL_PATH_MAP[pred_config.model_type]\n",
    "    pred_config.model_name_or_path = MODEL_PATH_MAP[pred_config.model_type]\n",
    "\n",
    "    tokenizer = load_tokenizer(pred_config)\n",
    "    dev_dataset = load_and_cache_examples(pred_config, tokenizer, mode=\"dev\")\n",
    "    test_dataset = load_and_cache_examples(pred_config, tokenizer, mode=\"test\")\n",
    "\n",
    "    predict(pred_config, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] could you look for mary, retrieve the orange from the cupboard, and set it on the sink [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([0, 1, 1, 1, 1, 7, 1, 1, 1, 2, 1, 1, 8, 1, 1, 1, 2, 1, 1, 3, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer = load_tokenizer(pred_config)\n",
    "sample = test_dataset[0]\n",
    "print(tokenizer.decode(sample[0]))\n",
    "# dev_dataset[0]\n",
    "print(sample[4])\n",
    "print(sample[9])\n",
    "sample[10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 69/69 [00:07<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "args = get_args(pred_config)\n",
    "model = load_model(pred_config, args, 'cuda')\n",
    "sampler = SequentialSampler(test_dataset)\n",
    "data_loader = DataLoader(test_dataset, sampler=sampler, batch_size=pred_config.batch_size)\n",
    "for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "        batch = tuple(t.to('cuda') for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'intent_label_ids': batch[3],\n",
    "                      'slot_labels_ids': batch[4],\n",
    "                      'intent_token_ids': batch[5],\n",
    "                      'B_tag_mask': batch[6],\n",
    "                      'BI_tag_mask': batch[7],\n",
    "                      'tag_intent_label': batch[8],\n",
    "                       'referee_labels_ids' : batch[9],\n",
    "                       'pro_labels_ids' : batch[10]}\n",
    "\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            if args.pro and args.intent_seq and args.tag_intent: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits, referee_token_logits,all_referee_token_logits) = outputs[:2] #!!!!!!!!!!!!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([False, False,  True, False, False, False, False,  True, False, False,\n        False, False, False,  True,  True, False, False, False, False, False,\n        False, False,  True, False, False, False,  True, False, False,  True,\n        False, False, False,  True, False,  True,  True,  True,  True,  True,\n         True, False, False,  True, False,  True, False,  True, False,  True,\n        False, False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False], device='cuda:0')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] >0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'[CLS] leave sponge on the sink, grasp the fruits from the dining table, and deliver to it to me please [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][7])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([67, 32])\n",
      "torch.Size([67, 32])\n",
      "torch.Size([67])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pro_labels_ids\"].shape)\n",
    "print(inputs[\"referee_labels_ids\"].shape)\n",
    "b = torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] >0\n",
    "print(b.shape)\n",
    "print(inputs[\"referee_labels_ids\"][7])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 2, 3, 4],\n       [4, 5, 6, 7]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4],[2,3,4,5],[4,5,6,7]])\n",
    "b = np.array([True,False,True])\n",
    "c = a[b]\n",
    "c"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,2,3,4,5]\n",
    "for c,d in zip(a,b):\n",
    "    print(c,d)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jointbert",
   "language": "python",
   "display_name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
