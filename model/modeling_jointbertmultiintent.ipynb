{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from transformers.modeling_bert import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel, BertConfig\n",
    "# from torchcrf import CRF\n",
    "from TorchCRF import CRF\n",
    "from module import IntentClassifier, SlotClassifier, IntentTokenClassifier, MultiIntentClassifier, TagIntentClassifier\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBERTMultiIntent(BertPreTrainedModel):\n",
    "    # multi_intent: 1,\n",
    "    # intent_seq: 1,\n",
    "    # tag_intent: 1,\n",
    "    # bi_tag: 1,\n",
    "    # cls_token_cat: 1,\n",
    "    # intent_attn: 1,\n",
    "    # num_mask: 4\n",
    "    def __init__(self, config, args, intent_label_lst, slot_label_lst):\n",
    "        super().__init__(config)\n",
    "        self.args = args\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.num_slot_labels = len(slot_label_lst)\n",
    "        # load pretrain bert\n",
    "        self.bert = BertModel(config=config)\n",
    "\n",
    "        # self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        self.multi_intent_classifier = MultiIntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, args.dropout_rate)\n",
    "        if args.intent_seq:\n",
    "            self.intent_token_classifier = IntentTokenClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        \n",
    "        if args.tag_intent:\n",
    "            if args.cls_token_cat:\n",
    "                self.tag_intent_classifier = TagIntentClassifier(2 * config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "            else:\n",
    "                self.tag_intent_classifier = TagIntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        \n",
    "        if args.use_crf:\n",
    "            self.crf = CRF(num_tags=self.num_slot_labels, batch_first=True)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                token_type_ids,\n",
    "                intent_label_ids,\n",
    "                slot_labels_ids,\n",
    "                intent_token_ids,\n",
    "                B_tag_mask,\n",
    "                BI_tag_mask,\n",
    "                tag_intent_label):\n",
    "        \"\"\"\n",
    "            Args: B: batch_size; L: sequence length; I: the number of intents; M: number of mask; D: the output dim of Bert\n",
    "            input_ids: B * L\n",
    "            token_type_ids: B * L\n",
    "            token_type_ids: B * L\n",
    "            intent_label_ids: B * I\n",
    "            slot_labels_ids: B * L\n",
    "            intent_token_ids: B * L\n",
    "            B_tag_mask: B * M * L\n",
    "            BI_tag_mask: B * M * L\n",
    "            tag_intent_label: B * M\n",
    "        \"\"\"\n",
    "        # input_ids:  torch.Size([32, 50])\n",
    "        # attention_mask:  torch.Size([32, 50])\n",
    "        # token_type_ids:  torch.Size([32, 50])\n",
    "        # intent_label_ids:  torch.Size([32, 10])\n",
    "        # slot_labels_ids:  torch.Size([32, 50])\n",
    "        # intent_token_ids:  torch.Size([32, 50])\n",
    "        # B_tag_mask:  torch.Size([32, 4, 50])\n",
    "        # BI_tag_mask:  torch.Size([32, 4, 50])\n",
    "        # tag_intent_label:  torch.Size([32, 4])\n",
    "        \n",
    "        # (len_seq, batch_size, hidden_dim), (batch_size, hidden_dim)\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        # B * L * D\n",
    "        sequence_output = outputs[0]\n",
    "        # B * D\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # ==================================== 1. Intent Softmax ========================================\n",
    "        # (batch_size, num_intents)\n",
    "        intent_logits = self.multi_intent_classifier(pooled_output)\n",
    "        intent_logits_cpu = intent_logits.data.cpu().numpy()\n",
    "        \n",
    "        if intent_label_ids is not None:\n",
    "            if self.num_intent_labels == 1:\n",
    "                intent_loss_fct = nn.MSELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1, self.num_intent_labels))\n",
    "            else:\n",
    "                # intent_loss_fct = nn.CrossEntropyLoss()\n",
    "                # default reduction is mean\n",
    "                intent_loss_fct = nn.BCELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels) + 1e-10, intent_label_ids.view(-1, self.num_intent_labels))\n",
    "            # Question: do we need to add weight here\n",
    "            total_loss += intent_loss\n",
    "            \n",
    "        if intent_label_ids.type() != torch.cuda.FloatTensor:\n",
    "            intent_label_ids = intent_label_ids.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "        # ==================================== 2. Slot Softmax ========================================\n",
    "        # (batch_size, seq_len, num_slots)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "        \n",
    "        if slot_labels_ids is not None:\n",
    "            if self.args.use_crf:\n",
    "                slot_loss = self.crf(slot_logits, slot_labels_ids, mask=attention_mask.byte(), reduction='mean')\n",
    "                slot_loss = -1 * slot_loss  # negative log-likelihood\n",
    "            else:\n",
    "                slot_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "                # Only keep active parts of the loss\n",
    "                if attention_mask is not None:\n",
    "                    try:\n",
    "                        active_loss = attention_mask.view(-1) == 1\n",
    "                        attention_mask_cpu = attention_mask.data.cpu().numpy()\n",
    "                        active_loss_cpu = active_loss.data.cpu().numpy()\n",
    "                        active_logits = slot_logits.view(-1, self.num_slot_labels)[active_loss]\n",
    "                        active_labels = slot_labels_ids.view(-1)[active_loss]\n",
    "                        slot_loss = slot_loss_fct(active_logits, active_labels)\n",
    "                    except:\n",
    "                        print('intent_logits: ', intent_logits_cpu)\n",
    "                        print('attention_mask: ', attention_mask_cpu)\n",
    "                        print('active_loss: ', active_loss_cpu)\n",
    "                        logger.info('intent_logits: ', intent_logits_cpu)\n",
    "                        logger.info('attention_mask: ', attention_mask_cpu)\n",
    "                        logger.info('active_loss: ', active_loss_cpu)\n",
    "                else:\n",
    "                    slot_loss = slot_loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_labels_ids.view(-1))\n",
    "\n",
    "            total_loss += self.args.slot_loss_coef * slot_loss\n",
    "        \n",
    "        \n",
    "        # ==================================== 3. Intent Token Softmax ========================================\n",
    "        intent_token_loss = 0.0\n",
    "        if self.args.intent_seq:\n",
    "            # (batch_size, seq_len, num_intents)\n",
    "            intent_token_logits = self.intent_token_classifier(sequence_output)\n",
    "\n",
    "            if intent_token_ids is not None:\n",
    "                if self.args.use_crf:\n",
    "                    intent_token_loss = self.crf(intent_token_logits, intent_token_ids, mask=attention_mask.byte, reduction='mean')\n",
    "                    intent_token_loss = -1 * intent_token_loss\n",
    "                else:\n",
    "                    intent_token_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "                    if attention_mask is not None:\n",
    "                        active_intent_loss = attention_mask.view(-1) == 1\n",
    "                        active_intent_logits = intent_token_logits.view(-1, self.num_intent_labels)[active_intent_loss]\n",
    "                        active_intent_tokens = intent_token_ids.view(-1)[active_intent_loss]\n",
    "                        intent_token_loss = intent_token_loss_fct(active_intent_logits, active_intent_tokens)\n",
    "                    else:\n",
    "                        intent_token_loss = intent_token_loss_fct(intent_token_logits.view(-1, self.num_intent_labels), intent_token_ids.view(-1))\n",
    "                \n",
    "                total_loss += self.args.slot_loss_coef * intent_token_loss\n",
    "        \n",
    "        # convert the sequence_out to long\n",
    "        if BI_tag_mask != None and  BI_tag_mask.type() != torch.cuda.FloatTensor:\n",
    "            BI_tag_mask = BI_tag_mask.type(torch.cuda.FloatTensor)\n",
    "\n",
    "        if B_tag_mask != None and B_tag_mask.type() != torch.cuda.FloatTensor:\n",
    "            B_tag_mask = B_tag_mask.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        tag_intent_loss = 0.0\n",
    "        if self.args.tag_intent:\n",
    "            # B * M * D\n",
    "            if self.args.BI_tag:\n",
    "                tag_intent_vec = torch.einsum('bml,bld->bmd', BI_tag_mask, sequence_output)\n",
    "            else:\n",
    "                tag_intent_vec = torch.einsum('bml,bld->bmd', B_tag_mask, sequence_output)\n",
    "            \n",
    "            if self.args.cls_token_cat:\n",
    "                cls_token = pooled_output.unsqueeze(1)\n",
    "                # B * M * D\n",
    "                cls_token = cls_token.repeat(1, self.args.num_mask, 1)\n",
    "                # B * M * 2D\n",
    "                tag_intent_vec = torch.cat((cls_token, tag_intent_vec), dim=2)\n",
    "            \n",
    "            tag_intent_vec = tag_intent_vec.view(tag_intent_vec.size(0) * tag_intent_vec.size(1), -1)\n",
    "            \n",
    "            # after softmax\n",
    "            tag_intent_logits = self.tag_intent_classifier(tag_intent_vec)\n",
    "\n",
    "            if self.args.intent_attn:\n",
    "                # (batch_size, num_intent) => (batch_size * num_mask, num_intent) sigmoid [0, 1]\n",
    "                intent_probs = intent_logits.unsqueeze(1)\n",
    "                intent_probs = intent_probs.repeat(1, self.args.num_mask, 1)\n",
    "                intent_probs = intent_probs.view(intent_probs.size(0) * intent_probs.size(1), -1)\n",
    "                # (batch_size * num_mask, num_intent)\n",
    "                tag_intent_logits = tag_intent_logits * intent_probs\n",
    "                tag_intent_logits = tag_intent_logits.div(tag_intent_logits.sum(dim=1, keepdim=True))\n",
    "            \n",
    "            # tag_intent_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "\n",
    "            # tag_intent_loss = tag_intent_loss_fct(tag_intent_logits, tag_intent_label.view(-1))\n",
    "\n",
    "            nll_fct = nn.NLLLoss(ignore_index=self.args.ignore_index)\n",
    "            \n",
    "            tag_intent_loss = nll_fct(torch.log(tag_intent_logits + 1e-10), tag_intent_label.view(-1))\n",
    "            \n",
    "            total_loss += self.args.tag_intent_coef * tag_intent_loss\n",
    "            \n",
    "        if self.args.intent_seq and self.args.tag_intent:\n",
    "            outputs = ((intent_logits, slot_logits, intent_token_logits, tag_intent_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        elif self.args.intent_seq:\n",
    "            outputs = ((intent_logits, slot_logits, intent_token_logits),) + outputs[2:]\n",
    "        elif self.args.tag_intent:\n",
    "            outputs = ((intent_logits, slot_logits, tag_intent_logits),) + outputs[2:]\n",
    "        else:\n",
    "            outputs = ((intent_logits, slot_logits),) + outputs[2:]\n",
    "        \n",
    "        outputs = ([total_loss, intent_loss, slot_loss, intent_token_loss, tag_intent_loss],) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01margparse\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     11\u001B[0m     time_wait \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n",
      "\u001B[0;31mImportError\u001B[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import argparse\n",
    "from .utils import init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_wait = random.uniform(0, 10)\n",
    "    time.sleep(time_wait)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\"--task\", default='gpsr', type=str, help=\"The name of the task to train\")\n",
    "\n",
    "#     parser.add_argument(\"--model_dir\", default='./gpsr_model', required=True, type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--model_dir\", default='./gpsr_model', type=str, help=\"Path to save, load model\")\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str, help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "#     parser.add_argument(\"--intent_seq\", type=int, default=0, help=\"whether we use intent seq setting\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "    parser.add_argument(\"--multi_intent\", type=int, default=1, help=\"whether we use multi intent setting\")\n",
    "    parser.add_argument(\"--tag_intent\", type=int, default=1, help=\"whether we can use tag to predict intent\")\n",
    "    \n",
    "    parser.add_argument(\"--BI_tag\", type=int, default=1, help='use BI sum or just B')\n",
    "    parser.add_argument(\"--cls_token_cat\", type=int, default=1, help='whether we cat the cls to the slot output of bert')\n",
    "    parser.add_argument(\"--intent_attn\", type=int, default=1, help='whether we use attention mechanism on the CLS intent output')\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "                                           #max slot num = 7\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--seed', type=int, default=25, help=\"random seed for initialization\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "#     parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "\n",
    "    parser.add_argument(\"--eval_batch_size\", default=128, type=int, help=\"Batch size for evaluation.\")\n",
    "    parser.add_argument(\"--max_seq_len\", default=35, type=int, help=\"The maximum total input sequence length after tokenization.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", default=10.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "                                            #####\n",
    "    \n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
    "    parser.add_argument('--logging_steps', type=int, default=500, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument('--save_steps', type=int, default=300, help=\"Save checkpoint every X updates steps.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "    parser.add_argument('--slot_loss_coef', type=float, default=2.0, help='Coefficient for the slot loss.')\n",
    "    parser.add_argument('--tag_intent_coef', type=float, default=1.0, help='Coefficient for the tag intent loss')\n",
    "\n",
    "    # CRF option\n",
    "    parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
    "    parser.add_argument(\"--slot_pad_label\", default=\"PAD\", type=str, help=\"Pad token for slot label pad (to be ignore when calculate loss)\")\n",
    "    parser.add_argument(\"--patience\", default=0, type=int, help=\"The initial learning rate for Adam.\")\n",
    "    \n",
    "    parser.add_argument('-f')#########################\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "    \n",
    "    tokenizer = load_tokenizer(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "# dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "# test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "# trainer = Trainer_multi(args, train_dataset, dev_dataset, test_dataset)\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointbert",
   "language": "python",
   "name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
