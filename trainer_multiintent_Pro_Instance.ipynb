{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import MODEL_CLASSES, compute_metrics, get_intent_labels, get_slot_labels, compute_metrics_multi_intent,compute_metrics_multi_intent_Pro,compute_metrics_final\n",
    "\n",
    "from seqeval.metrics.sequence_labeling import get_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_binary_cross_entropy(output, target, weights=None):\n",
    "#     if weights is not None:\n",
    "#         assert len(weights) == 2\n",
    "#\n",
    "#         loss = weights[1] * (target * torch.log(output)) + \\\n",
    "#                weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "#     else:\n",
    "#         loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
    "#\n",
    "#     return torch.neg(torch.mean(loss))\n",
    "\n",
    "class Trainer_multi(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        self.slot_preds_list = None\n",
    "        self.intent_token_preds_list = None\n",
    "        \n",
    "        \n",
    "        # set of intents\n",
    "        self.intent_label_lst = get_intent_labels(args)\n",
    "        # set of slots\n",
    "        self.slot_label_lst = get_slot_labels(args)\n",
    "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "        self.pad_token_label_id = args.ignore_index\n",
    "\n",
    "        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "        \n",
    "        self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "\n",
    "        self.model = self.model_class.from_pretrained(args.model_name_or_path,\n",
    "                                                      config=self.config,\n",
    "                                                      args=args,\n",
    "                                                      intent_label_lst=self.intent_label_lst,\n",
    "                                                      slot_label_lst=self.slot_label_lst,\n",
    "                                                      )\n",
    "        \n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "       \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        #logger.info(vars(self.args))\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n",
    "\n",
    "        if self.args.max_steps > 0:\n",
    "            t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.args.weight_decay},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
    "        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
    "        logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "        step_per_epoch = len(train_dataloader) // 2\n",
    "\n",
    "        # record the evaluation loss\n",
    "        eval_acc = 0.0\n",
    "        MAX_RECORD = self.args.patience\n",
    "        num_eval = -1\n",
    "        eval_result_record = (num_eval, eval_acc)\n",
    "        flag = False\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=FLAG)\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
    "\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4],\n",
    "                          'intent_token_ids': batch[5],\n",
    "                          'B_tag_mask': batch[6],\n",
    "                          'BI_tag_mask': batch[7],\n",
    "                          'tag_intent_label': batch[8],\n",
    "                          'referee_labels_ids' : batch[9], #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                          'pro_labels_ids' : batch[10]}\n",
    "                if self.args.model_type != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                losses = outputs[0]\n",
    "                loss = losses[0]\n",
    "                intent_loss = losses[1]\n",
    "                slot_loss = losses[2]\n",
    "                intent_token_loss = losses[3]\n",
    "                tag_intent_loss = losses[4]\n",
    "                referee_token_loss = losses[5] #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                # tag_intent_loss_softmax = losses[5]\n",
    "\n",
    "                if step == 500:\n",
    "                    print('referee_token_loss: ',referee_token_loss)\n",
    "\n",
    "                if self.args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
    "                    if self.args.logging_steps > 0 and global_step % step_per_epoch == 0:\n",
    "                        logger.info(\"***** Training Step %d *****\", step)\n",
    "                        logger.info(\"  total_loss = %f\", loss)\n",
    "                        logger.info(\"  intent_loss = %f\", intent_loss)\n",
    "                        logger.info(\"  slot_loss = %f\", slot_loss)\n",
    "                        logger.info(\"  intent_token_loss = %f\", intent_token_loss)\n",
    "                        logger.info(\"  tag_intent_loss = %f\", tag_intent_loss)\n",
    "                        logger.info(\"  referee_token_loss = %f\", referee_token_loss) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                        # logger.info(\"  tag_intent_loss_softmax = %f\", tag_intent_loss_softmax)\n",
    "\n",
    "                        dev_result = self.evaluate(\"dev\")\n",
    "                        test_result = self.evaluate(\"test\")\n",
    "                        num_eval += 1\n",
    "                        if self.args.patience != 0:\n",
    "                            if dev_result['sementic_frame_acc'] + dev_result['intent_acc'] + dev_result['slot_f1']   > eval_result_record[1]:\n",
    "                                self.save_model()\n",
    "                                eval_result_record = (num_eval, dev_result['sementic_frame_acc'] + dev_result['intent_acc'] + dev_result['slot_f1'] )\n",
    "                            else:\n",
    "                                cur_num_eval = eval_result_record[0]\n",
    "                                if num_eval - cur_num_eval >= MAX_RECORD:\n",
    "                                    # it has been ok\n",
    "                                    logger.info(' EARLY STOP Evaluate: at {}, best eval {} intent_slot_acc: {} '.format(num_eval, cur_num_eval, eval_result_record[1]))\n",
    "                                    flag = True\n",
    "                                    break\n",
    "                        else:\n",
    "                            self.save_model()\n",
    "\n",
    "                            \n",
    "\n",
    "                        # we check whether there is an overfitting issue for mixsnips\n",
    "                        \n",
    "\n",
    "                    # if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
    "                    #     self.save_model()\n",
    "                    \n",
    "                if 0 < self.args.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if flag:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "            if 0 < self.args.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    def evaluate(self, mode):\n",
    "        if mode == 'test':\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == 'dev':\n",
    "            dataset = self.dev_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"  Num examples = %d\", len(dataset))\n",
    "        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        intent_preds = None\n",
    "        slot_preds = None\n",
    "        intent_token_preds = None\n",
    "        out_intent_label_ids = None\n",
    "        out_slot_labels_ids = None\n",
    "        out_intent_token_ids = None\n",
    "        \n",
    "        tag_intent_preds = None\n",
    "        out_tag_intent_ids = None\n",
    "        referee_preds = None #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        out_referee_labels_ids = None #!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        all_referee_preds = None #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        all_out_referee_labels_ids = None #!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=FLAG):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4],\n",
    "                          'intent_token_ids': batch[5],\n",
    "                          'B_tag_mask': batch[6],\n",
    "                          'BI_tag_mask': batch[7],\n",
    "                          'tag_intent_label': batch[8],\n",
    "                          'referee_labels_ids' : batch[9],\n",
    "                          'pro_labels_ids' : batch[10]}\n",
    "                if self.args.model_type != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "                outputs = self.model(**inputs)\n",
    "                if self.args.pro and self.args.intent_seq and self.args.tag_intent: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits, referee_token_logits,all_referee_token_logits) = outputs[:2] #!!!!!!!!!!!!!\n",
    "                elif self.args.intent_seq and self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits) = outputs[:2]\n",
    "                elif self.args.intent_seq:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits) = outputs[:2]\n",
    "                elif self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, tag_intent_logits) = outputs[:2]\n",
    "                else:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "#                 eval_loss += tmp_eval_loss.mean().item()\n",
    "#             nb_eval_steps += 1\n",
    "\n",
    "            # ============================ Intent prediction =============================\n",
    "            if intent_preds is None:\n",
    "                intent_preds = intent_logits.detach().cpu().numpy()\n",
    "                out_intent_label_ids = inputs['intent_label_ids'].detach().cpu().numpy()\n",
    "            else:\n",
    "                intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
    "                out_intent_label_ids = np.append(\n",
    "                    out_intent_label_ids, inputs['intent_label_ids'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            # ============================= Slot prediction ==============================\n",
    "            if slot_preds is None:\n",
    "                if self.args.use_crf:\n",
    "                    # decode() in `torchcrf` returns list with best index directly\n",
    "                    slot_preds = np.array(self.model.crf.decode(slot_logits))\n",
    "                else:\n",
    "                    slot_preds = slot_logits.detach().cpu().numpy()\n",
    "\n",
    "                out_slot_labels_ids = inputs[\"slot_labels_ids\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                if self.args.use_crf:\n",
    "                    slot_preds = np.append(slot_preds, np.array(self.model.crf.decode(slot_logits)), axis=0)\n",
    "                else:\n",
    "                    slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                out_slot_labels_ids = np.append(out_slot_labels_ids, inputs[\"slot_labels_ids\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            # ============================= Pronoun referee prediction ==============================\n",
    "            if self.args.pro:\n",
    "                if all_referee_preds is None:\n",
    "                    all_referee_preds = all_referee_token_logits.detach().cpu().numpy()\n",
    "                    referee_preds = referee_token_logits.detach().cpu().numpy()\n",
    "\n",
    "                    pro_sample_mask_np = (torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] > 0).detach().cpu().numpy()\n",
    "                    all_out_referee_labels_ids = inputs[\"referee_labels_ids\"].detach().cpu().numpy()\n",
    "                    out_referee_labels_ids = all_out_referee_labels_ids[pro_sample_mask_np]#np.array([ele for i,ele in enumerate(all_out_referee_labels_ids) if pro_sample_mask_np[i] != False])\n",
    "\n",
    "\n",
    "                else:\n",
    "                    all_referee_preds = np.append(all_referee_preds,all_referee_token_logits.detach().cpu().numpy(), axis = 0)\n",
    "                    referee_preds = np.append(referee_preds, referee_token_logits.detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "                    pro_sample_mask_np = (torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] > 0).detach().cpu().numpy()\n",
    "                    new_all_out_referee_labels_ids = inputs[\"referee_labels_ids\"].detach().cpu().numpy()\n",
    "                    all_out_referee_labels_ids = np.append(all_out_referee_labels_ids,new_all_out_referee_labels_ids,axis = 0)\n",
    "                    new_out_referee_labels_ids = new_all_out_referee_labels_ids[pro_sample_mask_np]#np.array([ele for i,ele in enumerate(new_all_out_referee_labels_ids) if pro_sample_mask_np[i] != False])\n",
    "                    out_referee_labels_ids = np.append(out_referee_labels_ids, new_out_referee_labels_ids, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "            # print('referee_preds shape: ',referee_preds.shape)\n",
    "            # print('all_referee_preds shape: ', all_referee_preds.shape)\n",
    "            # print('out_referee_labels_ids shape: ',out_referee_labels_ids.shape)\n",
    "            # print('all_out_referee_labels_ids shape: ', all_out_referee_labels_ids.shape)\n",
    "            # ============================== Intent Token Seq =============================\n",
    "            if self.args.intent_seq:\n",
    "                if intent_token_preds is None:\n",
    "                    if self.args.use_crf:\n",
    "                        intent_token_preds = np.array(self.model.crf.decode(intent_token_logits))\n",
    "                    else:\n",
    "                        intent_token_preds = intent_token_logits.detach().cpu().numpy()\n",
    "\n",
    "                    out_intent_token_ids = inputs[\"intent_token_ids\"].detach().cpu().numpy()\n",
    "                else:\n",
    "                    if self.args.use_crf:\n",
    "                        intent_token_preds = np.append(intent_token_preds, np.array(self.model.crf.decode(intent_token_logits)), axis=0)\n",
    "                    else:\n",
    "                        intent_token_preds = np.append(intent_token_preds, intent_token_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    out_intent_token_ids = np.append(out_intent_token_ids, inputs[\"intent_token_ids\"].detach().cpu().numpy(), axis=0)\n",
    "        \n",
    "            # slot_preds: (64 * n, 50, 74)    \n",
    "        \n",
    "#         eval_loss = eval_loss / nb_eval_steps\n",
    "#         results = {\n",
    "#             \"loss\": eval_loss\n",
    "#         }\n",
    "\n",
    "        # Intent result\n",
    "        # (batch_size, )\n",
    "        # intent_preds = np.argmax(intent_preds, axis=1)\n",
    "        # (batch_size, num_intents)\n",
    "        # we set the threshold to 0.5\n",
    "        intent_preds = torch.as_tensor(intent_preds > 0.5, dtype=torch.int32)\n",
    "\n",
    "        # Slot result\n",
    "        # (batch_size, seq_len)\n",
    "        if not self.args.use_crf:\n",
    "            slot_preds = np.argmax(slot_preds, axis=2)\n",
    "        slot_label_map = {i: label for i, label in enumerate(self.slot_label_lst)}\n",
    "        out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "        slot_preds_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "        \n",
    "        B_tag_mask_pred = []\n",
    "        BI_tag_mask_pred = []\n",
    "        \n",
    "        # generate mask\n",
    "        for i in range(out_slot_labels_ids.shape[0]):\n",
    "            # record the padding position\n",
    "            pos_offset = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "            pos_cnt = 0\n",
    "            padding_recording = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "            \n",
    "            for j in range(out_slot_labels_ids.shape[1]):\n",
    "                if out_slot_labels_ids[i, j] != self.pad_token_label_id:\n",
    "                    out_slot_label_list[i].append(slot_label_map[out_slot_labels_ids[i][j]])\n",
    "                    slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
    "                    pos_offset[pos_cnt+1] = pos_offset[pos_cnt]\n",
    "                    pos_cnt += 1\n",
    "                else:\n",
    "                    pos_offset[pos_cnt] = pos_offset[pos_cnt] + 1\n",
    "                    padding_recording[j] = 1\n",
    "                    \n",
    "\n",
    "            entities = get_entities(slot_preds_list[i])\n",
    "            entities = [tag for entity_idx, tag in enumerate(entities) if slot_preds_list[i][tag[1]].startswith('B')]\n",
    "            \n",
    "            if len(entities) > self.args.num_mask:\n",
    "                entities = entities[:self.args.num_mask]\n",
    "            \n",
    "            entity_masks = []\n",
    "            \n",
    "            for entity_idx, entity in enumerate(entities):\n",
    "                entity_mask = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "                start_idx = entity[1] + pos_offset[entity[1]]\n",
    "                end_idx = entity[2] + pos_offset[entity[2]] + 1\n",
    "                if self.args.BI_tag:\n",
    "                    entity_mask[start_idx:end_idx] = [1] * (end_idx - start_idx)\n",
    "                    for padding_idx in range(start_idx, end_idx):\n",
    "                        if padding_recording[padding_idx]:\n",
    "                            entity_mask[padding_idx] = 0\n",
    "                else:\n",
    "                    entity_mask[start_idx] = 1\n",
    "                    \n",
    "                entity_masks.append(entity_mask)\n",
    "            \n",
    "            for extra_idx in range(self.args.num_mask - len(entity_masks)):\n",
    "                entity_masks.append([\n",
    "                    0 for _ in range(out_slot_labels_ids.shape[1])\n",
    "                ])\n",
    "\n",
    "            \n",
    "            if self.args.BI_tag:\n",
    "                BI_tag_mask_pred.append(entity_masks)\n",
    "            else:\n",
    "                B_tag_mask_pred.append(entity_masks)\n",
    "                \n",
    "        if self.args.BI_tag:\n",
    "            BI_tag_mask_pred_tensor = torch.FloatTensor(BI_tag_mask_pred)\n",
    "        else:\n",
    "            B_tag_mask_pred_tensor = torch.FloatTensor(B_tag_mask_pred)\n",
    "        \n",
    "        BI_tag_mask_pred_input = None\n",
    "        B_tag_mask_pred_input = None\n",
    "        \n",
    "        for eval_idx, batch in tqdm(enumerate(eval_dataloader), desc=\"Evaluating\", disable=FLAG):\n",
    "            if self.args.BI_tag:\n",
    "                BI_tag_mask_pred_input = BI_tag_mask_pred_tensor[eval_idx*self.args.eval_batch_size:(eval_idx+1)*self.args.eval_batch_size]\n",
    "            else:\n",
    "                B_tag_mask_pred_input = B_tag_mask_pred_tensor[eval_idx*self.args.eval_batch_size:(eval_idx+1)*self.args.eval_batch_size]\n",
    "            \n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4],\n",
    "                          'intent_token_ids': batch[5],\n",
    "                          'B_tag_mask': B_tag_mask_pred_input,\n",
    "                          'BI_tag_mask': BI_tag_mask_pred_input,\n",
    "                          'tag_intent_label': batch[8],\n",
    "                          'referee_labels_ids' : batch[9],\n",
    "                          'pro_labels_ids' : batch[10]}\n",
    "                if self.args.model_type != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "                outputs = self.model(**inputs)\n",
    "                if self.args.pro and self.args.intent_seq and self.args.tag_intent: #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    # print('len: ',len(outputs[:2][1]))\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits, referee_token_logits,all_referee_token_logits) = outputs[:2] #!!!!!!!!!!!!!\n",
    "                elif self.args.intent_seq and self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits) = outputs[:2]\n",
    "                elif self.args.intent_seq:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits) = outputs[:2]\n",
    "                elif self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, tag_intent_logits) = outputs[:2]\n",
    "                else:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "                # if mode == 'test':\n",
    "                #     print(eval_idx, ' ', tmp_eval_loss)\n",
    "\n",
    "\n",
    "\n",
    "                eval_loss += tmp_eval_loss[0].mean().item()\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            if self.args.tag_intent:\n",
    "                size_1 = inputs['tag_intent_label'].size(0)\n",
    "                size_2 = inputs['tag_intent_label'].size(1)\n",
    "                \n",
    "                if tag_intent_preds is None:\n",
    "                    tag_intent_preds = tag_intent_logits.view(size_1, size_2, -1).detach().cpu().numpy()\n",
    "                    out_tag_intent_ids = inputs['tag_intent_label'].detach().cpu().numpy()\n",
    "                else:\n",
    "                    tag_intent_preds = np.append(tag_intent_preds, tag_intent_logits.view(size_1, size_2, -1).detach().cpu().numpy(), axis=0)\n",
    "#                     print('out_tag_intent_ids shape: ', out_tag_intent_ids.shape)\n",
    "#                     print('tag_intent_label shape: ', inputs['tag_intent_label'].shape)\n",
    "                    out_tag_intent_ids = np.append(\n",
    "                        out_tag_intent_ids, inputs['tag_intent_label'].detach().cpu().numpy(), axis=0)\n",
    "                \n",
    "        \n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\n",
    "            \"loss\": eval_loss\n",
    "        }\n",
    "\n",
    "\n",
    "        # ============================= Pronoun Referee Prediction ============================ !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # print('referee_preds shape: ',referee_preds.shape)\n",
    "        # print('all_referee_preds shape: ', all_referee_preds.shape)\n",
    "        # print('out_referee_labels_ids shape: ',out_referee_labels_ids.shape)\n",
    "        print('all_out_referee_labels_ids shape: ', all_out_referee_labels_ids.shape)\n",
    "        print('out_intent_token_ids shape: ',out_intent_token_ids.shape)\n",
    "\n",
    "        referee_token_map = {0:'PAD', 1:'O' ,2: 'B-referee'} # All referee are just one word in EGPSR\n",
    "\n",
    "\n",
    "        if self.args.pro:\n",
    "            referee_preds = np.argmax(referee_preds, axis=2)\n",
    "            all_referee_preds = np.argmax(all_referee_preds, axis=2)\n",
    "\n",
    "\n",
    "            referee_preds_list = [[] for _ in range(out_referee_labels_ids.shape[0])]\n",
    "            out_referee_label_list = [[] for _ in range(out_referee_labels_ids.shape[0])]\n",
    "            all_referee_preds_list = [[] for _ in range(all_out_referee_labels_ids.shape[0])]\n",
    "            all_out_referee_label_list = [[] for _ in range(all_out_referee_labels_ids.shape[0])]\n",
    "\n",
    "\n",
    "\n",
    "            for i in range(out_referee_labels_ids.shape[0]):\n",
    "                for j in range(out_referee_labels_ids.shape[1]):\n",
    "                    if out_referee_labels_ids[i, j] != self.pad_token_label_id:\n",
    "                        out_referee_label_list[i].append(referee_token_map[out_referee_labels_ids[i][j]])\n",
    "                        referee_preds_list[i].append(referee_token_map[referee_preds[i][j]])\n",
    "\n",
    "            for i in range(all_out_referee_labels_ids.shape[0]):\n",
    "                for j in range(all_out_referee_labels_ids.shape[1]):\n",
    "                    if all_out_referee_labels_ids[i, j] != self.pad_token_label_id:\n",
    "                        all_out_referee_label_list[i].append(referee_token_map[all_out_referee_labels_ids[i][j]])\n",
    "                        all_referee_preds_list[i].append(referee_token_map[all_referee_preds[i][j]])\n",
    "\n",
    "\n",
    "\n",
    "        intent_token_map = {i: label for i, label in enumerate(self.intent_label_lst)}\n",
    "        out_intent_token_list = None\n",
    "        intent_token_preds_list = None\n",
    "        # ============================= Intent Seq Prediction ============================\n",
    "        if self.args.intent_seq:\n",
    "            if not self.args.use_crf:\n",
    "                intent_token_preds = np.argmax(intent_token_preds, axis=2)\n",
    "            out_intent_token_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "            intent_token_preds_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "\n",
    "            for i in range(out_intent_token_ids.shape[0]):\n",
    "                for j in range(out_intent_token_ids.shape[1]):\n",
    "                    if out_intent_token_ids[i, j] != self.pad_token_label_id:\n",
    "                        out_intent_token_list[i].append(intent_token_map[out_intent_token_ids[i][j]])\n",
    "                        intent_token_preds_list[i].append(intent_token_map[intent_token_preds[i][j]])\n",
    "\n",
    "\n",
    "\n",
    "        # ============================ Tag Intent Prediction ==============================\n",
    "        if self.args.tag_intent:\n",
    "            tag_intent_preds = np.argmax(tag_intent_preds, axis=2)\n",
    "            out_tag_intent_list = [[] for _ in range(out_tag_intent_ids.shape[0])]\n",
    "            tag_intent_preds_list = [[] for _ in range(out_tag_intent_ids.shape[0])]\n",
    "            \n",
    "            for i in range(out_tag_intent_ids.shape[0]):\n",
    "                for j in range(out_tag_intent_ids.shape[1]):\n",
    "                    if out_tag_intent_ids[i, j] != self.pad_token_label_id:\n",
    "                        out_tag_intent_list[i].append(intent_token_map[out_tag_intent_ids[i][j]])\n",
    "                        tag_intent_preds_list[i].append(intent_token_map[tag_intent_preds[i][j]])\n",
    "\n",
    "        #######################################################################3\n",
    "        # # eval_referee_token_preds_list = [['B-referee' if logit > BINARY_THRESHOLD else 'O' for logit in sample] for sample in only_referee_token_preds]\n",
    "        # # eval_out_referee_token_list = [['B-referee' if label == 1 else 'O' for label in sample] for sample in only_out_referee_token_ids]\n",
    "        #\n",
    "        # eval_referee_token_preds_list = [[label for label in sample] for sample in referee_token_preds_list]\n",
    "        # eval_out_referee_token_list = [[label for label in sample] for sample in out_referee_token_list]\n",
    "        #\n",
    "        # # print('eval_referee_token_preds_list shape: ',len(eval_referee_token_preds_list), len(eval_referee_token_preds_list[0]))\n",
    "        # # print('slot_preds_list shape: ',len(slot_preds_list), len(slot_preds_list[0]))\n",
    "        #\n",
    "        #\n",
    "        # print('eval_referee_token_preds_list[0]:', eval_referee_token_preds_list[0])\n",
    "        # print('eval_out_referee_token_list[0]:', eval_out_referee_token_list[0],'\\n')\n",
    "        # #\n",
    "        # # print('eval_referee_token_preds_list[1]:', eval_referee_token_preds_list[1])\n",
    "        # # print('eval_out_referee_token_list[1]:', eval_out_referee_token_list[1])\n",
    "        #############################################################3\n",
    "\n",
    "# referee_preds_list = [[] for _ in range(out_referee_labels_ids.shape[0])]\n",
    "#             out_referee_label_list = [[] for _ in range(out_referee_labels_ids.shape[0])]\n",
    "#             all_referee_preds_list = [[] for _ in range(all_out_referee_labels_ids.shape[0])]\n",
    "#             all_out_referee_label_list\n",
    "\n",
    "\n",
    "        # print('all_referee_preds_list len: ',len(all_referee_preds_list))\n",
    "        # print('all_out_referee_label_list len: ',len(all_out_referee_label_list))\n",
    "\n",
    "        total_result = compute_metrics_multi_intent_Pro(intent_preds,\n",
    "                                       out_intent_label_ids,\n",
    "                                       slot_preds_list,\n",
    "                                       out_slot_label_list,\n",
    "                                       intent_token_preds_list,\n",
    "                                       out_intent_token_list,\n",
    "                                       tag_intent_preds_list,\n",
    "                                       out_tag_intent_list,\n",
    "                                       referee_preds_list,\n",
    "                                       out_referee_label_list\n",
    "                                      )\n",
    "        results.update(total_result)\n",
    "\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Pronoun Acc !!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        correct = 0\n",
    "        for ref_pred_seq,ref_label_seq in zip(referee_preds_list,out_referee_label_list):\n",
    "            if ref_pred_seq == ref_label_seq:\n",
    "                correct += 1\n",
    "        ref_acc = correct/len(referee_preds_list)\n",
    "        print('Pronoun Accurac: ',ref_acc, ', correct: ',correct, ', total: ',len(referee_preds_list))\n",
    "        results.update({'Pronoun Accuracy':ref_acc})\n",
    "\n",
    "        #-----------------------------------------------------\n",
    "\n",
    "        # tp = 0\n",
    "        # fn = 0\n",
    "        # fp = 0\n",
    "        # tn = 0\n",
    "        # for sample_idx,has_pro in enumerate(pro_sample_mask_np):\n",
    "        #     if has_pro:\n",
    "        #         if referee_token_preds_list[sample_idx] == out_referee_token_list[sample_idx]:\n",
    "        #             tp += 1\n",
    "        #         elif all(referee_token_preds_list[sample_idx] == 'O'): #referee not detected\n",
    "        #             fn += 1\n",
    "        #         else:\n",
    "        #             fp += 1 #detected the wrong referee\n",
    "        #\n",
    "        # ref_acc = tp/sum(pro_sample_mask_np)\n",
    "        # results.update({'Pronoun Accuracy':ref_acc})\n",
    "\n",
    "        #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"  %s_%s = %s\", mode, key, str(results[key]))\n",
    "        \n",
    "        #self.store_pred(slot_preds_list,intent_token_preds_list)\n",
    "        self.slot_preds_list = slot_preds_list\n",
    "        self.intent_token_preds_list = intent_token_preds_list\n",
    "        return results\n",
    "\n",
    "    def save_model(self):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            os.makedirs(self.args.model_dir)\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        model_to_save.save_pretrained(self.args.model_dir)\n",
    "\n",
    "        # Save training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(self.args.model_dir, 'training_args.bin'))\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
    "\n",
    "    def load_model(self):\n",
    "        # Check whether model exists\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            raise Exception(\"Model doesn't exists! Train first!\")\n",
    "\n",
    "        try:\n",
    "            self.model = self.model_class.from_pretrained(self.args.model_dir,\n",
    "                                                          args=self.args,\n",
    "                                                          intent_label_lst=self.intent_label_lst,\n",
    "                                                          slot_label_lst=self.slot_label_lst)\n",
    "            self.model.to(self.device)\n",
    "            logger.info(\"***** Model Loaded *****\")\n",
    "        except:\n",
    "            raise Exception(\"Some model files might be missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "FLAG = False\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "#from trainer import Trainer, Trainer_multi, Trainer_woISeq\n",
    "from utils import init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "from data_loader import load_and_cache_examples, processors\n",
    "\n",
    "def init_logger():\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "\n",
    "def main(args):\n",
    "#     init_logger(args)\n",
    "#     init_logger()\n",
    "\n",
    "    set_seed(args)\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "    dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "    test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "    \n",
    "    if args.multi_intent == 1:\n",
    "        trainer = Trainer_multi(args, train_dataset, dev_dataset, test_dataset)\n",
    "    else:\n",
    "        trainer = Trainer(args, train_dataset, dev_dataset, test_dataset)\n",
    "    if args.do_train:\n",
    "        trainer.train()\n",
    "    if args.do_eval:\n",
    "        trainer.load_model()\n",
    "        trainer.evaluate(\"test\")\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_wait = random.uniform(0, 10)\n",
    "    time.sleep(time_wait)\n",
    "    parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--task\", default='mixsnips', required=True, type=str, help=\"The name of the task to train\")\n",
    "    parser.add_argument(\"--task\", default='gpsr_pro_instance', type=str, help=\"The name of the task to train\")\n",
    "\n",
    "#     parser.add_argument(\"--model_dir\", default='./gpsr_model', required=True, type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--model_dir\", default='./gpsr_pro_instance_model', type=str, help=\"Path to save, load model\")\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str, help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "#     parser.add_argument(\"--intent_seq\", type=int, default=0, help=\"whether we use intent seq setting\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--pro\", type=int, default=1, help=\"support pronoun disambiguition\")#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--multi_intent\", type=int, default=1, help=\"whether we use multi intent setting\")\n",
    "    parser.add_argument(\"--tag_intent\", type=int, default=1, help=\"whether we can use tag to predict intent\")\n",
    "    \n",
    "    parser.add_argument(\"--BI_tag\", type=int, default=1, help='use BI sum or just B')\n",
    "    parser.add_argument(\"--cls_token_cat\", type=int, default=1, help='whether we cat the cls to the slot output of bert')\n",
    "    parser.add_argument(\"--intent_attn\", type=int, default=1, help='whether we use attention mechanism on the CLS intent output')\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "                                           #max slot num = 7\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--seed', type=int, default=25, help=\"random seed for initialization\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "#     parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "\n",
    "    parser.add_argument(\"--eval_batch_size\", default=128, type=int, help=\"Batch size for evaluation.\")\n",
    "    parser.add_argument(\"--max_seq_len\", default=32, type=int, help=\"The maximum total input sequence length after tokenization.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", default=10.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=4.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "                                            #####\n",
    "    \n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
    "    parser.add_argument('--logging_steps', type=int, default=500, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument('--save_steps', type=int, default=300, help=\"Save checkpoint every X updates steps.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "    parser.add_argument('--slot_loss_coef', type=float, default=2.0, help='Coefficient for the slot loss.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--pro_loss_coef', type=float, default=10.0, help='Coefficient for the pronoun loss.')\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument('--tag_intent_coef', type=float, default=1.0, help='Coefficient for the tag intent loss')\n",
    "\n",
    "    # CRF option\n",
    "    parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
    "    parser.add_argument(\"--slot_pad_label\", default=\"PAD\", type=str, help=\"Pad token for slot label pad (to be ignore when calculate loss)\")\n",
    "    parser.add_argument(\"--patience\", default=0, type=int, help=\"The initial learning rate for Adam.\")\n",
    "    \n",
    "    parser.add_argument('-f')#########################\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    now = datetime.now()\n",
    "    args.model_dir = args.model_dir + '_' + now.strftime('%m-%d-%H:%M:%S')\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "\n",
    "tokenizer = load_tokenizer(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "500 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/configuration_utils.py:601\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    599\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 601\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_path\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError:\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/utils/hub.py:284\u001B[0m, in \u001B[0;36mcached_path\u001B[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_url(url_or_filename):\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001B[39;00m\n\u001B[0;32m--> 284\u001B[0m     output_path \u001B[38;5;241m=\u001B[39m \u001B[43mget_from_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_or_filename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(url_or_filename):\n\u001B[1;32m    295\u001B[0m     \u001B[38;5;66;03m# File, and it exists.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/utils/hub.py:495\u001B[0m, in \u001B[0;36mget_from_cache\u001B[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001B[0m\n\u001B[1;32m    494\u001B[0m r \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mhead(url, headers\u001B[38;5;241m=\u001B[39mheaders, allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, proxies\u001B[38;5;241m=\u001B[39mproxies, timeout\u001B[38;5;241m=\u001B[39metag_timeout)\n\u001B[0;32m--> 495\u001B[0m \u001B[43m_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    496\u001B[0m etag \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX-Linked-Etag\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m r\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mETag\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/utils/hub.py:417\u001B[0m, in \u001B[0;36m_raise_for_status\u001B[0;34m(response)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m401\u001B[39m:\n\u001B[1;32m    416\u001B[0m     \u001B[38;5;66;03m# The repo was not found and the user is not Authenticated\u001B[39;00m\n\u001B[0;32m--> 417\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(\n\u001B[1;32m    418\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m401 Client Error: Repository not found for url: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    419\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf the repo is private, make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    420\u001B[0m     )\n\u001B[1;32m    422\u001B[0m response\u001B[38;5;241m.\u001B[39mraise_for_status()\n",
      "\u001B[0;31mRepositoryNotFoundError\u001B[0m: 401 Client Error: Repository not found for url: https://huggingface.co/500/resolve/main/config.json. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m dev_dataset \u001B[38;5;241m=\u001B[39m load_and_cache_examples(args, tokenizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m load_and_cache_examples(args, tokenizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer_multi\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdev_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "Cell \u001B[0;32mIn[2], line 35\u001B[0m, in \u001B[0;36mTrainer_multi.__init__\u001B[0;34m(self, args, train_dataset, dev_dataset, test_dataset)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(args\u001B[38;5;241m.\u001B[39mmodel_name_or_path, finetuning_task\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mtask)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m500\u001B[39m\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mintent_label_lst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintent_label_lst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mslot_label_lst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mslot_label_lst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# GPU or CPU\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args\u001B[38;5;241m.\u001B[39mno_cuda \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/modeling_utils.py:1922\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PretrainedConfig):\n\u001B[1;32m   1921\u001B[0m     config_path \u001B[38;5;241m=\u001B[39m config \u001B[38;5;28;01mif\u001B[39;00m config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m pretrained_model_name_or_path\n\u001B[0;32m-> 1922\u001B[0m     config, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_unused_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1927\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1928\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1929\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1930\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1931\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1932\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_from_auto\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_auto_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1933\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_from_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_pipeline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1934\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1935\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1936\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1937\u001B[0m     model_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/configuration_utils.py:526\u001B[0m, in \u001B[0;36mPretrainedConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_pretrained\u001B[39m(\u001B[38;5;28mcls\u001B[39m, pretrained_model_name_or_path: Union[\u001B[38;5;28mstr\u001B[39m, os\u001B[38;5;241m.\u001B[39mPathLike], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPretrainedConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    524\u001B[0m \u001B[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001B[39;00m\n\u001B[1;32m    525\u001B[0m \u001B[38;5;124;03m    ```\"\"\"\u001B[39;00m\n\u001B[0;32m--> 526\u001B[0m     config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    527\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_type:\n\u001B[1;32m    528\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    529\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are using a model of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to instantiate a model of type \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    530\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    531\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/configuration_utils.py:553\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    551\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    552\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 553\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_config_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;66;03m# That config file may point us toward another config file to use.\u001B[39;00m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfiguration_files\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict:\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/configuration_utils.py:613\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    601\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m cached_path(\n\u001B[1;32m    602\u001B[0m         config_file,\n\u001B[1;32m    603\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    609\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    610\u001B[0m     )\n\u001B[1;32m    612\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError:\n\u001B[0;32m--> 613\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    614\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier listed on \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    615\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token having \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    616\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpermission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    617\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`use_auth_token=True`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    618\u001B[0m     )\n\u001B[1;32m    619\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError:\n\u001B[1;32m    620\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    621\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists for this \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    622\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel name. Check the model page at \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    623\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavailable revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    624\u001B[0m     )\n",
      "\u001B[0;31mOSError\u001B[0m: 500 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "trainer = Trainer_multi(args, train_dataset, dev_dataset, test_dataset)\n",
    "trainer.train()\n",
    "# test_dataset[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate(\"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jointbert",
   "language": "python",
   "display_name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
