{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from transformers.modeling_bert import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "# import torchvision.transforms.functional as TF\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import argparse\n",
    "from utils import init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP, get_intent_labels, get_slot_labels, compute_metrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# from torchcrf import CRF\n",
    "from TorchCRF import CRF\n",
    "from module import IntentClassifier, SlotClassifier, IntentTokenClassifier, MultiIntentClassifier, TagIntentClassifier\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compute_metrics([['B-obj','B-sour']],[['B-obj','B-sour']],[['B-obj','B-sour']],[['B-obj','B-sour']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    time_wait = random.uniform(0, 10)\n",
    "    time.sleep(time_wait)\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--task\", default='mixatis', type=str, help=\"The name of the task to train\")\n",
    "\n",
    "#     parser.add_argument(\"--model_dir\", default='./gpsr_model', required=True, type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--model_dir\", default='./mixatis_model', type=str, help=\"Path to save, load model\")\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str, help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "#     parser.add_argument(\"--intent_seq\", type=int, default=0, help=\"whether we use intent seq setting\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "    parser.add_argument(\"--multi_intent\", type=int, default=1, help=\"whether we use multi intent setting\")\n",
    "    parser.add_argument(\"--tag_intent\", type=int, default=1, help=\"whether we can use tag to predict intent\")\n",
    "\n",
    "    parser.add_argument(\"--BI_tag\", type=int, default=1, help='use BI sum or just B')\n",
    "    parser.add_argument(\"--cls_token_cat\", type=int, default=1, help='whether we cat the cls to the slot output of bert')\n",
    "    parser.add_argument(\"--intent_attn\", type=int, default=1, help='whether we use attention mechanism on the CLS intent output')\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "                                           #max slot num = 7\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=25, help=\"random seed for initialization\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=256, type=int, help=\"Batch size for training.\")\n",
    "#     parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "\n",
    "    parser.add_argument(\"--eval_batch_size\", default=128, type=int, help=\"Batch size for evaluation.\")\n",
    "\n",
    "\n",
    "    # gpsr\n",
    "    # parser.add_argument(\"--max_seq_len\", default=32, type=int, help=\"The maximum total input sequence length after tokenization.\")\n",
    "\n",
    "    # SNIPS ATIS\n",
    "    # parser.add_argument(\"--max_seq_len\", default=64, type=int, help=\"The maximum total input sequence length after tokenization.\")#!!!!!!!!!!!! need toadd crop in unet!!!!\n",
    "    parser.add_argument(\"--max_seq_len\", default=50, type=int, help=\"The maximum total input sequence length after tokenization.\")#!!!!!!!!!!!! need toadd crop in unet!!!!\n",
    "\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=1e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "\n",
    "#     parser.add_argument(\"--num_train_epochs\", default=10.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=15.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "                                            #####\n",
    "\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
    "    parser.add_argument('--logging_steps', type=int, default=500, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument('--save_steps', type=int, default=300, help=\"Save checkpoint every X updates steps.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "    parser.add_argument('--slot_loss_coef', type=float, default=2.0, help='Coefficient for the slot loss.')\n",
    "    parser.add_argument('--tag_intent_coef', type=float, default=1.0, help='Coefficient for the tag intent loss')\n",
    "\n",
    "    # CRF option\n",
    "    parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
    "    parser.add_argument(\"--slot_pad_label\", default=\"PAD\", type=str, help=\"Pad token for slot label pad (to be ignore when calculate loss)\")\n",
    "    parser.add_argument(\"--patience\", default=0, type=int, help=\"The initial learning rate for Adam.\")\n",
    "\n",
    "    parser.add_argument('-f')#########################\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n",
    "\n",
    "    tokenizer = load_tokenizer(args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "a[::-1]\n",
    "b = [1,2,3]\n",
    "b[::-1][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "UNet(\n  (downs): ModuleList(\n    (0): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(768, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (1): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(800, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(900, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (2): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(900, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(1000, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (ups): ModuleList(\n    (0): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(2000, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(900, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (1): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(1800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (2): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(1600, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up_conv_trans): ModuleList(\n    (0): ConvTranspose1d(1000, 1000, kernel_size=(7,), stride=(1,))\n    (1): ConvTranspose1d(900, 900, kernel_size=(14,), stride=(1,))\n    (2): ConvTranspose1d(800, 800, kernel_size=(26,), stride=(1,))\n  )\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (bottle_neck): DoubleConv(\n    (conv): Sequential(\n      (0): Conv1d(1000, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n      (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n      (3): Conv1d(1000, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n      (4): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (5): ReLU(inplace=True)\n    )\n  )\n  (final_conv): Conv1d(800, 145, kernel_size=(1,), stride=(1,))\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                      nn.Conv1d(in_channels,out_channels, kernel_size = 4,padding = 'same',bias = False),\n",
    "                      nn.BatchNorm1d(out_channels),\n",
    "                      nn.ReLU(inplace = True),\n",
    "                      nn.Conv1d(out_channels,out_channels, kernel_size = 4,padding = 'same',bias = False),\n",
    "                      nn.BatchNorm1d(out_channels),\n",
    "                      nn.ReLU(inplace = True),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, args, config, num_intent_labels, num_slot_labels, features = [800,900,1000]):\n",
    "        super(UNet, self).__init__()\n",
    "        max_seq_len = args.max_seq_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.features = features\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.up_conv_trans = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool1d(2, return_indices=False)\n",
    "\n",
    "        in_channels = config.hidden_size\n",
    "        out_channels = num_intent_labels + num_slot_labels\n",
    "\n",
    "        ################################ define downs ################################\n",
    "        for feat in features:\n",
    "            self.downs.append(DoubleConv(in_channels,feat))\n",
    "            in_channels = feat\n",
    "\n",
    "        ################################ define BottleNeck ################################\n",
    "        #ks = int(max_seq_len/(2**len(features)) +1)\n",
    "        # self.bottle_neck = nn.ModuleList([DoubleConv(features[-1],features[-1]*2),\n",
    "        #                                  nn.ConvTranspose1d(features[-1]*2,features[-1], kernel_size=ks)])\n",
    "        self.bottle_neck = DoubleConv(features[-1],features[-1])\n",
    "\n",
    "\n",
    "        ################################ define ups ################################\n",
    "        rev = features[::-1]\n",
    "        for i,feat in enumerate(rev):\n",
    "            if i < len(rev)-1:\n",
    "                self.ups.append(DoubleConv(feat*2,rev[i+1]))\n",
    "                # print(feat*2,' ',rev[i+1])\n",
    "            else:\n",
    "                self.ups.append(DoubleConv(feat*2,feat))\n",
    "\n",
    "        #---------------------------- function for a perfect conv transpose kernel size, so no need to crop or pad ----------------------------\n",
    "        #conv_trans_ks = [int(self.max_seq_len/(2**i) +1) for i in range(1,len(features)+1)][::-1] # [9,17,33] for max_seq_len = 64\n",
    "        lens = [max_seq_len]\n",
    "        for i in range(len(features)):\n",
    "            lens.append(lens[-1]//2) #floor, cuz max pool will floor\n",
    "\n",
    "        conv_trans_ks = []\n",
    "        cur_len = lens.pop()\n",
    "        for l in lens[::-1]:\n",
    "            conv_trans_ks.append(l - cur_len + 1)\n",
    "            cur_len = l\n",
    "        #-------------------------------------------------- --------------------------------------------------------------------------------\n",
    "\n",
    "        for i,ks in enumerate(conv_trans_ks):\n",
    "            feat = features[::-1][i]\n",
    "            self.up_conv_trans.append(nn.ConvTranspose1d(feat,feat, kernel_size=ks))\n",
    "            # print(f'in: {feat}, out: {feat}')\n",
    "\n",
    "        ################################ final convs map to 1hot outputs ################################\n",
    "        self.final_conv = nn.Conv1d(features[0],out_channels, kernel_size = 1)\n",
    "\n",
    "\n",
    "    def forward(self,embeded):\n",
    "        x = embeded.permute(0,2,1)\n",
    "        skip_connections = []\n",
    "        #print('-------------------------------- downs --------------------------------')\n",
    "        for i,down in enumerate(self.downs):\n",
    "            #print(f'{i}: {x.shape}')\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        #print('-------------------------------- bottle neck --------------------------------')\n",
    "\n",
    "        x = self.bottle_neck(x)\n",
    "        # print(f'after bottle neck x shape: {x.shape}\\n')\n",
    "        # print('-------------------------------- ups --------------------------------')\n",
    "\n",
    "\n",
    "        for idx,up in enumerate(self.ups):\n",
    "            x = self.up_conv_trans[idx](x)\n",
    "            # print(f'upsampling x shape: {x.shape}')\n",
    "\n",
    "            sc = skip_connections.pop()\n",
    "            # print(f'poped sc shape: {sc.shape}')\n",
    "\n",
    "            x = torch.cat((sc,x),dim = 1)\n",
    "            # print(f'cat shape: {x.shape}')\n",
    "            x = up(x)\n",
    "            # print(f'after double conv x shape: {x.shape}')\n",
    "\n",
    "\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "config_class, model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "num_intent_labels = len(get_intent_labels(args))\n",
    "num_slot_labels = len(get_slot_labels(args))\n",
    "u = UNet(args,config, num_intent_labels, num_slot_labels, features = [800,900,1000])\n",
    "u"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, config, num_intent_labels, num_slot_labels):\n",
    "#         super().__init__()\n",
    "#         self.embedding_dim = config.hidden_size\n",
    "#         self.num_intent_labels = num_intent_labels\n",
    "#         self.num_slot_labels = num_slot_labels\n",
    "#\n",
    "#         self.down_layers = nn.ModuleList([nn.Conv1d(in_channels = self.embedding_dim,out_channels = 800, kernel_size = 12,padding='same'),\n",
    "#                                          nn.Conv1d(in_channels = 800,out_channels = 900, kernel_size = 4,padding='same'),\n",
    "#                                          nn.Conv1d(in_channels = 900,out_channels = 1024, kernel_size = 3,padding='same')])\n",
    "#\n",
    "#         self.bottle_neck = nn.Conv1d(in_channels = 1024,out_channels = 1024, kernel_size = 3, padding='same')\n",
    "#\n",
    "#\n",
    "#         self.up_layers = nn.ModuleList([nn.Conv1d(in_channels = 1024,out_channels = 900, kernel_size = 3, padding='same'),\n",
    "#                                          nn.Conv1d(in_channels = 900*2,out_channels = 800, kernel_size = 4,padding='same'),\n",
    "#                                          nn.Conv1d(in_channels = 800*2,out_channels = self.embedding_dim, kernel_size =12,padding='same')])\n",
    "#\n",
    "#         self.conv_to_17 = nn.ModuleList([nn.Conv1d(in_channels = self.embedding_dim,out_channels = 256, kernel_size = 8,padding='same'),\n",
    "#                                          nn.Conv1d(in_channels = 256,out_channels = num_intent_labels + num_slot_labels, kernel_size = 4,padding='same')])\n",
    "#\n",
    "#         self.pool = nn.MaxPool1d(2, return_indices=False)\n",
    "#         self.unpool = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#\n",
    "#\n",
    "#     def forward(self, embeded):\n",
    "#         x = embeded.permute(0,2,1)\n",
    "#         #print('permute size: ',embeded.size())\n",
    "#         skip_connections = []\n",
    "#         indices = []\n",
    "#         for i, layer in enumerate(self.down_layers):\n",
    "#             x = F.relu(layer(x))\n",
    "#             # print('x shape: ', x.size())\n",
    "#\n",
    "#             if i < 2:\n",
    "#                 skip_connections.append(x)\n",
    "#                 x = self.pool(x)\n",
    "#                 # print('small x size: ',x.size())\n",
    "#         # print('-------------------------------------------------')\n",
    "#         # bottle neck\n",
    "#         x = F.relu(self.bottle_neck(x))\n",
    "#         #print('-------------------------------------------------')\n",
    "#         for i, layer in enumerate(self.up_layers):\n",
    "#             if i > 0:\n",
    "#                 x = self.unpool(x)\n",
    "#                 # print('large x size: ',x.size())\n",
    "#                 sc = skip_connections.pop()\n",
    "#                 #print('pop size: ', sc.size())\n",
    "#                 #print('curr x size: ', x.size())\n",
    "#                 x = torch.cat((sc,x),dim = 1)\n",
    "#                 # print('!cat size: ',x.size())\n",
    "#             x = F.relu(layer(x))\n",
    "#             # if i < len(self.up_layers) - 1: x = F.relu(x)\n",
    "#\n",
    "#         for i, layer in enumerate(self.conv_to_17):\n",
    "#             x = layer(x)\n",
    "#             if i < len(self.conv_to_17) - 1: x = F.relu(x)\n",
    "#         return (x)\n",
    "#\n",
    "#\n",
    "# config_class, model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "# config = config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "# num_intent_labels = len(get_intent_labels(args))\n",
    "# num_slot_labels = len(get_slot_labels(args))\n",
    "# unet = UNet(config, num_intent_labels, num_slot_labels)\n",
    "# unet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Unet Archetecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from data_loader import load_and_cache_examples\n",
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# from tqdm import tqdm, trange\n",
    "# import numpy as np\n",
    "#\n",
    "# train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "# # dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "# # test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "# train_sampler = RandomSampler(train_dataset)\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "# train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "#\n",
    "# config_class, model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "# config = config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "#\n",
    "# bert = BertModel(config=config)\n",
    "# bert.to('cuda')\n",
    "# for _ in train_iterator:\n",
    "#     #epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False )\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         batch = tuple(t.to('cuda') for t in batch)  # GPU or CPU\n",
    "#\n",
    "#         inputs = {'input_ids': batch[0],\n",
    "#                   'attention_mask': batch[1],\n",
    "#                   'token_type_ids' : batch[2]}#,\n",
    "#\n",
    "#         outputs = bert(**inputs)\n",
    "#\n",
    "#         # B * L * D: [64, 35, 768]\n",
    "#         sequence_output = outputs[0]\n",
    "#         #print(sequence_output.size())\n",
    "#         # B * D\n",
    "#         pooled_output = outputs[1]  # [CLS]\n",
    "#\n",
    "#         #unet.to('cuda')\n",
    "#         u = UNet(args,config, num_intent_labels, num_slot_labels, features = [800,900,1000]).to('cuda')\n",
    "#         out = u(sequence_output)\n",
    "#         print(out.size())\n",
    "#         break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('sentence level [:,0,:] ',out[:,0,:].size())\n",
    "# print('token level CE [:,:,0] ',out[:,:,0].size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "JSF_Seg(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (unet): UNet(\n    (downs): ModuleList(\n      (0): DoubleConv(\n        (conv): Sequential(\n          (0): Conv1d(768, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv1d(800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (4): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n      (1): DoubleConv(\n        (conv): Sequential(\n          (0): Conv1d(800, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (1): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv1d(900, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (4): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n      (2): DoubleConv(\n        (conv): Sequential(\n          (0): Conv1d(900, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv1d(1000, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (4): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (ups): ModuleList(\n      (0): DoubleConv(\n        (conv): Sequential(\n          (0): Conv1d(2000, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (1): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv1d(900, 900, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (4): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n      (1): DoubleConv(\n        (conv): Sequential(\n          (0): Conv1d(1800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv1d(800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (4): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n      (2): DoubleConv(\n        (conv): Sequential(\n          (0): Conv1d(1600, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv1d(800, 800, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n          (4): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n    (up_conv_trans): ModuleList(\n      (0): ConvTranspose1d(1000, 1000, kernel_size=(7,), stride=(1,))\n      (1): ConvTranspose1d(900, 900, kernel_size=(14,), stride=(1,))\n      (2): ConvTranspose1d(800, 800, kernel_size=(26,), stride=(1,))\n    )\n    (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (bottle_neck): DoubleConv(\n      (conv): Sequential(\n        (0): Conv1d(1000, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv1d(1000, 1000, kernel_size=(4,), stride=(1,), padding=same, bias=False)\n        (4): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n    (final_conv): Conv1d(800, 145, kernel_size=(1,), stride=(1,))\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_class, model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "\n",
    "class JSF_Seg(nn.Module):\n",
    "    def __init__(self, args,config, num_intent_labels, num_slot_labels, froze_last = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel(config=config)\n",
    "        self.unet = UNet(args,config, num_intent_labels, num_slot_labels)\n",
    "\n",
    "        if froze_last:\n",
    "            for name, param in self.bert.named_parameters():\n",
    "                if name.startswith(\"encoder.layer.11\"): # choose whatever you like here\n",
    "                    param.requires_grad = True\n",
    "                elif name.startswith(\"encoder.layer.10\"):\n",
    "                    param.requires_grad = True\n",
    "                elif name.startswith(\"encoder.layer.9\"):\n",
    "                    param.requires_grad = True\n",
    "                # elif name.startswith(\"encoder.layer.8\"):\n",
    "                #     param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self,**inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        sequence_output = outputs[0]\n",
    "        out = self.unet(sequence_output)\n",
    "        return out\n",
    "\n",
    "num_intent_labels = len(get_intent_labels(args))\n",
    "num_slot_labels = len(get_slot_labels(args))\n",
    "model = JSF_Seg(args,config, num_intent_labels, num_slot_labels)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Training Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/torch/nn/modules/conv.py:309: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:895.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "\n",
      "Iteration:   1%|▏         | 1/71 [00:01<01:59,  1.71s/it]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:02<01:19,  1.15s/it]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:03<01:05,  1.03it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:59,  1.13it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:04<00:55,  1.19it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:05<00:52,  1.23it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:06<00:51,  1.25it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:07<00:49,  1.27it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.29it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:08<00:47,  1.29it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:09<00:46,  1.30it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:10<00:45,  1.30it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:44,  1.30it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:11<00:43,  1.31it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:12<00:42,  1.31it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:13<00:41,  1.31it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:41,  1.31it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:40,  1.31it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:15<00:39,  1.31it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:16<00:38,  1.31it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.31it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:37,  1.31it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:18<00:36,  1.31it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:19<00:35,  1.31it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.31it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:34,  1.31it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:33,  1.31it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:22<00:32,  1.31it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:23<00:31,  1.31it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.31it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:30,  1.31it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:29,  1.31it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:26<00:28,  1.31it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.31it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:27,  1.31it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:26,  1.31it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:29<00:25,  1.31it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.31it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.31it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:23,  1.31it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:22,  1.31it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.31it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.31it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:20,  1.31it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:19,  1.31it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.31it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.31it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.31it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:16,  1.31it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.31it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.31it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.31it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:13,  1.31it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.31it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.31it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.31it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.31it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:09,  1.31it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.31it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.31it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.31it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:06,  1.31it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.31it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.31it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.30it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.30it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.31it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:52<00:02,  1.30it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.30it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.30it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.30it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 1 last batch loss: 50.274295806884766\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.45it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.40it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.38it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.38it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.40it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.42it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.52it/s]\u001B[A\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: UNK seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_capacity seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_flight seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_quantity seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_city seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_restriction seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_distance seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_ground_fare seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_airfare seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_airline seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_ground_service seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_aircraft seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_airport seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_abbreviation seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_flight_time seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_meal seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_cheapest seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: atis_flight_no seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch:   7%|▋         | 1/15 [00:57<13:19, 57.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(57.0652, device='cuda:0'), 'intent_precision': 0.4679778933680104, 'intent_recall': 0.459463772741781, 'intent_f1': 0.46368175229505554, 'slot_precision': 0.6079456351280711, 'slot_recall': 0.5184249628528975, 'slot_f1': 0.5596278771352956, 'sementic_frame_acc': 0.005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:51,  1.35it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:52,  1.32it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:51,  1.31it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:51,  1.31it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:50,  1.31it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:49,  1.31it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:48,  1.31it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:48,  1.31it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:06<00:47,  1.30it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:46,  1.31it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:45,  1.31it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:45,  1.29it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:09<00:44,  1.29it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.29it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.30it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.30it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:41,  1.30it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:13<00:40,  1.30it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:39,  1.30it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.30it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.30it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:16<00:37,  1.30it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:36,  1.30it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.30it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.30it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:19<00:34,  1.30it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:20<00:33,  1.30it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:32,  1.30it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.30it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.30it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:23<00:30,  1.30it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:29,  1.30it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.30it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.30it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:26<00:27,  1.30it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:27<00:26,  1.30it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.30it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.30it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:29<00:24,  1.30it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:30<00:23,  1.30it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.30it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.30it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.30it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:33<00:20,  1.30it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:34<00:19,  1.30it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.30it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.30it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:36<00:17,  1.30it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:37<00:16,  1.30it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.30it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.30it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:39<00:14,  1.30it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:40<00:13,  1.30it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:41<00:13,  1.30it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.30it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.30it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:43<00:10,  1.30it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:44<00:10,  1.30it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.30it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.30it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:46<00:07,  1.30it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:47<00:06,  1.30it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:48<00:06,  1.30it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.30it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:49<00:04,  1.30it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:50<00:03,  1.30it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:51<00:03,  1.30it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:52<00:02,  1.30it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.30it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:53<00:00,  1.30it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.31it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 2 last batch loss: 9.928638458251953\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.38it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.37it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.38it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.41it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.38it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.39it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.50it/s]\u001B[A\n",
      "Epoch:  13%|█▎        | 2/15 [01:53<12:17, 56.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(19.1764, device='cuda:0'), 'intent_precision': 0.7346124328472755, 'intent_recall': 0.7638046600702202, 'intent_f1': 0.7489241843361238, 'slot_precision': 0.8140659340659341, 'slot_recall': 0.82555720653789, 'slot_f1': 0.8197713021025452, 'sementic_frame_acc': 0.226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:51,  1.35it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:52,  1.31it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.31it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:51,  1.30it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:50,  1.30it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:49,  1.30it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.30it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:48,  1.30it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:06<00:47,  1.30it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.30it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.30it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:45,  1.30it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:09<00:44,  1.30it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:43,  1.30it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.29it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:41,  1.29it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:13<00:41,  1.29it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.29it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.29it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.29it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:16<00:37,  1.29it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.29it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.29it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.30it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:34,  1.30it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:20<00:33,  1.30it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.29it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.29it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.29it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:23<00:30,  1.30it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.29it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.30it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.30it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:27,  1.30it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:27<00:27,  1.29it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.29it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.29it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.29it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:30<00:23,  1.30it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.29it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.29it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.29it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:33<00:20,  1.29it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:34<00:20,  1.29it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.29it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.29it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.29it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:37<00:16,  1.29it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.29it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.29it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.29it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:40<00:13,  1.29it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:41<00:13,  1.29it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.29it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.29it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.29it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:44<00:10,  1.29it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.29it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.29it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.29it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:47<00:06,  1.29it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:48<00:06,  1.29it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.29it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.29it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:50<00:03,  1.29it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:51<00:03,  1.29it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:52<00:02,  1.29it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.29it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.29it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.31it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 3 last batch loss: 2.8058245182037354\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.35it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.35it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.35it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.38it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.38it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.36it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.47it/s]\u001B[A\n",
      "Epoch:  20%|██        | 3/15 [02:50<11:21, 56.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(11.5543, device='cuda:0'), 'intent_precision': 0.7971186692410516, 'intent_recall': 0.8565272901372486, 'intent_f1': 0.8257558273713362, 'slot_precision': 0.8373132244099986, 'slot_recall': 0.8909361069836553, 'slot_f1': 0.8632927794975164, 'sementic_frame_acc': 0.338}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.34it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:52,  1.31it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.30it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:51,  1.30it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:50,  1.30it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.30it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.29it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:48,  1.29it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:06<00:47,  1.29it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.29it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.29it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:45,  1.29it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:44,  1.29it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.29it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.29it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.29it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:41,  1.29it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:13<00:41,  1.29it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.29it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.29it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.29it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:37,  1.29it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.29it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.29it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.29it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:34,  1.29it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:20<00:34,  1.29it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.29it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.29it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.29it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:23<00:31,  1.29it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.29it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.29it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.29it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:27,  1.29it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:27<00:27,  1.29it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.29it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.29it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.29it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:30<00:24,  1.29it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.29it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.29it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.29it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:20,  1.29it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:34<00:20,  1.29it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.29it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.29it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:13,  1.29it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:41<00:13,  1.29it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.29it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.29it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.29it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:44<00:10,  1.29it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.29it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.29it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.29it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:06,  1.29it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:48<00:06,  1.29it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.29it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.29it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.29it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:51<00:03,  1.29it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:52<00:02,  1.29it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.29it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.29it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.30it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 4 last batch loss: 1.193660020828247\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.38it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.33it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.34it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.36it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.35it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.45it/s]\u001B[A\n",
      "Epoch:  27%|██▋       | 4/15 [03:47<10:25, 56.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(9.2126, device='cuda:0'), 'intent_precision': 0.8594160358411864, 'intent_recall': 0.8878072135333546, 'intent_f1': 0.8733809561190046, 'slot_precision': 0.9013186494710912, 'slot_recall': 0.924219910846954, 'slot_f1': 0.9126256327488813, 'sementic_frame_acc': 0.418}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.33it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.30it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.30it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:51,  1.29it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.29it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.29it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.29it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:48,  1.29it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:06<00:48,  1.29it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.29it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.29it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:45,  1.29it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.29it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.29it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.29it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.29it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:41,  1.29it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:13<00:41,  1.29it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.29it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.29it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.29it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.29it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.29it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.29it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:20<00:34,  1.29it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.29it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.29it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:27<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.29it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.29it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.29it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.29it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.29it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:34<00:20,  1.29it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.29it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.29it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.29it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.29it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.29it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.29it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.29it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:41<00:13,  1.29it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.29it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:48<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.29it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:52<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.29it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.29it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.30it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 5 last batch loss: 1.2861989736557007\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.37it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.27it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.33it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.34it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.44it/s]\u001B[A\n",
      "Epoch:  33%|███▎      | 5/15 [04:44<09:29, 56.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.2338, device='cuda:0'), 'intent_precision': 0.8687888198757764, 'intent_recall': 0.8929141398021067, 'intent_f1': 0.8806862899417598, 'slot_precision': 0.9058565153733529, 'slot_recall': 0.9193164933135215, 'slot_f1': 0.9125368731563422, 'sementic_frame_acc': 0.472}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.33it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.30it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:51,  1.29it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.29it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.29it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.29it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.29it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:06<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:45,  1.29it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.27it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.27it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.29it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.29it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:52<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 6 last batch loss: 0.7234594821929932\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.34it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.29it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.33it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.44it/s]\u001B[A\n",
      "Epoch:  40%|████      | 6/15 [05:41<08:33, 57.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(7.7103, device='cuda:0'), 'intent_precision': 0.8874193040466068, 'intent_recall': 0.8994573890839451, 'intent_f1': 0.8933977966236032, 'slot_precision': 0.9193193486871057, 'slot_recall': 0.9312035661218425, 'slot_f1': 0.9252232966708496, 'sementic_frame_acc': 0.495}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.34it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.30it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:45<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 7 last batch loss: 0.32282665371894836\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.35it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.35it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.44it/s]\u001B[A\n",
      "Epoch:  47%|████▋     | 7/15 [06:39<07:37, 57.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.1820, device='cuda:0'), 'intent_precision': 0.861703760552571, 'intent_recall': 0.8959463772741781, 'intent_f1': 0.8784915108363978, 'slot_precision': 0.9102193995381063, 'slot_recall': 0.936998514115899, 'slot_f1': 0.9234148484404744, 'sementic_frame_acc': 0.461}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.33it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.27it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:52,  1.27it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:51,  1.27it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:31<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:38<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 8 last batch loss: 0.42283034324645996\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.33it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.25it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.30it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.42it/s]\u001B[A\n",
      "Epoch:  53%|█████▎    | 8/15 [07:36<06:40, 57.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.0246, device='cuda:0'), 'intent_precision': 0.8649646262688404, 'intent_recall': 0.8975422917331631, 'intent_f1': 0.880952380952381, 'slot_precision': 0.9206510681586979, 'slot_recall': 0.9413075780089153, 'slot_f1': 0.9308647417529939, 'sementic_frame_acc': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:24<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:49<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 9 last batch loss: 0.14952386915683746\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.33it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.27it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.31it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.43it/s]\u001B[A\n",
      "Epoch:  60%|██████    | 9/15 [08:33<05:43, 57.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(7.6977, device='cuda:0'), 'intent_precision': 0.8736728727496538, 'intent_recall': 0.9061602298116821, 'intent_f1': 0.8896200548374462, 'slot_precision': 0.9204479348458406, 'slot_recall': 0.9404160475482912, 'slot_f1': 0.9303248566808761, 'sementic_frame_acc': 0.491}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:53,  1.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.27it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.27it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:51,  1.27it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:50<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:55<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 10 last batch loss: 0.13450613617897034\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.35it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.26it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.31it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.43it/s]\u001B[A\n",
      "Epoch:  67%|██████▋   | 10/15 [09:31<04:46, 57.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.2944, device='cuda:0'), 'intent_precision': 0.8792511700468019, 'intent_recall': 0.8994573890839451, 'intent_f1': 0.8892395077311456, 'slot_precision': 0.9223343571741992, 'slot_recall': 0.936998514115899, 'slot_f1': 0.929608609125083, 'sementic_frame_acc': 0.476}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:53,  1.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.28it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:50<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 11 last batch loss: 0.06869179010391235\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.29it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.26it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.31it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.43it/s]\u001B[A\n",
      "Epoch:  73%|███████▎  | 11/15 [10:28<03:49, 57.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.6311, device='cuda:0'), 'intent_precision': 0.8767208043310132, 'intent_recall': 0.9045643153526971, 'intent_f1': 0.8904249469798131, 'slot_precision': 0.9202266782911944, 'slot_recall': 0.9410104011887073, 'slot_f1': 0.9305024977960622, 'sementic_frame_acc': 0.482}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.33it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.28it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:42<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:50<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:53<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 12 last batch loss: 0.06021310016512871\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.26it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.31it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.31it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.42it/s]\u001B[A\n",
      "Epoch:  80%|████████  | 12/15 [11:26<02:52, 57.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(7.9786, device='cuda:0'), 'intent_precision': 0.8798449612403101, 'intent_recall': 0.9056814554739866, 'intent_f1': 0.8925762818496381, 'slot_precision': 0.9281122150789012, 'slot_recall': 0.9438335809806835, 'slot_f1': 0.9359068808015323, 'sementic_frame_acc': 0.486}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:52,  1.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.29it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:17<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:32,  1.28it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.28it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:30,  1.28it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:28<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:35<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:15,  1.26it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.27it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.27it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:43<00:12,  1.27it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.27it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:46<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:50<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.27it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:54<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:55<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 13 last batch loss: 0.05118600279092789\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.32it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.30it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.32it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.33it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.43it/s]\u001B[A\n",
      "Epoch:  87%|████████▋ | 13/15 [12:24<01:54, 57.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.3408, device='cuda:0'), 'intent_precision': 0.8769278223318939, 'intent_recall': 0.9074369613788701, 'intent_f1': 0.8919215686274511, 'slot_precision': 0.9249599300597406, 'slot_recall': 0.9432392273402674, 'slot_f1': 0.9340101522842639, 'sementic_frame_acc': 0.484}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:53,  1.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:53,  1.28it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.28it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:50,  1.27it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.27it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.27it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.27it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:47,  1.27it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.27it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.27it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.27it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.27it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.28it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.28it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:18<00:37,  1.28it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:36,  1.28it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.28it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.28it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.28it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.27it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:32,  1.27it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.27it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:30,  1.27it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.27it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:29,  1.27it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.27it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.27it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:29<00:26,  1.27it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.27it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:25,  1.27it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.27it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.27it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.27it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:22,  1.27it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.27it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.27it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:36<00:19,  1.27it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.27it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:18,  1.27it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.27it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.27it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:40<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.27it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:43<00:12,  1.27it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.27it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.27it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.27it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.27it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:47<00:08,  1.27it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.27it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.27it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.27it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:50<00:05,  1.27it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:51<00:04,  1.27it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.27it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.27it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.27it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:54<00:01,  1.27it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.27it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:55<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 14 last batch loss: 0.04770822077989578\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.31it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.25it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.28it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.27it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.28it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.39it/s]\u001B[A\n",
      "Epoch:  93%|█████████▎| 14/15 [13:21<00:57, 57.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.1467, device='cuda:0'), 'intent_precision': 0.8767735965453424, 'intent_recall': 0.9072773699329716, 'intent_f1': 0.8917647058823529, 'slot_precision': 0.9235251274581209, 'slot_recall': 0.9420505200594353, 'slot_f1': 0.9326958440603162, 'sementic_frame_acc': 0.489}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 0/71 [00:00<?, ?it/s]\u001B[A\n",
      "Iteration:   1%|▏         | 1/71 [00:00<00:53,  1.32it/s]\u001B[A\n",
      "Iteration:   3%|▎         | 2/71 [00:01<00:53,  1.29it/s]\u001B[A\n",
      "Iteration:   4%|▍         | 3/71 [00:02<00:53,  1.28it/s]\u001B[A\n",
      "Iteration:   6%|▌         | 4/71 [00:03<00:52,  1.28it/s]\u001B[A\n",
      "Iteration:   7%|▋         | 5/71 [00:03<00:51,  1.28it/s]\u001B[A\n",
      "Iteration:   8%|▊         | 6/71 [00:04<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  10%|▉         | 7/71 [00:05<00:50,  1.28it/s]\u001B[A\n",
      "Iteration:  11%|█▏        | 8/71 [00:06<00:49,  1.28it/s]\u001B[A\n",
      "Iteration:  13%|█▎        | 9/71 [00:07<00:48,  1.28it/s]\u001B[A\n",
      "Iteration:  14%|█▍        | 10/71 [00:07<00:47,  1.28it/s]\u001B[A\n",
      "Iteration:  15%|█▌        | 11/71 [00:08<00:47,  1.27it/s]\u001B[A\n",
      "Iteration:  17%|█▋        | 12/71 [00:09<00:46,  1.28it/s]\u001B[A\n",
      "Iteration:  18%|█▊        | 13/71 [00:10<00:45,  1.28it/s]\u001B[A\n",
      "Iteration:  20%|█▉        | 14/71 [00:10<00:44,  1.28it/s]\u001B[A\n",
      "Iteration:  21%|██        | 15/71 [00:11<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  23%|██▎       | 16/71 [00:12<00:43,  1.28it/s]\u001B[A\n",
      "Iteration:  24%|██▍       | 17/71 [00:13<00:42,  1.27it/s]\u001B[A\n",
      "Iteration:  25%|██▌       | 18/71 [00:14<00:41,  1.28it/s]\u001B[A\n",
      "Iteration:  27%|██▋       | 19/71 [00:14<00:40,  1.28it/s]\u001B[A\n",
      "Iteration:  28%|██▊       | 20/71 [00:15<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  30%|██▉       | 21/71 [00:16<00:39,  1.28it/s]\u001B[A\n",
      "Iteration:  31%|███       | 22/71 [00:17<00:38,  1.26it/s]\u001B[A\n",
      "Iteration:  32%|███▏      | 23/71 [00:18<00:37,  1.26it/s]\u001B[A\n",
      "Iteration:  34%|███▍      | 24/71 [00:18<00:37,  1.27it/s]\u001B[A\n",
      "Iteration:  35%|███▌      | 25/71 [00:19<00:36,  1.27it/s]\u001B[A\n",
      "Iteration:  37%|███▋      | 26/71 [00:20<00:35,  1.27it/s]\u001B[A\n",
      "Iteration:  38%|███▊      | 27/71 [00:21<00:34,  1.27it/s]\u001B[A\n",
      "Iteration:  39%|███▉      | 28/71 [00:21<00:33,  1.27it/s]\u001B[A\n",
      "Iteration:  41%|████      | 29/71 [00:22<00:32,  1.27it/s]\u001B[A\n",
      "Iteration:  42%|████▏     | 30/71 [00:23<00:32,  1.27it/s]\u001B[A\n",
      "Iteration:  44%|████▎     | 31/71 [00:24<00:31,  1.27it/s]\u001B[A\n",
      "Iteration:  45%|████▌     | 32/71 [00:25<00:30,  1.27it/s]\u001B[A\n",
      "Iteration:  46%|████▋     | 33/71 [00:25<00:29,  1.28it/s]\u001B[A\n",
      "Iteration:  48%|████▊     | 34/71 [00:26<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  49%|████▉     | 35/71 [00:27<00:28,  1.28it/s]\u001B[A\n",
      "Iteration:  51%|█████     | 36/71 [00:28<00:27,  1.28it/s]\u001B[A\n",
      "Iteration:  52%|█████▏    | 37/71 [00:29<00:26,  1.28it/s]\u001B[A\n",
      "Iteration:  54%|█████▎    | 38/71 [00:29<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  55%|█████▍    | 39/71 [00:30<00:25,  1.28it/s]\u001B[A\n",
      "Iteration:  56%|█████▋    | 40/71 [00:31<00:24,  1.28it/s]\u001B[A\n",
      "Iteration:  58%|█████▊    | 41/71 [00:32<00:23,  1.28it/s]\u001B[A\n",
      "Iteration:  59%|█████▉    | 42/71 [00:32<00:22,  1.28it/s]\u001B[A\n",
      "Iteration:  61%|██████    | 43/71 [00:33<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  62%|██████▏   | 44/71 [00:34<00:21,  1.28it/s]\u001B[A\n",
      "Iteration:  63%|██████▎   | 45/71 [00:35<00:20,  1.28it/s]\u001B[A\n",
      "Iteration:  65%|██████▍   | 46/71 [00:36<00:19,  1.28it/s]\u001B[A\n",
      "Iteration:  66%|██████▌   | 47/71 [00:36<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  68%|██████▊   | 48/71 [00:37<00:18,  1.28it/s]\u001B[A\n",
      "Iteration:  69%|██████▉   | 49/71 [00:38<00:17,  1.28it/s]\u001B[A\n",
      "Iteration:  70%|███████   | 50/71 [00:39<00:16,  1.28it/s]\u001B[A\n",
      "Iteration:  72%|███████▏  | 51/71 [00:39<00:15,  1.28it/s]\u001B[A\n",
      "Iteration:  73%|███████▎  | 52/71 [00:40<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  75%|███████▍  | 53/71 [00:41<00:14,  1.28it/s]\u001B[A\n",
      "Iteration:  76%|███████▌  | 54/71 [00:42<00:13,  1.28it/s]\u001B[A\n",
      "Iteration:  77%|███████▋  | 55/71 [00:43<00:12,  1.28it/s]\u001B[A\n",
      "Iteration:  79%|███████▉  | 56/71 [00:43<00:11,  1.28it/s]\u001B[A\n",
      "Iteration:  80%|████████  | 57/71 [00:44<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  82%|████████▏ | 58/71 [00:45<00:10,  1.28it/s]\u001B[A\n",
      "Iteration:  83%|████████▎ | 59/71 [00:46<00:09,  1.28it/s]\u001B[A\n",
      "Iteration:  85%|████████▍ | 60/71 [00:47<00:08,  1.28it/s]\u001B[A\n",
      "Iteration:  86%|████████▌ | 61/71 [00:47<00:07,  1.27it/s]\u001B[A\n",
      "Iteration:  87%|████████▋ | 62/71 [00:48<00:07,  1.28it/s]\u001B[A\n",
      "Iteration:  89%|████████▊ | 63/71 [00:49<00:06,  1.28it/s]\u001B[A\n",
      "Iteration:  90%|█████████ | 64/71 [00:50<00:05,  1.28it/s]\u001B[A\n",
      "Iteration:  92%|█████████▏| 65/71 [00:50<00:04,  1.28it/s]\u001B[A\n",
      "Iteration:  93%|█████████▎| 66/71 [00:51<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  94%|█████████▍| 67/71 [00:52<00:03,  1.28it/s]\u001B[A\n",
      "Iteration:  96%|█████████▌| 68/71 [00:53<00:02,  1.28it/s]\u001B[A\n",
      "Iteration:  97%|█████████▋| 69/71 [00:54<00:01,  1.28it/s]\u001B[A\n",
      "Iteration:  99%|█████████▊| 70/71 [00:54<00:00,  1.28it/s]\u001B[A\n",
      "Iteration: 100%|██████████| 71/71 [00:55<00:00,  1.29it/s]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoche 15 last batch loss: 0.050648968666791916\n",
      "---------------------- eval on dev -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001B[A\n",
      "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  4.33it/s]\u001B[A\n",
      "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  4.25it/s]\u001B[A\n",
      "Evaluating:  38%|███▊      | 3/8 [00:00<00:01,  4.29it/s]\u001B[A\n",
      "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating:  62%|██████▎   | 5/8 [00:01<00:00,  4.30it/s]\u001B[A\n",
      "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  4.29it/s]\u001B[A\n",
      "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  4.30it/s]\u001B[A\n",
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.40it/s]\u001B[A\n",
      "Epoch: 100%|██████████| 15/15 [14:19<00:00, 57.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(8.1688, device='cuda:0'), 'intent_precision': 0.8800988875154512, 'intent_recall': 0.9090328758378551, 'intent_f1': 0.894331920238656, 'slot_precision': 0.9246685123123998, 'slot_recall': 0.9429420505200594, 'slot_f1': 0.9337158831751635, 'sementic_frame_acc': 0.499}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1065, 9.777460357259976)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_loader import load_and_cache_examples\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        config_class, model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "        config = config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "\n",
    "        self.intent_dict = {i:label for i,label in enumerate(get_intent_labels(args))}\n",
    "        self.slot_dict = {i:label for i,label in enumerate(get_slot_labels(args))}\n",
    "\n",
    "        self.num_intent_labels = len(self.intent_dict)\n",
    "        self.num_slot_labels = len(self.slot_dict)\n",
    "\n",
    "        self.model = JSF_Seg(self.args,config, self.num_intent_labels, self.num_slot_labels)\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # self.loss_function = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def get_losses(self, outputs,slot_label,intent_label):\n",
    "        loss = 0\n",
    "        for token_idx in range(outputs.shape[2]): # shape[2]: iterate through token level\n",
    "            # true\n",
    "            intent_true = intent_label[:,token_idx]\n",
    "            slot_true = slot_label[:,token_idx]\n",
    "\n",
    "            # preds\n",
    "            intent_pred_one_hot = self.get_token_preds(outputs,token_idx, one_hot = True)['token_intent_pred']\n",
    "            slot_pred_one_hot = self.get_token_preds(outputs,token_idx, one_hot = True)['token_slot_pred']\n",
    "\n",
    "            # seperate losses\n",
    "            intent_loss = self.loss_function(intent_pred_one_hot.float(),intent_true.long())\n",
    "            slot_loss = self.loss_function(slot_pred_one_hot.float(),slot_true.long())\n",
    "\n",
    "            # accumulate them to total loss\n",
    "            loss += intent_loss + slot_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    # get the token level prediction of the slot and\n",
    "    def get_token_preds(self,outputs,token_idx, one_hot = True):\n",
    "        token_pred = outputs[:,:,token_idx] #token_pred in shape: 64x17\n",
    "\n",
    "        # get current token intention one-hot pred\n",
    "        out_intent_pos = self.num_intent_labels\n",
    "        token_intent_pred = token_pred[:,:out_intent_pos]\n",
    "\n",
    "        start_slot_pos = self.num_intent_labels\n",
    "        token_slot_pred = token_pred[:,start_slot_pos:]\n",
    "\n",
    "        if one_hot:\n",
    "            # size of BATCH_SIZE * MAX_SEN_LEN\n",
    "            return {'token_intent_pred':token_intent_pred, 'token_slot_pred':token_slot_pred}\n",
    "\n",
    "        else:\n",
    "            prediction_intent = torch.argmax(token_intent_pred, dim = 1)\n",
    "            prediction_slot = torch.argmax(token_slot_pred, dim = 1)\n",
    "            return {'token_intent_pred':prediction_intent, 'token_slot_pred':prediction_slot}\n",
    "\n",
    "\n",
    "    # get the setence level prediction of all the slots and intents\n",
    "    # output: dict of sentence slot and intent labels lst\n",
    "    def get_sentence_preds(self, outputs, env = 'train'):\n",
    "        # print(outputs[:,:self.num_intent_labels,:].shape)\n",
    "        # print(outputs[:,self.num_intent_labels:,:].shape)\n",
    "        batch_intent_pred = torch.argmax(outputs[:,:self.num_intent_labels,:], dim = 1).detach().cpu().tolist()# size 64 * 32\n",
    "        start_slot_pos = self.num_intent_labels\n",
    "\n",
    "        # print(f'outputs[:,start_slot_pos:,:][0]: {outputs[:,start_slot_pos:,:][0]}')\n",
    "        batch_slot_pred = torch.argmax(outputs[:,start_slot_pos:,:], dim = 1).detach().cpu().tolist()# size 64 * 32\n",
    "        # print(f'batch_slot_pred[0]:  {batch_slot_pred[0]}')\n",
    "        # print(f'batch_slot_pred shape: {len(batch_slot_pred)} x {len(batch_slot_pred[0])}')\n",
    "\n",
    "\n",
    "        intent_pred_labels = [[self.intent_dict[id] for id in sentence] for sentence in batch_intent_pred]\n",
    "        slot_pred_labels = [[self.slot_dict[id] for id in sentence] for sentence in batch_slot_pred]\n",
    "\n",
    "        return {'slot_sentence_pred':slot_pred_labels, 'intent_sentence_pred':intent_pred_labels}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n",
    "\n",
    "        if self.args.max_steps > 0:\n",
    "            t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.args.weight_decay},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
    "        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
    "        logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
    "\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "        for n_ep,_ in enumerate(train_iterator):\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
    "\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'token_type_ids' : batch[2]}\n",
    "\n",
    "                outputs = self.model(**inputs) #outpus size: [64, 17, 32]\n",
    "                ################################ define loss ################################\n",
    "                #could do a token-level CE loss or a sentence mask level IoU or DIce score or fiscal\n",
    "\n",
    "                #intent loss\n",
    "                loss = 0\n",
    "\n",
    "                slot_label = batch[4]\n",
    "                intent_label = batch[5]\n",
    "\n",
    "                # accumulate current loss to total loss\n",
    "                loss += self.get_losses(outputs,slot_label,intent_label)\n",
    "\n",
    "                if self.args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "                #print(f'training loss: {loss}')\n",
    "                tr_loss += loss.item()\n",
    "\n",
    "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
    "                    #     continue\n",
    "                    #     self.evaluate(\"dev\")\n",
    "\n",
    "                    # if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
    "                    #     self.save_model()\n",
    "\n",
    "                if 0 < self.args.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if 0 < self.args.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "            print(f'training epoche {n_ep + 1} last batch loss: {loss.item()}')\n",
    "            print('---------------------- eval on dev -----------------------------')\n",
    "            print(self.evaluate(\"dev\"))\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    ######################################################################## EVAL ########################################################################\n",
    "    def evaluate(self, mode):\n",
    "        if mode == 'test':\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == 'dev':\n",
    "            dataset = self.dev_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"  Num examples = %d\", len(dataset))\n",
    "        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # init pred/true lists\n",
    "        slot_sentence_pred = []\n",
    "        intent_sentence_pred = []\n",
    "        true_slot_label = []\n",
    "        true_intent_label  = []\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids' : batch[2]}\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                slot_label = batch[4]\n",
    "                intent_label = batch[5]\n",
    "                eval_loss += self.get_losses(outputs,slot_label,intent_label)\n",
    "\n",
    "                #print pred labels\n",
    "                sentence_pred_dict = self.get_sentence_preds(outputs, env = 'dev')\n",
    "\n",
    "                slot_sentence_pred.extend(sentence_pred_dict['slot_sentence_pred'])\n",
    "                intent_sentence_pred.extend(sentence_pred_dict['intent_sentence_pred'])\n",
    "\n",
    "                true_slot_label.extend( [[self.slot_dict[id] for id in sentence]for sentence in slot_label.tolist()])\n",
    "                true_intent_label.extend( [[self.intent_dict[id] for id in sentence]for sentence in intent_label.tolist()])\n",
    "\n",
    "                # print(f'true_slot_label: {true_slot_label[0]}')\n",
    "                # print(f'slot_sentence_pred: {slot_sentence_pred[0]}\\n')\n",
    "                #\n",
    "                # print(f'true_intent_label: {true_intent_label[0]}')\n",
    "                # print(f'intent_sentence_pred: {intent_sentence_pred[0]}\\n')\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\n",
    "            \"loss\": eval_loss\n",
    "        }\n",
    "\n",
    "        #string labels\n",
    "        slot_preds_list = slot_sentence_pred\n",
    "        out_slot_label_list = true_slot_label\n",
    "\n",
    "        intent_preds_list = intent_sentence_pred\n",
    "        out_intent_label_list = true_intent_label\n",
    "\n",
    "        # compute metrics\n",
    "        the_results = compute_metrics(intent_preds_list, out_intent_label_list,slot_preds_list, out_slot_label_list)\n",
    "\n",
    "        results.update(the_results)\n",
    "\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(results[key]))\n",
    "\n",
    "        return results\n",
    "\n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "trainer = Trainer(args,  train_dataset, dev_dataset,  test_dataset)\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  4.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'loss': tensor(19.0645, device='cuda:0'),\n 'intent_precision': 0.7514387929693576,\n 'intent_recall': 0.7474856877611017,\n 'intent_f1': 0.7494570276140243,\n 'slot_precision': 0.8734063296835158,\n 'slot_recall': 0.8546895640686922,\n 'slot_f1': 0.8639465875370921,\n 'sementic_frame_acc': 0.237}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate('test')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jointbert",
   "language": "python",
   "display_name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
