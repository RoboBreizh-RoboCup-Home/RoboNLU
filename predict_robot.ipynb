{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "# import qi\n",
    "from onnxruntime import InferenceSession, SessionOptions, get_all_providers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "\n",
    "class slot_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights = load(dir+'/weights.npy')\n",
    "        self.linear_bias = load(dir+'/bias.npy')\n",
    "    def ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights) + self.linear_bias\n",
    "        return x\n",
    "\n",
    "class intent_token_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights = load(dir + '/weights.npy')\n",
    "        self.linear_bias = load(dir + '/bias.npy')\n",
    "    # def ReLU(self,x):\n",
    "    #     return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        # x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights) + self.linear_bias\n",
    "        return x\n",
    "\n",
    "class pro_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights1 = load(dir + '/weights1.npy')\n",
    "        self.linear_bias1 = load(dir + '/bias1.npy')\n",
    "        self.linear_weights2 = load(dir + '/weights2.npy')\n",
    "        self.linear_bias2 = load(dir + '/bias2.npy')\n",
    "    def ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        x = x @ np.transpose(self.linear_weights1) + self.linear_bias1\n",
    "        x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights2) + self.linear_bias2\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "# from onnxruntime_tools import optimizer\n",
    "\n",
    "class CommandProcessor():\n",
    "    def __init__(self,session = None):\n",
    "        self.INTENT_CLASSES = ['PAD','O','B-greet','I-greet','B-guide','I-guide','B-follow','I-follow','B-find','I-find','B-take','I-take','B-go','I-go','B-know','I-know']\n",
    "        self.SLOT_CLASSES = ['PAD','O','B-obj','B-dest','I-sour','I-obj','I-dest','B-per','B-sour','I-per']\n",
    "        self.PRO_CLASSES = ['PAD','O','B-referee']\n",
    "\n",
    "        self.referee_token_map = {i:label for i,label in enumerate(self.PRO_CLASSES)}\n",
    "        self.intent_token_map = {i:label for i,label in enumerate(self.INTENT_CLASSES)}\n",
    "        self.slot_label_map = {i:label for i,label in enumerate(self.SLOT_CLASSES)}\n",
    "\n",
    "        self.max_seq_len = 32\n",
    "        self.pro_lst = ['him','her','it','its']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pad_token_label_id = 0\n",
    "\n",
    "        self.input_text_path = './sample_pred_in.txt'#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.output_file = './outputs'\n",
    "\n",
    "        self.bert_ort_session = self.initONNX('./quantized_models/bert.quantV1.onnx')\n",
    "        # self.slot_classifier_ort_session = self.initONNX('./quantized_models/slot_classifier.quant.onnx')\n",
    "        # self.intent_token_classifier_ort_session = self.initONNX('./quantized_models/intent_token_classifier.quant.onnx')\n",
    "        # self.pro_classifier_ort_session = self.initONNX('./quantized_models/pro_classifier.quant.onnx')\n",
    "\n",
    "    def initONNX(self,path):\n",
    "        start = time.time()\n",
    "        sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "        sess_options.intra_op_num_threads = 1#4\n",
    "        sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL\n",
    "        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "        ort_session  = onnxruntime.InferenceSession(path, sess_options)\n",
    "        print(\"Loading time ONNX: \", time.time() - start)\n",
    "        return ort_session\n",
    "\n",
    "    def read_input_file(self):\n",
    "        with open(self.input_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.readline().strip().split()\n",
    "            # for line in f:\n",
    "            #     line = line.strip()\n",
    "            #     words = line.split()\n",
    "            #     break # I should delete precessed commands!!!!!!!!!!!!!\n",
    "\n",
    "        return words\n",
    "\n",
    "    def convert_input_file_to_dataloader(self,words,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        # Setting based on the current model type\n",
    "        cls_token = tokenizer.cls_token\n",
    "        sep_token = tokenizer.sep_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        tokens = []\n",
    "        slot_label_mask = []\n",
    "        pro_labels_ids = []\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            if word in self.pro_lst:\n",
    "                pro_label = 1\n",
    "            else:\n",
    "                pro_label = 0\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_label_mask.extend([self.pad_token_label_id + 1] + [self.pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            pro_labels_ids.extend([pro_label] + [self.pad_token_label_id] * (len(word_tokens) - 1)) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > self.max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (self.max_seq_len - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(self.max_seq_len - special_tokens_count)]\n",
    "            pro_labels_ids = pro_labels_ids[:(self.max_seq_len - special_tokens_count)] #!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [self.pad_token_label_id]\n",
    "        pro_labels_ids += [self.pad_token_label_id]#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [self.pad_token_label_id] + slot_label_mask\n",
    "        pro_labels_ids = [self.pad_token_label_id] + pro_labels_ids#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = self.max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([self.pad_token_label_id] * padding_length)\n",
    "        pro_labels_ids = pro_labels_ids + ([self.pad_token_label_id] * padding_length)\n",
    "\n",
    "\n",
    "        input_ids = np.array(input_ids)\n",
    "        attention_mask = np.array(attention_mask)\n",
    "        token_type_ids = np.array(token_type_ids)\n",
    "        pro_labels_ids = np.array(pro_labels_ids)\n",
    "        # print('input_ids shape: ',input_ids.shape)\n",
    "        # print('attention_mask shape: ',attention_mask.shape)\n",
    "        # print('token_type_ids shape: ',token_type_ids.shape)\n",
    "        # print('pro_labels_ids shape: ',pro_labels_ids.shape)\n",
    "        sample = {'input_ids':input_ids[None,:], 'attention_mask':attention_mask[None,:], 'token_type_ids': token_type_ids[None,:]}\n",
    "\n",
    "        return sample,slot_label_mask,pro_labels_ids\n",
    "\n",
    "    def predict(self):\n",
    "        lines = self.read_input_file()\n",
    "        sample,slot_label_mask,pro_labels_ids = self.convert_input_file_to_dataloader(lines)\n",
    "        start = time.time()\n",
    "\n",
    "        sequence_output, _ = self.bert_ort_session.run(None, sample)\n",
    "        # ============================= Slot prediction ==============================\n",
    "        slot_classifier = slot_classifier_np('./numpy_para/slot_classifier')\n",
    "        slot_logits = slot_classifier.forward(sequence_output)\n",
    "        # slot_logits = self.slot_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "        slot_preds = np.squeeze(np.array(slot_logits))\n",
    "        slot_preds = np.argmax(slot_preds, axis=1)\n",
    "\n",
    "        # ============================== Intent Token Seq =============================\n",
    "        intent_token_classifier = intent_token_classifier_np('./numpy_para/intent_token_classifier')\n",
    "        intent_token_logits = intent_token_classifier.forward(sequence_output)\n",
    "        # intent_token_logits = self.intent_token_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "        intent_token_preds = np.squeeze(np.array(intent_token_logits))\n",
    "        intent_token_preds = np.argmax(intent_token_preds, axis=1)\n",
    "\n",
    "        # ============================= Pronoun referee prediction ==============================\n",
    "        if any(pro_labels_ids):\n",
    "            sq_sequence_output = np.squeeze(sequence_output)\n",
    "            pro_token = sq_sequence_output[pro_labels_ids == 1]\n",
    "            repeat_pro = np.tile(pro_token,(self.max_seq_len,1))\n",
    "            concated_input = np.concatenate((sq_sequence_output,repeat_pro),axis = 1)[None,:]\n",
    "\n",
    "            pro_classifier = pro_classifier_np('./numpy_para/pro_classifier')\n",
    "            referee_token_logits = pro_classifier.forward(concated_input)\n",
    "            # referee_token_logits = self.pro_classifier_ort_session.run(None, {'concated_input':concated_input})\n",
    "            referee_preds = np.squeeze(np.array(referee_token_logits))\n",
    "            referee_preds = np.argmax(referee_preds, axis=1)\n",
    "\n",
    "        else:\n",
    "            referee_preds = np.ones(self.max_seq_len)\n",
    "\n",
    "\n",
    "        print('------------------------------------------------------')\n",
    "        print(\"Total inference time: \", time.time() - start)\n",
    "\n",
    "\n",
    "        slot_preds_list = []\n",
    "        intent_token_preds_list = []\n",
    "        referee_preds_list = []\n",
    "\n",
    "        for token_idx in range(len(slot_label_mask)):\n",
    "            if slot_label_mask[token_idx] != self.pad_token_label_id:\n",
    "                referee_preds_list.append(self.referee_token_map[referee_preds[token_idx]])\n",
    "                intent_token_preds_list.append(self.intent_token_map[intent_token_preds[token_idx]])\n",
    "                slot_preds_list.append(self.slot_label_map[slot_preds[token_idx]])\n",
    "\n",
    "\n",
    "        self.write_readable_outputs(slot_preds_list,intent_token_preds_list,referee_preds_list)\n",
    "\n",
    "        return\n",
    "\n",
    "    def write_readable_outputs(self,slot_preds_list,intent_token_preds_list,referee_preds_list):\n",
    "        words = self.read_input_file()\n",
    "        line = ''\n",
    "        for token_idx,(word, i_pred, s_pred, r_pred) in enumerate(zip(words, intent_token_preds_list, slot_preds_list,referee_preds_list)):\n",
    "            if s_pred == 'O' and i_pred == 'O' and r_pred == 'O':\n",
    "                line = line + word + \" \"\n",
    "            elif i_pred != 'O':\n",
    "                if word not in self.pro_lst:\n",
    "                    line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "\n",
    "                    if r_pred == 'B-referee':\n",
    "                        r_idx = token_idx\n",
    "\n",
    "                else:\n",
    "                    line = line + \"[{}({}):{}:{}] \".format(word,words[r_idx], i_pred,s_pred)\n",
    "\n",
    "        with open(self.output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            if 'B-referee' in referee_preds_list:\n",
    "                f.write('\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n')\n",
    "                f.write('* Pro Case: \\n')\n",
    "                f.write(line.strip()+'\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n \\n')\n",
    "            else:\n",
    "                f.write(line.strip()+'\\n')\n",
    "            print(line)\n",
    "            print('=====================================')\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time ONNX:  0.13010644912719727\n",
      "------------------------------------------------------\n",
      "Total inference time:  0.02127528190612793\n",
      "[find:B-find:O] [jack:I-find:B-per] , and [follow:B-follow:O] [him(jack):I-follow:B-per] \n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "inference = CommandProcessor()\n",
    "inference.predict()\n",
    "\n",
    "# print(inference.ort_session.get_inputs()[0].name)\n",
    "# print(inference.ort_session.get_inputs()[1].name)\n",
    "# print(inference.ort_session.get_inputs()[2].name)\n",
    "# print(inference.ort_session.get_inputs()[3].name)\n",
    "#\n",
    "# print('--------------------------------------')\n",
    "# print(inference.ort_session.get_outputs()[0].name)\n",
    "# print(inference.ort_session.get_outputs()[1].name)\n",
    "# print(inference.ort_session.get_outputs()[2].name)\n",
    "# print(inference.ort_session.get_outputs()[3].name)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jointbert",
   "language": "python",
   "display_name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
