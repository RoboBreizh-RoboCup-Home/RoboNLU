{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/scispacy/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/crossing/miniconda3/envs/scispacy/lib/python3.8/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352465323/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-03-30 18:10:32.512469: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from utils import load_tokenizer, get_intent_labels, get_slot_labels, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import argparse\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_file\", default=\"sample_pred_in.txt\", type=str, help=\"Input file for prediction\")\n",
    "    # parser.add_argument(\"--input_file\", default=\"data/gpsr_pro_instance/test/seq.in\", type=str, help=\"Input file for prediction\")\n",
    "\n",
    "    parser.add_argument(\"--task\", default='gpsr_pro_instance', type=str, help=\"The name of the task to train\")\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str,\n",
    "                        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--pro\", type=int, default=1, help=\"support pronoun disambiguition\")#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "\n",
    "    parser.add_argument(\"--output_file\", default=\"final_predict.txt\", type=str, help=\"Output file for prediction\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"./gpsr_final_model_03-27-13:28:21\", type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--max_seq_len\", default=32, type=int,\n",
    "                        help=\"The maximum total input sequence length after tokenization.\")\n",
    "    parser.add_argument(\"--batch_size\", default=128, type=int, help=\"Batch size for prediction\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument('-f')  #########################\n",
    "\n",
    "    pred_config = parser.parse_args()\n",
    "\n",
    "    pred_config.model_name_or_path = MODEL_PATH_MAP[pred_config.model_type]\n",
    "    pred_config.model_name_or_path = MODEL_PATH_MAP[pred_config.model_type]\n",
    "\n",
    "    tokenizer = load_tokenizer(pred_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Some model files might be missing...",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(args, device)\u001B[0m\n\u001B[1;32m      7\u001B[0m slot_label_lst\u001B[38;5;241m=\u001B[39mget_slot_labels(args)\n\u001B[0;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mMODEL_CLASSES\u001B[49m\u001B[43m[\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m                                                          \u001B[49m\u001B[43mintent_label_lst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mintent_label_lst\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m                                                          \u001B[49m\u001B[43mslot_label_lst\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mslot_label_lst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/miniconda3/envs/scispacy/lib/python3.8/site-packages/transformers/modeling_utils.py:2478\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   2469\u001B[0m         torch\u001B[38;5;241m.\u001B[39mset_default_dtype(dtype_orig)\n\u001B[1;32m   2471\u001B[0m     (\n\u001B[1;32m   2472\u001B[0m         model,\n\u001B[1;32m   2473\u001B[0m         missing_keys,\n\u001B[1;32m   2474\u001B[0m         unexpected_keys,\n\u001B[1;32m   2475\u001B[0m         mismatched_keys,\n\u001B[1;32m   2476\u001B[0m         offload_index,\n\u001B[1;32m   2477\u001B[0m         error_msgs,\n\u001B[0;32m-> 2478\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloaded_state_dict_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# XXX: rename?\u001B[39;49;00m\n\u001B[1;32m   2482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresolved_archive_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2485\u001B[0m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2486\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_fast_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_fast_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2488\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2489\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2490\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2492\u001B[0m \u001B[43m        \u001B[49m\u001B[43mload_in_8bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mload_in_8bit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_in_fp32_modules\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2494\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2496\u001B[0m model\u001B[38;5;241m.\u001B[39mis_loaded_in_8bit \u001B[38;5;241m=\u001B[39m load_in_8bit\n",
      "File \u001B[0;32m~/miniconda3/envs/scispacy/lib/python3.8/site-packages/transformers/modeling_utils.py:2844\u001B[0m, in \u001B[0;36mPreTrainedModel._load_pretrained_model\u001B[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001B[0m\n\u001B[1;32m   2841\u001B[0m         error_msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2842\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2843\u001B[0m         )\n\u001B[0;32m-> 2844\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merror_msg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(unexpected_keys) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for JointBERTMultiIntent:\n\tsize mismatch for multi_intent_classifier.linear.weight: copying a param with shape torch.Size([20, 768]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for multi_intent_classifier.linear.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for slot_classifier.linear.weight: copying a param with shape torch.Size([12, 768]) from checkpoint, the shape in current model is torch.Size([10, 768]).\n\tsize mismatch for slot_classifier.linear.bias: copying a param with shape torch.Size([12]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for intent_token_classifier.linear.weight: copying a param with shape torch.Size([20, 768]) from checkpoint, the shape in current model is torch.Size([16, 768]).\n\tsize mismatch for intent_token_classifier.linear.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([16]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome model files might be missing...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[0;32m---> 17\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_config\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 14\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(args, device)\u001B[0m\n\u001B[1;32m     12\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m---> 14\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome model files might be missing...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "\u001B[0;31mException\u001B[0m: Some model files might be missing..."
     ]
    }
   ],
   "source": [
    "def load_model(args, device):\n",
    "    # Check whether model exists\n",
    "    if not os.path.exists('./gpsr_final_model_03-27-13:28:21'):\n",
    "        raise Exception(\"Model doesn't exists! Train first!\")\n",
    "    try:\n",
    "        intent_label_lst = get_intent_labels(args)\n",
    "        slot_label_lst=get_slot_labels(args)\n",
    "        model = MODEL_CLASSES[args.model_type][1].from_pretrained(args.model_dir,\n",
    "                                                                  intent_label_lst=intent_label_lst,\n",
    "                                                                  slot_label_lst=slot_label_lst)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    except:\n",
    "        raise Exception(\"Some model files might be missing...\")\n",
    "    return model\n",
    "\n",
    "model = load_model(pred_config,'cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('dropout', Dropout(p=0.1, inplace=False)),\n             ('activation', ReLU()),\n             ('linear', Linear(in_features=768, out_features=10, bias=True))])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.slot_classifier.linear.weight.shape\n",
    "model.slot_classifier._modules['dropout'].p\n",
    "model.slot_classifier._modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "slot_classifier_w = model.slot_classifier._modules['linear'].weight.detach().numpy()\n",
    "save('./numpy_para/slot_classifier/weights.npy', slot_classifier_w)\n",
    "slot_classifier_b = model.slot_classifier._modules['linear'].bias.detach().numpy()\n",
    "save('./numpy_para/slot_classifier/bias.npy', slot_classifier_b)\n",
    "\n",
    "\n",
    "intent_token_classifier_w = model.intent_token_classifier._modules['linear'].weight.detach().numpy()\n",
    "save('./numpy_para/intent_token_classifier/weights.npy', intent_token_classifier_w)\n",
    "intent_token_classifier_b = model.intent_token_classifier._modules['linear'].bias.detach().numpy()\n",
    "save('./numpy_para/intent_token_classifier/bias.npy', intent_token_classifier_b)\n",
    "\n",
    "\n",
    "\n",
    "pro_classifier_w1 = model.pro_classifier._modules['linear1'].weight.detach().numpy()\n",
    "save('./numpy_para/pro_classifier/weights1.npy', pro_classifier_w1)\n",
    "pro_classifier_b1 = model.pro_classifier._modules['linear1'].bias.detach().numpy()\n",
    "save('./numpy_para/pro_classifier/bias1.npy', pro_classifier_b1)\n",
    "\n",
    "pro_classifier_w2 = model.pro_classifier._modules['linear2'].weight.detach().numpy()\n",
    "save('./numpy_para/pro_classifier/weights2.npy', pro_classifier_w2)\n",
    "pro_classifier_b2 = model.pro_classifier._modules['linear2'].bias.detach().numpy()\n",
    "save('./numpy_para/pro_classifier/bias2.npy', pro_classifier_b2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.slot_classifier_np at 0x7fd38989cbb0>"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class slot_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights = load(dir+'/weights.npy')\n",
    "        self.linear_bias = load(dir+'/bias.npy')\n",
    "    def ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights) + self.linear_bias\n",
    "        return x\n",
    "\n",
    "s =  slot_classifier_np('./numpy_para/slot_classifier')\n",
    "s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "IntentTokenClassifier(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear): Linear(in_features=768, out_features=16, bias=True)\n)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intent_token_classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.intent_token_classifier_np at 0x7fd3a9bdd9a0>"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class intent_token_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights = load(dir + '/weights.npy')\n",
    "        self.linear_bias = load(dir + '/bias.npy')\n",
    "    # def ReLU(self,x):\n",
    "    #     return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        # x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights) + self.linear_bias\n",
    "        return x\n",
    "\n",
    "i =  intent_token_classifier_np('./numpy_para/intent_token_classifier')\n",
    "i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('dropout', Dropout(p=0.1, inplace=False)),\n             ('linear1',\n              Linear(in_features=1536, out_features=768, bias=True)),\n             ('linear2', Linear(in_features=768, out_features=3, bias=True))])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pro_classifier._modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.pro_classifier_np at 0x7fd3a9b00c10>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class pro_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights1 = load(dir + '/weights1.npy')\n",
    "        self.linear_bias1 = load(dir + '/bias1.npy')\n",
    "        self.linear_weights2 = load(dir + '/weights2.npy')\n",
    "        self.linear_bias2 = load(dir + '/bias2.npy')\n",
    "    def ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        x = x @ np.transpose(self.linear_weights1) + self.linear_bias1\n",
    "        x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights2) + self.linear_bias2\n",
    "        return x\n",
    "\n",
    "p =  pro_classifier_np('./numpy_para/pro_classifier')\n",
    "p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time ONNX:  0.10057282447814941\n",
      "Loading time ONNX:  0.0010554790496826172\n",
      "Loading time ONNX:  0.0006890296936035156\n",
      "Loading time ONNX:  0.001600503921508789\n",
      "------------------------------------------------------\n",
      "Total inference time:  0.04411673545837402\n",
      "[find:B-find:O] [jack:I-find:B-per] , and [follow:B-follow:O] [him(jack):I-follow:B-per] \n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class CommandProcessor():\n",
    "    def __init__(self,session = None):\n",
    "        self.INTENT_CLASSES = ['PAD','O','B-greet','I-greet','B-guide','I-guide','B-follow','I-follow','B-find','I-find','B-take','I-take','B-go','I-go','B-know','I-know']\n",
    "        self.SLOT_CLASSES = ['PAD','O','B-obj','B-dest','I-sour','I-obj','I-dest','B-per','B-sour','I-per']\n",
    "        self.PRO_CLASSES = ['PAD','O','B-referee']\n",
    "\n",
    "        self.referee_token_map = {i:label for i,label in enumerate(self.PRO_CLASSES)}\n",
    "        self.intent_token_map = {i:label for i,label in enumerate(self.INTENT_CLASSES)}\n",
    "        self.slot_label_map = {i:label for i,label in enumerate(self.SLOT_CLASSES)}\n",
    "\n",
    "        self.max_seq_len = 32\n",
    "        self.pro_lst = ['him','her','it','its']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pad_token_label_id = 0\n",
    "\n",
    "        self.input_text_path = './sample_pred_in.txt'#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.output_file = './outputs'\n",
    "\n",
    "        self.bert_ort_session = self.initONNX('./quantized_models/bert.quantV1.onnx')\n",
    "        self.slot_classifier_ort_session = self.initONNX('./quantized_models/slot_classifier.quant.onnx')\n",
    "        self.intent_token_classifier_ort_session = self.initONNX('./quantized_models/intent_token_classifier.quant.onnx')\n",
    "        self.pro_classifier_ort_session = self.initONNX('./quantized_models/pro_classifier.quant.onnx')\n",
    "\n",
    "    def initONNX(self,path):\n",
    "        start = time.time()\n",
    "        sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "        sess_options.intra_op_num_threads = 1#4\n",
    "        sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL\n",
    "        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "        ort_session  = onnxruntime.InferenceSession(path, sess_options)\n",
    "        print(\"Loading time ONNX: \", time.time() - start)\n",
    "        return ort_session\n",
    "\n",
    "    def read_input_file(self):\n",
    "        with open(self.input_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.readline().strip().split()\n",
    "            # for line in f:\n",
    "            #     line = line.strip()\n",
    "            #     words = line.split()\n",
    "            #     break # I should delete precessed commands!!!!!!!!!!!!!\n",
    "\n",
    "        return words\n",
    "\n",
    "    def convert_input_file_to_dataloader(self,words,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        # Setting based on the current model type\n",
    "        cls_token = tokenizer.cls_token\n",
    "        sep_token = tokenizer.sep_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        tokens = []\n",
    "        slot_label_mask = []\n",
    "        pro_labels_ids = []\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            if word in self.pro_lst:\n",
    "                pro_label = 1\n",
    "            else:\n",
    "                pro_label = 0\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_label_mask.extend([self.pad_token_label_id + 1] + [self.pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            pro_labels_ids.extend([pro_label] + [self.pad_token_label_id] * (len(word_tokens) - 1)) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > self.max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (self.max_seq_len - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(self.max_seq_len - special_tokens_count)]\n",
    "            pro_labels_ids = pro_labels_ids[:(self.max_seq_len - special_tokens_count)] #!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [self.pad_token_label_id]\n",
    "        pro_labels_ids += [self.pad_token_label_id]#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [self.pad_token_label_id] + slot_label_mask\n",
    "        pro_labels_ids = [self.pad_token_label_id] + pro_labels_ids#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = self.max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([self.pad_token_label_id] * padding_length)\n",
    "        pro_labels_ids = pro_labels_ids + ([self.pad_token_label_id] * padding_length)\n",
    "\n",
    "\n",
    "        input_ids = np.array(input_ids)\n",
    "        attention_mask = np.array(attention_mask)\n",
    "        token_type_ids = np.array(token_type_ids)\n",
    "        pro_labels_ids = np.array(pro_labels_ids)\n",
    "        # print('input_ids shape: ',input_ids.shape)\n",
    "        # print('attention_mask shape: ',attention_mask.shape)\n",
    "        # print('token_type_ids shape: ',token_type_ids.shape)\n",
    "        # print('pro_labels_ids shape: ',pro_labels_ids.shape)\n",
    "        sample = {'input_ids':input_ids[None,:], 'attention_mask':attention_mask[None,:], 'token_type_ids': token_type_ids[None,:]}\n",
    "\n",
    "        return sample,slot_label_mask,pro_labels_ids\n",
    "\n",
    "    def predict(self):\n",
    "        lines = self.read_input_file()\n",
    "        sample,slot_label_mask,pro_labels_ids = self.convert_input_file_to_dataloader(lines)\n",
    "        start = time.time()\n",
    "\n",
    "        sequence_output, _ = self.bert_ort_session.run(None, sample)\n",
    "        # ============================= Slot prediction ==============================\n",
    "        slot_classifier = slot_classifier_np('./numpy_para/slot_classifier')\n",
    "        slot_logits = slot_classifier.forward(sequence_output)\n",
    "        # slot_logits = self.slot_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "        slot_preds = np.squeeze(np.array(slot_logits))\n",
    "        slot_preds = np.argmax(slot_preds, axis=1)\n",
    "\n",
    "        # ============================== Intent Token Seq =============================\n",
    "        intent_token_classifier = intent_token_classifier_np('./numpy_para/intent_token_classifier')\n",
    "        intent_token_logits = intent_token_classifier.forward(sequence_output)\n",
    "        # intent_token_logits = self.intent_token_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "        intent_token_preds = np.squeeze(np.array(intent_token_logits))\n",
    "        intent_token_preds = np.argmax(intent_token_preds, axis=1)\n",
    "\n",
    "        # ============================= Pronoun referee prediction ==============================\n",
    "        if any(pro_labels_ids):\n",
    "            sq_sequence_output = np.squeeze(sequence_output)\n",
    "            pro_token = sq_sequence_output[pro_labels_ids == 1]\n",
    "            repeat_pro = np.tile(pro_token,(self.max_seq_len,1))\n",
    "            concated_input = np.concatenate((sq_sequence_output,repeat_pro),axis = 1)[None,:]\n",
    "\n",
    "            pro_classifier = pro_classifier_np('./numpy_para/pro_classifier')\n",
    "            referee_token_logits = pro_classifier.forward(concated_input)\n",
    "            # referee_token_logits = self.pro_classifier_ort_session.run(None, {'concated_input':concated_input})\n",
    "            referee_preds = np.squeeze(np.array(referee_token_logits))\n",
    "            referee_preds = np.argmax(referee_preds, axis=1)\n",
    "\n",
    "        else:\n",
    "            referee_preds = np.ones(self.max_seq_len)\n",
    "\n",
    "\n",
    "        print('------------------------------------------------------')\n",
    "        print(\"Total inference time: \", time.time() - start)\n",
    "\n",
    "\n",
    "        slot_preds_list = []\n",
    "        intent_token_preds_list = []\n",
    "        referee_preds_list = []\n",
    "\n",
    "        for token_idx in range(len(slot_label_mask)):\n",
    "            if slot_label_mask[token_idx] != self.pad_token_label_id:\n",
    "                referee_preds_list.append(self.referee_token_map[referee_preds[token_idx]])\n",
    "                intent_token_preds_list.append(self.intent_token_map[intent_token_preds[token_idx]])\n",
    "                slot_preds_list.append(self.slot_label_map[slot_preds[token_idx]])\n",
    "\n",
    "\n",
    "        self.write_readable_outputs(slot_preds_list,intent_token_preds_list,referee_preds_list)\n",
    "\n",
    "        return\n",
    "\n",
    "    def write_readable_outputs(self,slot_preds_list,intent_token_preds_list,referee_preds_list):\n",
    "        words = self.read_input_file()\n",
    "        line = ''\n",
    "        for token_idx,(word, i_pred, s_pred, r_pred) in enumerate(zip(words, intent_token_preds_list, slot_preds_list,referee_preds_list)):\n",
    "            if s_pred == 'O' and i_pred == 'O' and r_pred == 'O':\n",
    "                line = line + word + \" \"\n",
    "            elif i_pred != 'O':\n",
    "                if word not in self.pro_lst:\n",
    "                    line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "\n",
    "                    if r_pred == 'B-referee':\n",
    "                        r_idx = token_idx\n",
    "\n",
    "                else:\n",
    "                    line = line + \"[{}({}):{}:{}] \".format(word,words[r_idx], i_pred,s_pred)\n",
    "\n",
    "        with open(self.output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            if 'B-referee' in referee_preds_list:\n",
    "                f.write('\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n')\n",
    "                f.write('* Pro Case: \\n')\n",
    "                f.write(line.strip()+'\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n \\n')\n",
    "            else:\n",
    "                f.write(line.strip()+'\\n')\n",
    "            print(line)\n",
    "            print('=====================================')\n",
    "        return\n",
    "\n",
    "inference = CommandProcessor()\n",
    "inference.predict()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "ProClassifier(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear1): Linear(in_features=1536, out_features=768, bias=True)\n  (linear2): Linear(in_features=768, out_features=3, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pro_classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout\n",
      "dropout\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dropout' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m             Weight\u001B[38;5;241m.\u001B[39mappend((name,layer\u001B[38;5;241m.\u001B[39mweight))\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Weight\n\u001B[0;32m---> 14\u001B[0m \u001B[43mget_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mslot_classifier\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[50], line 11\u001B[0m, in \u001B[0;36mget_weights\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m      9\u001B[0m         Weight\u001B[38;5;241m.\u001B[39mappend((name))\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 11\u001B[0m         Weight\u001B[38;5;241m.\u001B[39mappend((name,\u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m))\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Weight\n",
      "File \u001B[0;32m~/miniconda3/envs/jointbert/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1205\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1206\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1207\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1208\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Dropout' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "def get_weights(model):\n",
    "    Weight = []\n",
    "    for name,layer in model._modules.items():\n",
    "        print(name)\n",
    "        if name == 'dropout':\n",
    "            print(name)\n",
    "            Weight.append((name,layer.p))\n",
    "        if name == 'activation':\n",
    "            Weight.append((name))\n",
    "        else:\n",
    "            Weight.append((name,layer.weight))\n",
    "\n",
    "    return Weight\n",
    "get_weights(model.slot_classifier)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "IntentTokenClassifier(\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear): Linear(in_features=768, out_features=16, bias=True)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "scispacy",
   "language": "python",
   "display_name": "scispacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
