{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from utils import MODEL_CLASSES, compute_metrics, get_intent_labels, get_slot_labels, compute_metrics_multi_intent\n",
    "\n",
    "from seqeval.metrics.sequence_labeling import get_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG = False\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_multi(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        \n",
    "        self.slot_preds_list = None\n",
    "        self.intent_token_preds_list = None\n",
    "        \n",
    "        \n",
    "        # set of intents\n",
    "        self.intent_label_lst = get_intent_labels(args)\n",
    "        # set of slots\n",
    "        self.slot_label_lst = get_slot_labels(args)\n",
    "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "        self.pad_token_label_id = args.ignore_index\n",
    "\n",
    "        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "        \n",
    "        self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "\n",
    "        self.model = self.model_class.from_pretrained(args.model_name_or_path,\n",
    "                                                      config=self.config,\n",
    "                                                      args=args,\n",
    "                                                      intent_label_lst=self.intent_label_lst,\n",
    "                                                      slot_label_lst=self.slot_label_lst)\n",
    "        \n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "       \n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        #logger.info(vars(self.args))\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n",
    "\n",
    "        if self.args.max_steps > 0:\n",
    "            t_total = self.args.max_steps\n",
    "            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.args.weight_decay},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
    "        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
    "        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
    "        logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "        step_per_epoch = len(train_dataloader) // 2\n",
    "\n",
    "        # record the evaluation loss\n",
    "        eval_acc = 0.0\n",
    "        MAX_RECORD = self.args.patience\n",
    "        num_eval = -1\n",
    "        eval_result_record = (num_eval, eval_acc)\n",
    "        flag = False\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=FLAG)\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
    "\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4],\n",
    "                          'intent_token_ids': batch[5],\n",
    "                          'B_tag_mask': batch[6],\n",
    "                          'BI_tag_mask': batch[7],\n",
    "                          'tag_intent_label': batch[8]}\n",
    "                if self.args.model_type != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "                outputs = self.model(**inputs)\n",
    "                losses = outputs[0]\n",
    "                loss = losses[0]\n",
    "                intent_loss = losses[1]\n",
    "                slot_loss = losses[2]\n",
    "                intent_token_loss = losses[3]\n",
    "                tag_intent_loss = losses[4]\n",
    "                # tag_intent_loss_softmax = losses[5]\n",
    "\n",
    "                if self.args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    self.model.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
    "                    if self.args.logging_steps > 0 and global_step % step_per_epoch == 0:\n",
    "                        logger.info(\"***** Training Step %d *****\", step)\n",
    "                        logger.info(\"  total_loss = %f\", loss)\n",
    "                        logger.info(\"  intent_loss = %f\", intent_loss)\n",
    "                        logger.info(\"  slot_loss = %f\", slot_loss)\n",
    "                        logger.info(\"  intent_token_loss = %f\", intent_token_loss)\n",
    "                        logger.info(\"  tag_intent_loss = %f\", tag_intent_loss)\n",
    "                        # logger.info(\"  tag_intent_loss_softmax = %f\", tag_intent_loss_softmax)\n",
    "\n",
    "                        dev_result = self.evaluate(\"dev\")\n",
    "                        test_result = self.evaluate(\"test\")\n",
    "                        num_eval += 1\n",
    "                        if self.args.patience != 0:\n",
    "                            if dev_result['sementic_frame_acc'] + dev_result['intent_acc'] + dev_result['slot_f1']   > eval_result_record[1]:\n",
    "                                self.save_model()\n",
    "                                eval_result_record = (num_eval, dev_result['sementic_frame_acc'] + dev_result['intent_acc'] + dev_result['slot_f1'] )\n",
    "                            else:\n",
    "                                cur_num_eval = eval_result_record[0]\n",
    "                                if num_eval - cur_num_eval >= MAX_RECORD:\n",
    "                                    # it has been ok\n",
    "                                    logger.info(' EARLY STOP Evaluate: at {}, best eval {} intent_slot_acc: {} '.format(num_eval, cur_num_eval, eval_result_record[1]))\n",
    "                                    flag = True\n",
    "                                    break\n",
    "                        else:\n",
    "                            self.save_model()\n",
    "\n",
    "                            \n",
    "\n",
    "                        # we check whether there is an overfitting issue for mixsnips\n",
    "                        \n",
    "\n",
    "                    # if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
    "                    #     self.save_model()\n",
    "                    \n",
    "                if 0 < self.args.max_steps < global_step:\n",
    "                    epoch_iterator.close()\n",
    "                    break\n",
    "\n",
    "            if flag:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "            if 0 < self.args.max_steps < global_step:\n",
    "                train_iterator.close()\n",
    "                break\n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    def evaluate(self, mode):\n",
    "        if mode == 'test':\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == 'dev':\n",
    "            dataset = self.dev_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"  Num examples = %d\", len(dataset))\n",
    "        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        intent_preds = None\n",
    "        slot_preds = None\n",
    "        intent_token_preds = None\n",
    "        out_intent_label_ids = None\n",
    "        out_slot_labels_ids = None\n",
    "        out_intent_token_ids = None\n",
    "        \n",
    "        tag_intent_preds = None\n",
    "        out_tag_intent_ids = None\n",
    "        \n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=FLAG):\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4],\n",
    "                          'intent_token_ids': batch[5],\n",
    "                          'B_tag_mask': batch[6],\n",
    "                          'BI_tag_mask': batch[7],\n",
    "                          'tag_intent_label': batch[8]}\n",
    "                if self.args.model_type != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "                outputs = self.model(**inputs)\n",
    "                if self.args.intent_seq and self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits) = outputs[:2]\n",
    "                elif self.args.intent_seq:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits) = outputs[:2]\n",
    "                elif self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, tag_intent_logits) = outputs[:2]\n",
    "                else:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "#                 eval_loss += tmp_eval_loss.mean().item()\n",
    "#             nb_eval_steps += 1\n",
    "\n",
    "            # ============================ Intent prediction =============================\n",
    "            if intent_preds is None:\n",
    "                intent_preds = intent_logits.detach().cpu().numpy()\n",
    "                out_intent_label_ids = inputs['intent_label_ids'].detach().cpu().numpy()\n",
    "            else:\n",
    "                intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
    "                out_intent_label_ids = np.append(\n",
    "                    out_intent_label_ids, inputs['intent_label_ids'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "            # ============================= Slot prediction ==============================\n",
    "            if slot_preds is None:\n",
    "                if self.args.use_crf:\n",
    "                    # decode() in `torchcrf` returns list with best index directly\n",
    "                    slot_preds = np.array(self.model.crf.decode(slot_logits))\n",
    "                else:\n",
    "                    slot_preds = slot_logits.detach().cpu().numpy()\n",
    "\n",
    "                out_slot_labels_ids = inputs[\"slot_labels_ids\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                if self.args.use_crf:\n",
    "                    slot_preds = np.append(slot_preds, np.array(self.model.crf.decode(slot_logits)), axis=0)\n",
    "                else:\n",
    "                    slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                out_slot_labels_ids = np.append(out_slot_labels_ids, inputs[\"slot_labels_ids\"].detach().cpu().numpy(), axis=0)\n",
    "            \n",
    "            # ============================== Intent Token Seq =============================\n",
    "            if self.args.intent_seq:\n",
    "                if intent_token_preds is None:\n",
    "                    if self.args.use_crf:\n",
    "                        intent_token_preds = np.array(self.model.crf.decode(intent_token_logits))\n",
    "                    else:\n",
    "                        intent_token_preds = intent_token_logits.detach().cpu().numpy()\n",
    "\n",
    "                    out_intent_token_ids = inputs[\"intent_token_ids\"].detach().cpu().numpy()\n",
    "                else:\n",
    "                    if self.args.use_crf:\n",
    "                        intent_token_preds = np.append(intent_token_preds, np.array(self.model.crf.decode(intent_token_logits)), axis=0)\n",
    "                    else:\n",
    "                        intent_token_preds = np.append(intent_token_preds, intent_token_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                    out_intent_token_ids = np.append(out_intent_token_ids, inputs[\"intent_token_ids\"].detach().cpu().numpy(), axis=0)\n",
    "        \n",
    "            # slot_preds: (64 * n, 50, 74)    \n",
    "        \n",
    "#         eval_loss = eval_loss / nb_eval_steps\n",
    "#         results = {\n",
    "#             \"loss\": eval_loss\n",
    "#         }\n",
    "\n",
    "        # Intent result\n",
    "        # (batch_size, )\n",
    "        # intent_preds = np.argmax(intent_preds, axis=1)\n",
    "        # (batch_size, num_intents)\n",
    "        # we set the threshold to 0.5\n",
    "        intent_preds = torch.as_tensor(intent_preds > 0.5, dtype=torch.int32)\n",
    "\n",
    "        # Slot result\n",
    "        # (batch_size, seq_len)\n",
    "        if not self.args.use_crf:\n",
    "            slot_preds = np.argmax(slot_preds, axis=2)\n",
    "        slot_label_map = {i: label for i, label in enumerate(self.slot_label_lst)}\n",
    "        out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "        slot_preds_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "        \n",
    "        B_tag_mask_pred = []\n",
    "        BI_tag_mask_pred = []\n",
    "        \n",
    "        # generate mask\n",
    "        for i in range(out_slot_labels_ids.shape[0]):\n",
    "            # record the padding position\n",
    "            pos_offset = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "            pos_cnt = 0\n",
    "            padding_recording = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "            \n",
    "            for j in range(out_slot_labels_ids.shape[1]):\n",
    "                if out_slot_labels_ids[i, j] != self.pad_token_label_id:\n",
    "                    out_slot_label_list[i].append(slot_label_map[out_slot_labels_ids[i][j]])\n",
    "                    slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
    "                    pos_offset[pos_cnt+1] = pos_offset[pos_cnt]\n",
    "                    pos_cnt += 1\n",
    "                else:\n",
    "                    pos_offset[pos_cnt] = pos_offset[pos_cnt] + 1\n",
    "                    padding_recording[j] = 1\n",
    "                    \n",
    "\n",
    "            entities = get_entities(slot_preds_list[i])\n",
    "            entities = [tag for entity_idx, tag in enumerate(entities) if slot_preds_list[i][tag[1]].startswith('B')]\n",
    "            \n",
    "            if len(entities) > self.args.num_mask:\n",
    "                entities = entities[:self.args.num_mask]\n",
    "            \n",
    "            entity_masks = []\n",
    "            \n",
    "            for entity_idx, entity in enumerate(entities):\n",
    "                entity_mask = [0 for _ in range(out_slot_labels_ids.shape[1])]\n",
    "                start_idx = entity[1] + pos_offset[entity[1]]\n",
    "                end_idx = entity[2] + pos_offset[entity[2]] + 1\n",
    "                if self.args.BI_tag:\n",
    "                    entity_mask[start_idx:end_idx] = [1] * (end_idx - start_idx)\n",
    "                    for padding_idx in range(start_idx, end_idx):\n",
    "                        if padding_recording[padding_idx]:\n",
    "                            entity_mask[padding_idx] = 0\n",
    "                else:\n",
    "                    entity_mask[start_idx] = 1\n",
    "                    \n",
    "                entity_masks.append(entity_mask)\n",
    "            \n",
    "            for extra_idx in range(self.args.num_mask - len(entity_masks)):\n",
    "                entity_masks.append([\n",
    "                    0 for _ in range(out_slot_labels_ids.shape[1])\n",
    "                ])\n",
    "\n",
    "            \n",
    "            if self.args.BI_tag:\n",
    "                BI_tag_mask_pred.append(entity_masks)\n",
    "            else:\n",
    "                B_tag_mask_pred.append(entity_masks)\n",
    "                \n",
    "        if self.args.BI_tag:\n",
    "            BI_tag_mask_pred_tensor = torch.FloatTensor(BI_tag_mask_pred)\n",
    "        else:\n",
    "            B_tag_mask_pred_tensor = torch.FloatTensor(B_tag_mask_pred)\n",
    "        \n",
    "        BI_tag_mask_pred_input = None\n",
    "        B_tag_mask_pred_input = None\n",
    "        \n",
    "        for eval_idx, batch in tqdm(enumerate(eval_dataloader), desc=\"Evaluating\", disable=FLAG):\n",
    "            if self.args.BI_tag:\n",
    "                BI_tag_mask_pred_input = BI_tag_mask_pred_tensor[eval_idx*self.args.eval_batch_size:(eval_idx+1)*self.args.eval_batch_size]\n",
    "            else:\n",
    "                B_tag_mask_pred_input = B_tag_mask_pred_tensor[eval_idx*self.args.eval_batch_size:(eval_idx+1)*self.args.eval_batch_size]\n",
    "            \n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1],\n",
    "                          'intent_label_ids': batch[3],\n",
    "                          'slot_labels_ids': batch[4],\n",
    "                          'intent_token_ids': batch[5],\n",
    "                          'B_tag_mask': B_tag_mask_pred_input,\n",
    "                          'BI_tag_mask': BI_tag_mask_pred_input,\n",
    "                          'tag_intent_label': batch[8]}\n",
    "                if self.args.model_type != 'distilbert':\n",
    "                    inputs['token_type_ids'] = batch[2]\n",
    "                outputs = self.model(**inputs)\n",
    "                if self.args.intent_seq and self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits, tag_intent_logits) = outputs[:2]\n",
    "                elif self.args.intent_seq:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, intent_token_logits) = outputs[:2]\n",
    "                elif self.args.tag_intent:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits, tag_intent_logits) = outputs[:2]\n",
    "                else:\n",
    "                    tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "                # if mode == 'test':\n",
    "                #     print(eval_idx, ' ', tmp_eval_loss)\n",
    "                eval_loss += tmp_eval_loss[0].mean().item()\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            if self.args.tag_intent:\n",
    "                size_1 = inputs['tag_intent_label'].size(0)\n",
    "                size_2 = inputs['tag_intent_label'].size(1)\n",
    "                \n",
    "                if tag_intent_preds is None:\n",
    "                    tag_intent_preds = tag_intent_logits.view(size_1, size_2, -1).detach().cpu().numpy()\n",
    "                    out_tag_intent_ids = inputs['tag_intent_label'].detach().cpu().numpy()\n",
    "                else:\n",
    "                    tag_intent_preds = np.append(tag_intent_preds, tag_intent_logits.view(size_1, size_2, -1).detach().cpu().numpy(), axis=0)\n",
    "#                     print('out_tag_intent_ids shape: ', out_tag_intent_ids.shape)\n",
    "#                     print('tag_intent_label shape: ', inputs['tag_intent_label'].shape)\n",
    "                    out_tag_intent_ids = np.append(\n",
    "                        out_tag_intent_ids, inputs['tag_intent_label'].detach().cpu().numpy(), axis=0)\n",
    "                \n",
    "        \n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        results = {\n",
    "            \"loss\": eval_loss\n",
    "        }\n",
    "        \n",
    "        intent_token_map = {i: label for i, label in enumerate(self.intent_label_lst)}\n",
    "        out_intent_token_list = None\n",
    "        intent_token_preds_list = None\n",
    "        # ============================= Intent Seq Prediction ============================\n",
    "        if self.args.intent_seq:\n",
    "            if not self.args.use_crf:\n",
    "                intent_token_preds = np.argmax(intent_token_preds, axis=2)\n",
    "            out_intent_token_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "            intent_token_preds_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "\n",
    "            for i in range(out_intent_token_ids.shape[0]):\n",
    "                for j in range(out_intent_token_ids.shape[1]):\n",
    "                    if out_intent_token_ids[i, j] != self.pad_token_label_id:\n",
    "                        out_intent_token_list[i].append(intent_token_map[out_intent_token_ids[i][j]])\n",
    "                        intent_token_preds_list[i].append(intent_token_map[intent_token_preds[i][j]])\n",
    "        \n",
    "        out_tag_intent_list = None\n",
    "        tag_intent_preds_list = None\n",
    "        # ============================ Tag Intent Prediction ==============================\n",
    "        if self.args.tag_intent:\n",
    "            tag_intent_preds = np.argmax(tag_intent_preds, axis=2)\n",
    "            out_tag_intent_list = [[] for _ in range(out_tag_intent_ids.shape[0])]\n",
    "            tag_intent_preds_list = [[] for _ in range(out_tag_intent_ids.shape[0])]\n",
    "            \n",
    "            for i in range(out_tag_intent_ids.shape[0]):\n",
    "                for j in range(out_tag_intent_ids.shape[1]):\n",
    "                    if out_tag_intent_ids[i, j] != self.pad_token_label_id:\n",
    "                        out_tag_intent_list[i].append(intent_token_map[out_tag_intent_ids[i][j]])\n",
    "                        tag_intent_preds_list[i].append(intent_token_map[tag_intent_preds[i][j]])\n",
    "                        \n",
    "        total_result = compute_metrics_multi_intent(intent_preds,\n",
    "                                       out_intent_label_ids,\n",
    "                                       slot_preds_list,\n",
    "                                       out_slot_label_list,\n",
    "                                       intent_token_preds_list,\n",
    "                                       out_intent_token_list,\n",
    "                                       tag_intent_preds_list,\n",
    "                                       out_tag_intent_list)\n",
    "        results.update(total_result)\n",
    "        \n",
    "#         print(slot_label_map)\n",
    "#         print(intent_token_map)\n",
    "\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"  %s_%s = %s\", mode, key, str(results[key]))\n",
    "        \n",
    "        #self.store_pred(slot_preds_list,intent_token_preds_list)\n",
    "        self.slot_preds_list = slot_preds_list\n",
    "        self.intent_token_preds_list = intent_token_preds_list\n",
    "        return results\n",
    "\n",
    "    def store_pred(self,slot_preds_list,intent_token_preds_list):\n",
    "        self.slot_preds_list = slot_preds_list\n",
    "        self.intent_token_preds_list = intent_token_preds_list\n",
    "#         if show:\n",
    "#             print(f'slot_preds_list: {self.slot_preds_list[:2]}\\n')\n",
    "#             print(f'intent_token_preds_list: {self.intent_token_preds_list[:2]}\\n')\n",
    "\n",
    "    def save_model(self):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            os.makedirs(self.args.model_dir)\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        model_to_save.save_pretrained(self.args.model_dir)\n",
    "\n",
    "        # Save training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(self.args.model_dir, 'training_args.bin'))\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
    "\n",
    "    def load_model(self):\n",
    "        # Check whether model exists\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            raise Exception(\"Model doesn't exists! Train first!\")\n",
    "\n",
    "        try:\n",
    "            self.model = self.model_class.from_pretrained(self.args.model_dir,\n",
    "                                                          args=self.args,\n",
    "                                                          intent_label_lst=self.intent_label_lst,\n",
    "                                                          slot_label_lst=self.slot_label_lst)\n",
    "            self.model.to(self.device)\n",
    "            logger.info(\"***** Model Loaded *****\")\n",
    "        except:\n",
    "            raise Exception(\"Some model files might be missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from trainer import Trainer, Trainer_multi, Trainer_woISeq\n",
    "from utils import init_logger, load_tokenizer, read_prediction_text, set_seed, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "from data_loader import load_and_cache_examples\n",
    "\n",
    "def init_logger():\n",
    "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                        level=logging.INFO)\n",
    "\n",
    "def main(args):\n",
    "#     init_logger(args)\n",
    "#     init_logger()\n",
    "\n",
    "    set_seed(args)\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "    dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "    test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "    \n",
    "    if args.multi_intent == 1:\n",
    "        trainer = Trainer_multi(args, train_dataset, dev_dataset, test_dataset)\n",
    "    else:\n",
    "        trainer = Trainer(args, train_dataset, dev_dataset, test_dataset)\n",
    "    if args.do_train:\n",
    "        trainer.train()\n",
    "    if args.do_eval:\n",
    "        trainer.load_model()\n",
    "        trainer.evaluate(\"test\")\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_wait = random.uniform(0, 10)\n",
    "    time.sleep(time_wait)\n",
    "    parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--task\", default='mixsnips', required=True, type=str, help=\"The name of the task to train\")\n",
    "    parser.add_argument(\"--task\", default='gpsr', type=str, help=\"The name of the task to train\")\n",
    "\n",
    "#     parser.add_argument(\"--model_dir\", default='./gpsr_model', required=True, type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--model_dir\", default='./gpsr_model', type=str, help=\"Path to save, load model\")\n",
    "\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str, help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "#     parser.add_argument(\"--intent_seq\", type=int, default=0, help=\"whether we use intent seq setting\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "    parser.add_argument(\"--multi_intent\", type=int, default=1, help=\"whether we use multi intent setting\")\n",
    "    parser.add_argument(\"--tag_intent\", type=int, default=1, help=\"whether we can use tag to predict intent\")\n",
    "    \n",
    "    parser.add_argument(\"--BI_tag\", type=int, default=1, help='use BI sum or just B')\n",
    "    parser.add_argument(\"--cls_token_cat\", type=int, default=1, help='whether we cat the cls to the slot output of bert')\n",
    "    parser.add_argument(\"--intent_attn\", type=int, default=1, help='whether we use attention mechanism on the CLS intent output')\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "                                           #max slot num = 7\n",
    "    \n",
    "    \n",
    "    parser.add_argument('--seed', type=int, default=25, help=\"random seed for initialization\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "#     parser.add_argument(\"--train_batch_size\", default=64, type=int, help=\"Batch size for training.\")\n",
    "\n",
    "    parser.add_argument(\"--eval_batch_size\", default=128, type=int, help=\"Batch size for evaluation.\")\n",
    "    parser.add_argument(\"--max_seq_len\", default=32, type=int, help=\"The maximum total input sequence length after tokenization.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", default=10.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=4.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "                                            #####\n",
    "    \n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                        help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "    parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
    "    parser.add_argument('--logging_steps', type=int, default=500, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument('--save_steps', type=int, default=300, help=\"Save checkpoint every X updates steps.\")\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "    parser.add_argument('--slot_loss_coef', type=float, default=2.0, help='Coefficient for the slot loss.')\n",
    "    parser.add_argument('--tag_intent_coef', type=float, default=1.0, help='Coefficient for the tag intent loss')\n",
    "\n",
    "    # CRF option\n",
    "    parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
    "    parser.add_argument(\"--slot_pad_label\", default=\"PAD\", type=str, help=\"Pad token for slot label pad (to be ignore when calculate loss)\")\n",
    "    parser.add_argument(\"--patience\", default=0, type=int, help=\"The initial learning rate for Adam.\")\n",
    "    \n",
    "    parser.add_argument('-f')#########################\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    now = datetime.now()\n",
    "    args.model_dir = args.model_dir + '_' + now.strftime('%m-%d-%H:%M:%S')\n",
    "    args.model_name_or_path = MODEL_PATH_MAP[args.model_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'pro'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_and_cache_examples\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m dev_dataset \u001B[38;5;241m=\u001B[39m load_and_cache_examples(args, tokenizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdev\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m load_and_cache_examples(args, tokenizer, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/Experiment/data_loader.py:1027\u001B[0m, in \u001B[0;36mload_and_cache_examples\u001B[0;34m(args, tokenizer, mode)\u001B[0m\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;66;03m# Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\u001B[39;00m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;66;03m# Defaultly, pad id will be set to 0\u001B[39;00m\n\u001B[1;32m   1026\u001B[0m pad_token_label_id \u001B[38;5;241m=\u001B[39m args\u001B[38;5;241m.\u001B[39mignore_index\n\u001B[0;32m-> 1027\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpro\u001B[49m:\n\u001B[1;32m   1028\u001B[0m     features \u001B[38;5;241m=\u001B[39m convert_examples_to_features_multi_Pro(examples, args\u001B[38;5;241m.\u001B[39mmax_seq_len, tokenizer,\n\u001B[1;32m   1029\u001B[0m                                                   pad_token_label_id\u001B[38;5;241m=\u001B[39mpad_token_label_id)\n\u001B[1;32m   1030\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m args\u001B[38;5;241m.\u001B[39mmulti_intent:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Namespace' object has no attribute 'pro'"
     ]
    }
   ],
   "source": [
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "trainer = Trainer_multi(args, train_dataset, dev_dataset, test_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointbert",
   "language": "python",
   "name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
