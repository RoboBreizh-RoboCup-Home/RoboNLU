{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "308d8f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from transformers.modeling_bert import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "\n",
    "\n",
    "# from torchcrf import CRF\n",
    "from TorchCRF import CRF\n",
    "# from .module import IntentClassifier, SlotClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc3f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_intent_labels, dropout_rate=0.):\n",
    "        super(IntentClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class SlotClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_slot_labels, dropout_rate=0.):\n",
    "        super(SlotClassifier, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, num_slot_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "854e1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config, args, intent_label_lst, slot_label_lst):\n",
    "        super(JointBERT, self).__init__(config)\n",
    "        self.args = args\n",
    "        self.config = config\n",
    "        self.num_intent_labels = len(intent_label_lst)\n",
    "        self.num_slot_labels = len(slot_label_lst)\n",
    "            \n",
    "        self.bert = AutoModel.from_pretrained('JointBERT-snips',config=self.config,local_files_only=True)\n",
    "#         self.bert = BertModel(config=config)  # Load pretrained bert\n",
    "\n",
    "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
    "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, args.dropout_rate)\n",
    "\n",
    "        if args.use_crf:\n",
    "            self.crf = CRF(num_tags=self.num_slot_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids, slot_labels_ids):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = outputs[1]  # [CLS]\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        total_loss = 0\n",
    "        # 1. Intent Softmax\n",
    "        if intent_label_ids is not None:\n",
    "            if self.num_intent_labels == 1:\n",
    "                intent_loss_fct = nn.MSELoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1))\n",
    "            else:\n",
    "                intent_loss_fct = nn.CrossEntropyLoss()\n",
    "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\n",
    "            total_loss += intent_loss\n",
    "\n",
    "        # 2. Slot Softmax\n",
    "        if slot_labels_ids is not None:\n",
    "            if self.args.use_crf:\n",
    "                slot_loss = self.crf(slot_logits, slot_labels_ids, mask=attention_mask.byte(), reduction='mean')\n",
    "                slot_loss = -1 * slot_loss  # negative log-likelihood\n",
    "            else:\n",
    "                slot_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
    "                # Only keep active parts of the loss\n",
    "                if attention_mask is not None:\n",
    "                    active_loss = attention_mask.view(-1) == 1\n",
    "                    active_logits = slot_logits.view(-1, self.num_slot_labels)[active_loss]\n",
    "                    active_labels = slot_labels_ids.view(-1)[active_loss]\n",
    "                    slot_loss = slot_loss_fct(active_logits, active_labels)\n",
    "                else:\n",
    "                    slot_loss = slot_loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_labels_ids.view(-1))\n",
    "            total_loss += self.args.slot_loss_coef * slot_loss\n",
    "\n",
    "        outputs = ((intent_logits, slot_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        outputs = (total_loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1672897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "# from ipynb.fs.defs.data_loader import load_and_cache_examples,convert_examples_to_features,JointProcessor,InputFeatures,InputExample\n",
    "\n",
    "\n",
    "# import logging\n",
    "\n",
    "\n",
    "# def init_logger():\n",
    "#     logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#                         datefmt='%m/%d/%Y %H:%M:%S',\n",
    "#                         level=logging.INFO)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     init_logger()\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     # parser.add_argument(\"--task\", default=None, required=True, type=str, help=\"The name of the task to train\")\n",
    "#     # parser.add_argument(\"--model_dir\", default=None, required=True, type=str, help=\"Path to save, load model\")\n",
    "#     parser.add_argument(\"--data_dir\", default=\"./Dataset\", type=str, help=\"The input data dir\")\n",
    "#     parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "#     parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "\n",
    "#     #parser.add_argument(\"--model_type\", default=\"bert\", type=str, help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "\n",
    "#     parser.add_argument('--seed', type=int, default=1234, help=\"random seed for initialization\")\n",
    "#     parser.add_argument(\"--train_batch_size\", default=32, type=int, help=\"Batch size for training.\")\n",
    "#     parser.add_argument(\"--eval_batch_size\", default=64, type=int, help=\"Batch size for evaluation.\")\n",
    "#     parser.add_argument(\"--max_seq_len\", default=50, type=int, help=\"The maximum total input sequence length after tokenization.\")\n",
    "#     parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", default=10.0, type=float, help=\"Total number of training epochs to perform.\")\n",
    "#     parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "#     parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "#                         help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "#     parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "#     parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "#     parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "#     parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "#     parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
    "\n",
    "#     parser.add_argument('--logging_steps', type=int, default=200, help=\"Log every X updates steps.\")\n",
    "#     parser.add_argument('--save_steps', type=int, default=200, help=\"Save checkpoint every X updates steps.\")\n",
    "\n",
    "#     parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "#     parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the test set.\")\n",
    "#     parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "\n",
    "#     parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "#                         help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "    \n",
    "#     parser.add_argument(\"--model_name_or_path\", default =  'bert-base-uncased')\n",
    "#     parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
    "#     parser.add_argument('--slot_loss_coef', type=float, default=1.0, help='Coefficient for the slot loss.')\n",
    "    \n",
    "#     parser.add_argument('-f')\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     args.model_name_or_path = 'bert-base-uncased'\n",
    "    \n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "#     dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"val\")\n",
    "#     test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7f314f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from transformers import AutoConfig\n",
    "\n",
    "# def get_slot_labels():\n",
    "#     path = './Dataset/slot_label.txt'\n",
    "#     lst = [label.strip() for label in open(path, 'r', encoding='utf-8')]\n",
    "#     return lst\n",
    "\n",
    "# def get_intent_labels():\n",
    "#     path = './Dataset/intent_label.txt'\n",
    "#     lst = [label.strip() for label in open(path, 'r', encoding='utf-8')]\n",
    "\n",
    "#     return [lst[0] for i in range(len(lst))]\n",
    "\n",
    "# # with open('JointBERT-snips/config.json') as config_file:\n",
    "# #         config = json.load(config_file) \n",
    "            \n",
    "# config = AutoConfig.from_pretrained('JointBERT-snips/config.json')\n",
    "# JB = JointBERT(config=config,\n",
    "#                   args=args,\n",
    "#                   intent_label_lst=get_intent_labels(),\n",
    "#                   slot_label_lst=get_slot_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a70a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# import json\n",
    "# from tqdm import tqdm, trange\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "# from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "# from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca43e097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_slot_labels():\n",
    "#     path = './Dataset/slot_label.txt'\n",
    "#     lst = [label.strip() for label in open(path, 'r', encoding='utf-8')]\n",
    "#     return lst\n",
    "\n",
    "# def get_intent_labels():\n",
    "#     path = './Dataset/intent_label.txt'\n",
    "#     lst = [label.strip() for label in open(path, 'r', encoding='utf-8')]\n",
    "\n",
    "#     return [lst[0] for i in range(len(lst))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40fc6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer(object):\n",
    "#     def __init__(self, args, train_dataset=None, val_dataset=None, test_dataset=None):\n",
    "#         self.args = args\n",
    "#         self.train_dataset = train_dataset\n",
    "#         self.val_dataset = val_dataset\n",
    "#         self.test_dataset = test_dataset\n",
    "\n",
    "#         self.intent_label_lst = get_intent_labels()#!!!!!!!!!!!!!!!!!1\n",
    "#         self.slot_label_lst = get_slot_labels()#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        \n",
    "#         # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "#         self.pad_token_label_id = args.ignore_index\n",
    "        \n",
    "\n",
    "# #         self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n",
    "# #         self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
    "# #         self.model = self.model_class.from_pretrained(args.model_name_or_path,\n",
    "# #                                                       config=self.config,\n",
    "# #                                                       args=args,\n",
    "# #                                                       intent_label_lst=self.intent_label_lst,\n",
    "# #                                                       slot_label_lst=self.slot_label_lst)\n",
    "# #         with open('JointBERT-snips/config.json') as config_file:\n",
    "# #             self.config = json.load(config_file) \n",
    "#         self.config = AutoConfig.from_pretrained('JointBERT-snips/config.json',local_files_only=True)\n",
    "#         self.model = JointBERT(config=self.config,\n",
    "#                                   args=self.args,\n",
    "#                                   intent_label_lst=self.intent_label_lst,\n",
    "#                                   slot_label_lst=self.slot_label_lst)\n",
    "#         print(self.model)\n",
    "\n",
    "            \n",
    "# #         self.model = AutoModel.from_pretrained('JointBERT-snips',\n",
    "# #                                                       config=self.config,local_files_only=True)#,\n",
    "\n",
    "\n",
    "#         # GPU or CPU\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "#         self.model.to(self.device)\n",
    "\n",
    "#     def train(self):\n",
    "#         train_sampler = RandomSampler(self.train_dataset)\n",
    "#         train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n",
    "\n",
    "#         if self.args.max_steps > 0:\n",
    "#             t_total = self.args.max_steps\n",
    "#             self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
    "#         else:\n",
    "#             t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
    "\n",
    "#         # Prepare optimizer and schedule (linear warmup and decay)\n",
    "#         no_decay = ['bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#              'weight_decay': self.args.weight_decay},\n",
    "#             {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ]\n",
    "#         optimizer = AdamW(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
    "#         scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n",
    "\n",
    "#         # Train!\n",
    "# #         init_logger()\n",
    "# #         logger.info(\"***** Running training *****\")\n",
    "# #         logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
    "# #         logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
    "# #         logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
    "# #         logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
    "# #         logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "# #         logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
    "# #         logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
    "\n",
    "#         global_step = 0\n",
    "#         tr_loss = 0.0\n",
    "#         self.model.zero_grad()\n",
    "\n",
    "#         train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
    "\n",
    "#         for _ in train_iterator:\n",
    "#             epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "#             for step, batch in enumerate(epoch_iterator):\n",
    "#                 ####################################################################################################\n",
    "#                 self.model.train()\n",
    "#                 batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
    "\n",
    "#                 inputs = {'input_ids': batch[0],\n",
    "#                           'attention_mask': batch[1],\n",
    "#                           'intent_label_ids': batch[3],\n",
    "#                           'slot_labels_ids': batch[4]}\n",
    "#                 # if self.args.model_type != 'distilbert':\n",
    "#                 inputs['token_type_ids'] = batch[2]\n",
    "#                 outputs = self.model(**inputs)\n",
    "#                 loss = outputs[0]\n",
    "\n",
    "#                 if self.args.gradient_accumulation_steps > 1:\n",
    "#                     loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 tr_loss += loss.item()\n",
    "#                 if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
    "#                     torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
    "\n",
    "#                     optimizer.step()\n",
    "#                     scheduler.step()  # Update learning rate schedule\n",
    "#                     self.model.zero_grad()\n",
    "#                     global_step += 1\n",
    "\n",
    "#                     if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
    "#                         self.evaluate(\"val\")\n",
    "\n",
    "#                     if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
    "#                         self.save_model()\n",
    "\n",
    "#                 if 0 < self.args.max_steps < global_step:\n",
    "#                     epoch_iterator.close()\n",
    "#                     break\n",
    "\n",
    "#             if 0 < self.args.max_steps < global_step:\n",
    "#                 train_iterator.close()\n",
    "#                 break\n",
    "\n",
    "#         return global_step, tr_loss / global_step\n",
    "\n",
    "#     def evaluate(self, mode):\n",
    "#         if mode == 'test':\n",
    "#             dataset = self.test_dataset\n",
    "#         elif mode == 'val':\n",
    "#             dataset = self.val_dataset\n",
    "#         else:\n",
    "#             raise Exception(\"Only val and test dataset available\")\n",
    "\n",
    "#         eval_sampler = SequentialSampler(dataset)\n",
    "#         eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
    "\n",
    "#         # Eval!\n",
    "# #         logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "# #         logger.info(\"  Num examples = %d\", len(dataset))\n",
    "# #         logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
    "#         eval_loss = 0.0\n",
    "#         nb_eval_steps = 0\n",
    "# #         intent_preds = None\n",
    "#         slot_preds = None\n",
    "#         out_intent_label_ids = None\n",
    "#         out_slot_labels_ids = None\n",
    "\n",
    "#         self.model.eval()\n",
    "\n",
    "#         for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "#             batch = tuple(t.to(self.device) for t in batch)\n",
    "#             with torch.no_grad():\n",
    "#                 inputs = {'input_ids': batch[0],\n",
    "#                           'attention_mask': batch[1],\n",
    "#                           'intent_label_ids': batch[3],\n",
    "#                           'slot_labels_ids': batch[4]}\n",
    "#                 #if self.args.model_type != 'distilbert':\n",
    "#                 inputs['token_type_ids'] = batch[2]\n",
    "#                 outputs = self.model(**inputs)\n",
    "#                 tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
    "\n",
    "#                 eval_loss += tmp_eval_loss.mean().item()\n",
    "#             nb_eval_steps += 1\n",
    "\n",
    "#             # Intent prediction\n",
    "#             if intent_preds is None:\n",
    "#                 intent_preds = intent_logits.detach().cpu().numpy()\n",
    "#                 out_intent_label_ids = inputs['intent_label_ids'].detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
    "#                 out_intent_label_ids = np.append(\n",
    "#                     out_intent_label_ids, inputs['intent_label_ids'].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#             # Slot prediction\n",
    "#             if slot_preds is None:\n",
    "#                 if self.args.use_crf:\n",
    "#                     # decode() in `torchcrf` returns list with best index directly\n",
    "#                     slot_preds = np.array(self.model.crf.decode(slot_logits))\n",
    "#                 else:\n",
    "#                     slot_preds = slot_logits.detach().cpu().numpy()\n",
    "\n",
    "#                 out_slot_labels_ids = inputs[\"slot_labels_ids\"].detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 if self.args.use_crf:\n",
    "#                     slot_preds = np.append(slot_preds, np.array(self.model.crf.decode(slot_logits)), axis=0)\n",
    "#                 else:\n",
    "#                     slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#                 out_slot_labels_ids = np.append(out_slot_labels_ids, inputs[\"slot_labels_ids\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "#         eval_loss = eval_loss / nb_eval_steps\n",
    "#         results = {\n",
    "#             \"loss\": eval_loss\n",
    "#         }\n",
    "\n",
    "#         # Intent result\n",
    "#         intent_preds = np.argmax(intent_preds, axis=1)\n",
    "\n",
    "#         # Slot result\n",
    "#         if not self.args.use_crf:\n",
    "#             slot_preds = np.argmax(slot_preds, axis=2)\n",
    "#         slot_label_map = {i: label for i, label in enumerate(self.slot_label_lst)}\n",
    "#         out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "#         slot_preds_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "\n",
    "#         for i in range(out_slot_labels_ids.shape[0]):\n",
    "#             for j in range(out_slot_labels_ids.shape[1]):\n",
    "#                 if out_slot_labels_ids[i, j] != self.pad_token_label_id:\n",
    "#                     out_slot_label_list[i].append(slot_label_map[out_slot_labels_ids[i][j]])\n",
    "#                     slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
    "\n",
    "#         total_result = compute_metrics(intent_preds, out_intent_label_ids, slot_preds_list, out_slot_label_list)\n",
    "\n",
    "#         results.update(total_result)\n",
    "\n",
    "#         logger.info(\"***** Eval results *****\")\n",
    "#         for key in sorted(results.keys()):\n",
    "#             logger.info(\"  %s = %s\", key, str(results[key]))\n",
    "\n",
    "#         return results\n",
    "\n",
    "#     def save_model(self):\n",
    "#         # Save model checkpoint (Overwrite)\n",
    "#         if not os.path.exists(self.args.model_dir):\n",
    "#             os.makedirs(self.args.model_dir)\n",
    "#         model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "#         model_to_save.save_pretrained(self.args.model_dir)\n",
    "\n",
    "#         # Save training arguments together with the trained model\n",
    "#         torch.save(self.args, os.path.join(self.args.model_dir, 'training_args.bin'))\n",
    "#         logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
    "\n",
    "#     def load_model(self):\n",
    "#         # Check whether model exists\n",
    "#         if not os.path.exists(self.args.model_dir):\n",
    "#             raise Exception(\"Model doesn't exists! Train first!\")\n",
    "\n",
    "#         try:\n",
    "#             self.model = JointBERT(config=self.config,\n",
    "#                                   args=self.args,\n",
    "#                                   intent_label_lst=self.intent_label_lst,\n",
    "#                                   slot_label_lst=self.slot_label_lst)\n",
    "# #                                                           \n",
    "#             self.model.to(self.device)\n",
    "#             logger.info(\"***** Model Loaded *****\")\n",
    "#         except:\n",
    "#             raise Exception(\"Some model files might be missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d38bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "# trainer = Trainer(args, train_dataset, dev_dataset, test_dataset)\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
