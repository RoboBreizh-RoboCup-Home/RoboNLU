{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimize transformer-based models with onnxruntime-tools\n",
    "from onnx import version_converter, helper\n",
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "input_model = '/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/distil_bert/distil_bert.onnx'\n",
    "output_fp16 = '/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/distil_bert/distil_bert_fp16.onnx'\n",
    "#optimized_model = optimizer.optimize_model(input_model, model_type='bert', num_heads=12, hidden_size=768)\n",
    "\n",
    "model = onnx.load(input_model)\n",
    "\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "\n",
    "onnx.save(model_fp16, output_fp16)\n",
    "\n",
    "# A full list of supported adapters can be found here:\n",
    "# https://github.com/onnx/onnx/blob/main/onnx/version_converter.py#L21\n",
    "# Apply the version conversion on the original model\n",
    "# onnx.checker.check_model(model_fp16)\n",
    "\n",
    "# converted_model = version_converter.convert_version(model_fp16, 6)\n",
    "\n",
    "# onnx.save(converted_model, \"/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/bert/bert_fp16_v6.onnx\")\n",
    "\n",
    "# print(f\"The model after conversion:\\n{converted_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "input_model = '/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/mobile_bert/mobile_bert'\n",
    "output_fp16 = '/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/mobile_bert/mobile_bert.opt.onnx'\n",
    "quant_pre_process(input_model_path=input_model+'.onnx', output_model_path=input_model+\".preprocessed.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_196]\n",
      "Ignore MatMul due to non constant B: /[MatMul_201]\n",
      "Ignore MatMul due to non constant B: /[MatMul_266]\n",
      "Ignore MatMul due to non constant B: /[MatMul_271]\n",
      "Ignore MatMul due to non constant B: /[MatMul_336]\n",
      "Ignore MatMul due to non constant B: /[MatMul_341]\n",
      "Ignore MatMul due to non constant B: /[MatMul_406]\n",
      "Ignore MatMul due to non constant B: /[MatMul_411]\n",
      "Ignore MatMul due to non constant B: /[MatMul_476]\n",
      "Ignore MatMul due to non constant B: /[MatMul_481]\n",
      "Ignore MatMul due to non constant B: /[MatMul_546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_551]\n",
      "Ignore MatMul due to non constant B: /[MatMul_616]\n",
      "Ignore MatMul due to non constant B: /[MatMul_621]\n",
      "Ignore MatMul due to non constant B: /[MatMul_686]\n",
      "Ignore MatMul due to non constant B: /[MatMul_691]\n",
      "Ignore MatMul due to non constant B: /[MatMul_756]\n",
      "Ignore MatMul due to non constant B: /[MatMul_761]\n",
      "Ignore MatMul due to non constant B: /[MatMul_826]\n",
      "Ignore MatMul due to non constant B: /[MatMul_831]\n",
      "Ignore MatMul due to non constant B: /[MatMul_896]\n",
      "Ignore MatMul due to non constant B: /[MatMul_901]\n",
      "Ignore MatMul due to non constant B: /[MatMul_966]\n",
      "Ignore MatMul due to non constant B: /[MatMul_971]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1036]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1041]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1106]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1111]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1176]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1181]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1246]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1251]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1316]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1321]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1386]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1391]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1456]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1461]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1526]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1531]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1596]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1601]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1666]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1671]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1736]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1741]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1806]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1811]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = '/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/mobile_bert/mobile_bert.onnx'\n",
    "model_quant = '/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/mobile_bert/mobile_bert.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant, optimize_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "model = onnx.load(\"/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/mobile_bert/mobile_bert.onnx\")\n",
    "model_fp16 = float16.convert_float_to_float16(model)\n",
    "onnx.save(model_fp16, \"/home/maelic/Documents/robocup2023/NLP/nlp/checkpoints/mobile_bert/mobile_bert_fp16.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!onnx2tf -i mobile_bert/mobilebert.onnx -osd -rtpo Erf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir saved_model/ --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def convert_input_file_to_dataloader(words,model_name,\n",
    "                                        cls_token_segment_id=0,\n",
    "                                        pad_token_segment_id=0,\n",
    "                                        sequence_a_segment_id=0,\n",
    "                                        mask_padding_with_zero=True):\n",
    "    if model_name == 'bert':\n",
    "        tokenizer_name = 'bert-base-uncased'\n",
    "    elif model_name == 'mobile_bert':\n",
    "        tokenizer_name = 'google/mobilebert-uncased'\n",
    "    elif model_name == 'distil_bert':\n",
    "        tokenizer_name = 'distilbert-base-uncased'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    max_seq_len = 32\n",
    "    pad_token_label_id = 0\n",
    "\n",
    "    # Setting based on the current model type\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    tokens = []\n",
    "    slot_label_mask = []\n",
    "    pro_labels_ids = []\n",
    "    for word in words:\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        if not word_tokens:\n",
    "            word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "        else:\n",
    "            pro_label = 0\n",
    "        tokens.extend(word_tokens)\n",
    "        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "        slot_label_mask.extend(\n",
    "            [pad_token_label_id + 1] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        pro_labels_ids.extend(\n",
    "            [pro_label] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "    # Account for [CLS] and [SEP]\n",
    "    special_tokens_count = 2\n",
    "    if len(tokens) > max_seq_len - special_tokens_count:\n",
    "        tokens = tokens[: (max_seq_len - special_tokens_count)]\n",
    "        slot_label_mask = slot_label_mask[:(\n",
    "            max_seq_len - special_tokens_count)]\n",
    "        # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        pro_labels_ids = pro_labels_ids[:(\n",
    "            max_seq_len - special_tokens_count)]\n",
    "\n",
    "    # Add [SEP] token\n",
    "    tokens += [sep_token]\n",
    "    token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "    slot_label_mask += [pad_token_label_id]\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    pro_labels_ids += [pad_token_label_id]\n",
    "\n",
    "    # Add [CLS] token\n",
    "    tokens = [cls_token] + tokens\n",
    "    token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "    slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "    pro_labels_ids = [pad_token_label_id] + \\\n",
    "        pro_labels_ids  # !!!!!!!!!!!!!!!!!!!!!!!\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "    attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_len - len(input_ids)\n",
    "    input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "    attention_mask = attention_mask + \\\n",
    "        ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "    token_type_ids = token_type_ids + \\\n",
    "        ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "    input_ids = np.array(input_ids).astype('int64')\n",
    "    attention_mask = np.array(attention_mask).astype('int64')\n",
    "    token_type_ids = np.array(token_type_ids).astype('int64')\n",
    "    # sample = {'input_ids': input_ids[None, :], 'attention_mask': attention_mask[None,\n",
    "    #                                                                             :], 'token_type_ids': token_type_ids[None, :]}\n",
    "\n",
    "    if model_name == 'distil_bert':\n",
    "        return input_ids[None, :], attention_mask[None, :]\n",
    "        #sample = {'input_ids': input_ids[None, :], 'attention_mask': attention_mask[None,:]}\n",
    "\n",
    "    return input_ids[None, :], attention_mask[None, :], token_type_ids[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def representative_dataset():\n",
    "    # open seq.in file and read the first 100 lines\n",
    "    model_name = 'mobile_bert'\n",
    "    with open('/home/maelic/Documents/nlp/NLU_Benchmarking/gpsr_pro_instance_say/test/seq.in', 'r') as f:\n",
    "        lines = f.read().splitlines()[:100]\n",
    "\n",
    "    for line in lines:\n",
    "        if model_name == 'distil_bert':\n",
    "            input_ids, attention_masks = convert_input_file_to_dataloader(line, model_name)\n",
    "        else:\n",
    "            input_ids, attention_masks, token_type_ids = convert_input_file_to_dataloader(line, model_name)\n",
    "\n",
    "    # input_ids = np.ones([10, 32], dtype=np.int64)\n",
    "    # attention_masks = np.ones([10, 32], dtype=np.int64)\n",
    "    # token_type_ids = np.ones([10, 32], dtype=np.int64)\n",
    "\n",
    "    if model_name == 'distil_bert':\n",
    "        for attention_mask, input_id in zip(attention_masks, input_ids):\n",
    "            yield {\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"input_ids\": input_id,\n",
    "            }\n",
    "    else:\n",
    "\n",
    "        for attention_mask, input_id, token_type_id \\\n",
    "            in zip(attention_masks, input_ids, token_type_ids):\n",
    "\n",
    "            yield {\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"input_ids\": input_id,\n",
    "                \"token_type_ids\": token_type_id,\n",
    "            }\n",
    "\n",
    "model_name = 'mobile_bert'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open('saved_model/int8_model.tflite', 'wb') as w:\n",
    "    w.write(tflite_quant_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
