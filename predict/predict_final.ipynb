{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from utils import init_logger, load_tokenizer, get_intent_labels, get_slot_labels, MODEL_CLASSES, MODEL_PATH_MAP\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device(pred_config):\n",
    "    return \"cuda\" if torch.cuda.is_available() and not pred_config.no_cuda else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(pred_config):\n",
    "    return torch.load(os.path.join(pred_config.model_dir, 'training_args.pt'))\n",
    "    # return torch.load(os.path.join(pred_config.model_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(args, device):\n",
    "    # Check whether model exists\n",
    "    if not os.path.exists(pred_config.model_dir):\n",
    "        raise Exception(\"Model doesn't exists! Train first!\")\n",
    "    try:\n",
    "        intent_label_lst = get_intent_labels(args)\n",
    "        slot_label_lst=get_slot_labels(args)\n",
    "        model = MODEL_CLASSES[args.model_type][1].from_pretrained(args.model_dir,\n",
    "                                                                  intent_label_lst=intent_label_lst,\n",
    "                                                                  slot_label_lst=slot_label_lst)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(\"***** Model Loaded *****\")\n",
    "    except:\n",
    "        raise Exception(\"Some model files might be missing...\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_file(pred_config):\n",
    "    lines = []\n",
    "    with open(pred_config.input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            words = line.split()\n",
    "            lines.append(words)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input_file_to_tensor_dataset(lines,\n",
    "                                         pred_config,\n",
    "                                         tokenizer,\n",
    "                                         pad_token_label_id,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "\n",
    "    max_seq_len = 32\n",
    "    pro_lst = ['him','her','it','its']\n",
    "    # Setting based on the current model type\n",
    "    cls_token = tokenizer.cls_token\n",
    "    sep_token = tokenizer.sep_token\n",
    "    unk_token = tokenizer.unk_token\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_token_type_ids = []\n",
    "    all_slot_label_mask = []\n",
    "    all_pro_labels_ids = []\n",
    "    for words in lines:\n",
    "        tokens = []\n",
    "        slot_label_mask = []\n",
    "        pro_labels_ids = []\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            if word in pro_lst:\n",
    "                pro_label = 1\n",
    "            else:\n",
    "                pro_label = 0\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_label_mask.extend([pad_token_label_id + 1] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            pro_labels_ids.extend([pro_label] + [pad_token_label_id] * (len(word_tokens) - 1)) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_len - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(max_seq_len - special_tokens_count)]\n",
    "            pro_labels_ids = pro_labels_ids[:(max_seq_len - special_tokens_count)] #!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [pad_token_label_id]\n",
    "        pro_labels_ids += [pad_token_label_id]#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
    "        pro_labels_ids = [pad_token_label_id] + pro_labels_ids#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
    "        pro_labels_ids = pro_labels_ids + ([pad_token_label_id] * padding_length) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        all_input_ids.append(input_ids)\n",
    "        all_attention_mask.append(attention_mask)\n",
    "        all_token_type_ids.append(token_type_ids)\n",
    "        all_slot_label_mask.append(slot_label_mask)\n",
    "        all_pro_labels_ids.append(pro_labels_ids) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # print('padding_length: \\n',padding_length,'\\n')\n",
    "        # print('input_ids: \\n',input_ids,'\\n')\n",
    "        # print('slot_label_mask: \\n',slot_label_mask,'\\n')\n",
    "        # print(f'attention_mask: \\n{attention_mask}\\n')\n",
    "\n",
    "    # Change to Tensor\n",
    "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
    "    all_slot_label_mask = torch.tensor(all_slot_label_mask, dtype=torch.long)\n",
    "    all_pro_labels_ids = torch.tensor(all_pro_labels_ids, dtype=torch.long) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_slot_label_mask)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids,all_slot_label_mask, all_pro_labels_ids)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = get_device(pred_config)\n",
    "# model = load_model(pred_config, device)\n",
    "\n",
    "ignore_index = 0\n",
    "pad_token_label_id = ignore_index\n",
    "tokenizer = load_tokenizer(pred_config)\n",
    "lines = read_input_file(pred_config)\n",
    "dataset = convert_input_file_to_tensor_dataset(lines, pred_config, tokenizer, pad_token_label_id)\n",
    "batch = dataset[1]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def predict(pred_config):\n",
    "    args = pred_config\n",
    "    device = 'cpu'#get_device(pred_config)\n",
    "    model = load_model(pred_config, device)\n",
    "\n",
    "    intent_label_lst = get_intent_labels(pred_config)\n",
    "    slot_label_lst = get_slot_labels(pred_config)\n",
    "\n",
    "    ignore_index = 0\n",
    "    # Convert input file to TensorDataset\n",
    "    pad_token_label_id = ignore_index\n",
    "    tokenizer = load_tokenizer(args)\n",
    "    lines = read_input_file(pred_config)\n",
    "    dataset = convert_input_file_to_tensor_dataset(lines, pred_config, tokenizer, pad_token_label_id)\n",
    "\n",
    "    # Predict\n",
    "    sampler = SequentialSampler(dataset)\n",
    "    data_loader = DataLoader(dataset, sampler=sampler, batch_size=pred_config.batch_size)\n",
    "\n",
    "    intent_token_preds = None\n",
    "    all_referee_preds = None\n",
    "    slot_preds = None\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0],\n",
    "                      \"attention_mask\": batch[1],\n",
    "                      \"pro_labels_ids\": batch[4]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = batch[2]\n",
    "\n",
    "            print(inputs['input_ids'])\n",
    "\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            slot_logits, intent_token_logits, referee_token_logits,all_referee_token_logits = outputs\n",
    "\n",
    "\n",
    "             # ============================= Slot prediction ==============================\n",
    "            if slot_preds is None:\n",
    "                slot_preds = slot_logits.detach().cpu().numpy()\n",
    "                all_slot_label_mask = batch[3].detach().cpu().numpy()\n",
    "\n",
    "                #out_slot_labels_ids = slot_labels_ids.detach().cpu().numpy()\n",
    "            else:\n",
    "                slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
    "                all_slot_label_mask = np.append(all_slot_label_mask, batch[3].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "                #out_slot_labels_ids = np.append(out_slot_labels_ids, slot_labels_ids.detach().cpu().numpy(),axis=0)\n",
    "\n",
    "\n",
    "            # ============================= Pronoun referee prediction ==============================\n",
    "\n",
    "\n",
    "\n",
    "            if all_referee_preds is None:\n",
    "                all_referee_preds = all_referee_token_logits.detach().cpu().numpy()\n",
    "                referee_preds = referee_token_logits.detach().cpu().numpy()\n",
    "\n",
    "                pro_sample_mask_np = (torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] ==1 ).detach().cpu().numpy()\n",
    "\n",
    "                # all_out_referee_labels_ids = referee_labels_ids.detach().cpu().numpy()\n",
    "                # out_referee_labels_ids = all_out_referee_labels_ids[pro_sample_mask_np]\n",
    "\n",
    "\n",
    "            else:\n",
    "                all_referee_preds = np.append(all_referee_preds,all_referee_token_logits.detach().cpu().numpy(), axis = 0)\n",
    "                referee_preds = np.append(referee_preds, referee_token_logits.detach().cpu().numpy(), axis = 0)\n",
    "\n",
    "                # pro_sample_mask_np = (torch.max(inputs[\"pro_labels_ids\"],dim = 1)[0] == 1).detach().cpu().numpy()\n",
    "                # new_all_out_referee_labels_ids = referee_labels_ids.detach().cpu().numpy()\n",
    "                # all_out_referee_labels_ids = np.append(all_out_referee_labels_ids,new_all_out_referee_labels_ids,axis = 0)\n",
    "                # small_new_out_referee_labels_ids = new_all_out_referee_labels_ids[pro_sample_mask_np]\n",
    "                # out_referee_labels_ids = np.append(out_referee_labels_ids, small_new_out_referee_labels_ids, axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # ============================== Intent Token Seq =============================\n",
    "            if intent_token_preds is None:\n",
    "                intent_token_preds = intent_token_logits.detach().cpu().numpy()\n",
    "                # out_intent_token_ids = intent_token_ids.detach().cpu().numpy()\n",
    "            else:\n",
    "                intent_token_preds = np.append(intent_token_preds, intent_token_logits.detach().cpu().numpy(),axis=0)\n",
    "                # out_intent_token_ids = np.append(out_intent_token_ids,intent_token_ids.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "\n",
    "    slot_preds = np.argmax(slot_preds, axis=2)\n",
    "    slot_label_map = {i: label for i, label in enumerate(slot_label_lst)}\n",
    "    # out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
    "    slot_preds_list = [[] for _ in range(slot_preds.shape[0])]\n",
    "\n",
    "    # generate mask\n",
    "    for i in range(slot_preds.shape[0]):\n",
    "        for j in range(slot_preds.shape[1]):\n",
    "            if all_slot_label_mask[i, j] != pad_token_label_id:\n",
    "                # out_slot_label_list[i].append(slot_label_map[out_slot_labels_ids[i][j]])\n",
    "                slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
    "\n",
    "\n",
    "\n",
    "    referee_token_map = {0:'PAD', 1:'O' ,2: 'B-referee'} # All referee are just one word in EGPSR\n",
    "\n",
    "    referee_preds = np.argmax(referee_preds, axis=2)\n",
    "    all_referee_preds = np.argmax(all_referee_preds, axis=2)\n",
    "\n",
    "\n",
    "    referee_preds_list = [[] for _ in range(referee_preds.shape[0])]\n",
    "    all_referee_preds_list = [[] for _ in range(all_referee_preds.shape[0])]\n",
    "\n",
    "\n",
    "    # for i in range(referee_preds.shape[0]):\n",
    "    #     for j in range(referee_preds.shape[1]):\n",
    "    #         if all_slot_label_mask[i, j] != pad_token_label_id: #out_slot_labels_ids,out_referee_labels_ids\n",
    "    #             referee_preds_list[i].append(referee_token_map[referee_preds[i][j]])\n",
    "\n",
    "    for i in range(all_referee_preds.shape[0]):\n",
    "        for j in range(all_referee_preds.shape[1]):\n",
    "            if all_slot_label_mask[i, j] != pad_token_label_id: #all_out_referee_labels_ids\n",
    "                all_referee_preds_list[i].append(referee_token_map[all_referee_preds[i][j]])\n",
    "\n",
    "\n",
    "\n",
    "    # ============================= Intent Seq Prediction ============================\n",
    "    intent_token_map = {i: label for i, label in enumerate(intent_label_lst)}\n",
    "\n",
    "    intent_token_preds = np.argmax(intent_token_preds, axis=2)\n",
    "    # out_intent_token_list = [[] for _ in range(out_intent_token_ids.shape[0])]\n",
    "    intent_token_preds_list = [[] for _ in range(intent_token_preds.shape[0])]\n",
    "\n",
    "    for i in range(intent_token_preds.shape[0]):\n",
    "        for j in range(intent_token_preds.shape[1]):\n",
    "            if all_slot_label_mask[i, j] != pad_token_label_id:\n",
    "                # out_intent_token_list[i].append(intent_token_map[out_intent_token_ids[i][j]])\n",
    "                intent_token_preds_list[i].append(intent_token_map[intent_token_preds[i][j]])\n",
    "\n",
    "\n",
    "    # print('slot_preds_list: ',len(slot_preds_list),len(slot_preds_list[0]))\n",
    "    # print('intent_token_preds_list: ',len(intent_token_preds_list),len(intent_token_preds_list[0]))\n",
    "    # print('all_referee_preds_list: ',len(all_referee_preds_list),len(all_referee_preds_list[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(intent_token_map)\n",
    "    print(slot_label_map)\n",
    "\n",
    "    # Write to output file\n",
    "    pronouns = ['him','her','it','its']\n",
    "    with open(pred_config.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx,(words, slot_preds, intent_preds,referee_preds) in enumerate(zip(lines, slot_preds_list, intent_token_preds_list,all_referee_preds_list)):\n",
    "            if idx <= 10:\n",
    "                print('words:              ',words, len(words))\n",
    "                print('slot_preds:         ',slot_preds, len(slot_preds))\n",
    "                print('referee_preds:      ',referee_preds, len(referee_preds))\n",
    "                print('intent_preds:       ',intent_preds, len(intent_preds))\n",
    "\n",
    "            line = \"\"\n",
    "            if 'B-referee' not in referee_preds:#all([word not in pronouns for word in words]):\n",
    "                for word, i_pred, s_pred in zip(words, intent_preds, slot_preds):\n",
    "                    if s_pred == 'O' and i_pred == 'O':\n",
    "                        line = line + word + \" \"\n",
    "                    else:\n",
    "                        line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "                f.write(line.strip()+'\\n')\n",
    "            else:\n",
    "                r_idx = referee_preds.index('B-referee')\n",
    "                for word, i_pred, s_pred, r_pred in zip(words, intent_preds, slot_preds,referee_preds):\n",
    "                    if s_pred == 'O' and i_pred == 'O':\n",
    "                        line = line + word + \" \"\n",
    "                    else:\n",
    "                        if word not in pronouns:\n",
    "                            line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "                            if r_pred == 'B-referee':\n",
    "                                ref = word\n",
    "                        else:\n",
    "                            line = line + \"[{}:{}:{}:{}] \".format(word,words[r_idx], i_pred,s_pred)\n",
    "\n",
    "                f.write('\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n')\n",
    "                f.write('* Pro Case: \\n')\n",
    "                f.write(line.strip()+'\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n \\n')\n",
    "            print(line)\n",
    "            print('=====================================')\n",
    "\n",
    "\n",
    "    logger.info(\"Prediction Done!\")\n",
    "    return model, slot_preds_list, intent_token_preds_list, all_referee_preds_list,inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2424, 1996, 6207, 2006, 1996, 2795,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 52\u001B[0m\n\u001B[1;32m     48\u001B[0m pred_config\u001B[38;5;241m.\u001B[39mmodel_name_or_path \u001B[38;5;241m=\u001B[39m MODEL_PATH_MAP[pred_config\u001B[38;5;241m.\u001B[39mmodel_type]\n\u001B[1;32m     50\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m load_tokenizer(pred_config)\n\u001B[0;32m---> 52\u001B[0m model, slot_preds_list, intent_token_preds_list, all_referee_preds_list,pred_inputs \u001B[38;5;241m=\u001B[39m  \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[15], line 58\u001B[0m, in \u001B[0;36mpredict\u001B[0;34m(pred_config)\u001B[0m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m#out_slot_labels_ids = np.append(out_slot_labels_ids, slot_labels_ids.detach().cpu().numpy(),axis=0)\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \n\u001B[1;32m     52\u001B[0m \n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# ============================= Pronoun referee prediction ==============================\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m all_referee_preds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     all_referee_preds \u001B[38;5;241m=\u001B[39m \u001B[43mall_referee_token_logits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     59\u001B[0m     referee_preds \u001B[38;5;241m=\u001B[39m referee_token_logits\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     61\u001B[0m     pro_sample_mask_np \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mmax(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpro_labels_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m],dim \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m\u001B[38;5;241m1\u001B[39m )\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "from data_loader import load_and_cache_examples\n",
    "from transformers import BertModel\n",
    "# train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_file\", default=\"sample_pred_in.txt\", type=str, help=\"Input file for prediction\")\n",
    "\n",
    "    #\n",
    "    # parser.add_argument(\"--input_file\", default=\"data/gpsr_pro_instance/test/seq.in\", type=str, help=\"Input file for prediction\")\n",
    "\n",
    "    # OOV test\n",
    "    # parser.add_argument(\"--input_file\", default=\"data/gpsr_pro_instance_say_vocab/checked/seq.in\", type=str, help=\"Input file for prediction\")\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--task\", default='gpsr_pro_instance', type=str, help=\"The name of the task to train\")\n",
    "    parser.add_argument(\"--task\", default='gpsr_pro_instance_say', type=str, help=\"The name of the task to train\")\n",
    "\n",
    "    # parser.add_argument(\"--model_type\", default=\"multibert\", type=str,\n",
    "    #                     help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "\n",
    "    parser.add_argument(\"--model_type\", default=\"multibert\", type=str,\n",
    "                        help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "    parser.add_argument(\"--data_dir\", default=\"./data\", type=str, help=\"The input data dir\")\n",
    "    parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
    "    parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
    "    parser.add_argument(\"--intent_seq\", type=int, default=1, help=\"whether we use intent seq setting\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--pro\", type=int, default=1, help=\"support pronoun disambiguition\")#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--num_mask\", type=int, default=7, help=\"assumptive number of slot in one sentence\")\n",
    "    parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
    "                        help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
    "\n",
    "    parser.add_argument(\"--output_file\", default=\"final_predict.txt\", type=str, help=\"Output file for prediction\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"bert_based_model_04-07-14:03:07\", type=str, help=\"Path to save, load model\")\n",
    "    parser.add_argument(\"--max_seq_len\", default=32, type=int,\n",
    "                        help=\"The maximum total input sequence length after tokenization.\")\n",
    "    parser.add_argument(\"--batch_size\", default=128, type=int, help=\"Batch size for prediction\")\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument('-f')  #########################\n",
    "\n",
    "    pred_config = parser.parse_args()\n",
    "\n",
    "    pred_config.model_name_or_path = MODEL_PATH_MAP[pred_config.model_type]\n",
    "\n",
    "    tokenizer = load_tokenizer(pred_config)\n",
    "\n",
    "    model, slot_preds_list, intent_token_preds_list, all_referee_preds_list,pred_inputs =  predict(pred_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from utils import compute_metrics_final\n",
    "# def eval(slot_preds_list,intent_token_preds_list,all_referee_preds_list):\n",
    "#     out_slot_label_list = [line.strip().split() for line in open('./data/gpsr_pro_instance_say_vocab/checked/seq.out')]\n",
    "#     out_intent_token_list = [line.strip().split() for line in open('./data/gpsr_pro_instance_say_vocab/checked/seq_intent.out')]\n",
    "#     out_referee_label_list = [line.strip().split() for line in open('./data/gpsr_pro_instance_say_vocab/checked/seq_pro.out')]\n",
    "#     out_referee_label_list = [[ele if ele != 'referral' else 'O' for ele in lst] for lst in out_referee_label_list]\n",
    "#\n",
    "#     # change 'pad' yo 'O', because some cases do not have pro, so 'O' is 'PAD' in preds\n",
    "#     all_referee_preds_list = [[ele if ele != 'PAD' else 'O' for ele in lst] for lst in all_referee_preds_list]\n",
    "#\n",
    "#\n",
    "#     # remove none pro cases\n",
    "#     all_referee_preds_list = [all_referee_preds_list[i] for i in range(len(all_referee_preds_list)) if 'B-referee' in out_referee_label_list[i]]\n",
    "#     out_referee_label_list = [out_referee_label_list[i] for i in range(len(out_referee_label_list)) if 'B-referee' in out_referee_label_list[i]]\n",
    "#\n",
    "#     total_result = compute_metrics_final(\n",
    "#                                            slot_preds_list,\n",
    "#                                            out_slot_label_list,\n",
    "#                                            intent_token_preds_list,\n",
    "#                                            out_intent_token_list,\n",
    "#                                            all_referee_preds_list,\n",
    "#                                            out_referee_label_list\n",
    "#                                           )\n",
    "#\n",
    "#     com_lst = [line.strip().split() for line in open('./data/gpsr_pro_instance_say_vocab/checked/seq.in')]\n",
    "#     for i in range(len(slot_preds_list)):\n",
    "#         pred = slot_preds_list[i]\n",
    "#         lab = out_slot_label_list[i]\n",
    "#\n",
    "#         if pred != lab:\n",
    "#             print(i)\n",
    "#             print(com_lst[i])\n",
    "#             print(lab)\n",
    "#             print(pred)\n",
    "#             print('-------------------------------------')\n",
    "#\n",
    "#\n",
    "#     return total_result\n",
    "# eval(slot_preds_list,intent_token_preds_list,all_referee_preds_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def print_pro(all_referee_preds_list):\n",
    "#     out_referee_label_list = [line.strip().split() for line in open('./data/gpsr_pro_instance_say_vocab/checked/seq_pro.out')]\n",
    "#     out_referee_label_list = [[ele if ele != 'referral' else 'O' for ele in lst] for lst in out_referee_label_list]\n",
    "#     all_referee_preds_list = [[ele if ele != 'PAD' else 'O' for ele in lst] for lst in all_referee_preds_list]\n",
    "#     for i in range(len(out_referee_label_list)):\n",
    "#         lab = out_referee_label_list[i]\n",
    "#         pred = all_referee_preds_list[i]\n",
    "#         # if 'B-referee' in  lab:\n",
    "#         #     print(i)\n",
    "#         #     print(lab)\n",
    "#         #     print(pred)\n",
    "#         #     print('-------------------------------------')\n",
    "#\n",
    "#         print(i)\n",
    "#         print(lab)\n",
    "#         print(pred)\n",
    "#         print('-------------------------------------')\n",
    "# print_pro(all_referee_preds_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.modules of JointBERTMultiIntent(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (multi_intent_classifier): MultiIntentClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear): Linear(in_features=768, out_features=20, bias=True)\n    (sigmoid): Sigmoid()\n  )\n  (slot_classifier): SlotClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (activation): ReLU()\n    (linear): Linear(in_features=768, out_features=12, bias=True)\n  )\n  (intent_token_classifier): IntentTokenClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear): Linear(in_features=768, out_features=20, bias=True)\n  )\n  (pro_classifier): ProClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear1): Linear(in_features=1536, out_features=768, bias=True)\n    (linear2): Linear(in_features=768, out_features=3, bias=True)\n  )\n)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "\n",
    "\n",
    "# dummy_input = inputs\n",
    "# torch.onnx.export(model,               # model being run\n",
    "#                 dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "#                 \"normal_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "#                 export_params=True,        # store the trained parameter weights inside the model file\n",
    "#                 opset_version=11,          # the ONNX version to export the model to\n",
    "#                 do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "#                 input_names = ['input_ids','attention_mask','token_type_ids'],   # the model's input names\n",
    "#                 output_names = ['slot_logits', 'intent_token_logits', 'referee_token_logits','all_referee_token_logits'], ##['all_logits','other'],\n",
    "#                 )\n",
    "#\n",
    "# # ['slot_logits', 'intent_token_logits', 'referee_token_logits','all_referee_token_logits']\n",
    "#\n",
    "#\n",
    "# from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "#\n",
    "# model_fp32 = 'normal_resolution.onnx'\n",
    "# model_quant = 'normal_resolution.quant.onnx'\n",
    "# quantized_model = quantize_dynamic(model_fp32, model_quant)\n",
    "# # for quantization\n",
    "# # propagate through the model\n",
    "# # outputs = model(dummy_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[  101,  8957,  3531,  ...,     0,     0,     0],\n         [  101,  3531,  2425,  ...,     0,     0,     0],\n         [  101,  2071,  2017,  ...,     0,     0,     0],\n         ...,\n         [  101,  2404,  1037,  ...,     0,     0,     0],\n         [  101, 17021,  1996,  ...,     0,     0,     0],\n         [  101,  3288,  1996,  ...,     0,     0,     0]]),\n 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         ...,\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0],\n         [1, 1, 1,  ..., 0, 0, 0]]),\n 'pro_labels_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]]),\n 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         ...,\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0],\n         [0, 0, 0,  ..., 0, 0, 0]])}"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_81]\n",
      "Ignore MatMul due to non constant B: /[MatMul_86]\n",
      "Ignore MatMul due to non constant B: /[MatMul_139]\n",
      "Ignore MatMul due to non constant B: /[MatMul_144]\n",
      "Ignore MatMul due to non constant B: /[MatMul_197]\n",
      "Ignore MatMul due to non constant B: /[MatMul_202]\n",
      "Ignore MatMul due to non constant B: /[MatMul_255]\n",
      "Ignore MatMul due to non constant B: /[MatMul_260]\n",
      "Ignore MatMul due to non constant B: /[MatMul_313]\n",
      "Ignore MatMul due to non constant B: /[MatMul_318]\n",
      "Ignore MatMul due to non constant B: /[MatMul_371]\n",
      "Ignore MatMul due to non constant B: /[MatMul_376]\n",
      "Ignore MatMul due to non constant B: /[MatMul_429]\n",
      "Ignore MatMul due to non constant B: /[MatMul_434]\n",
      "Ignore MatMul due to non constant B: /[MatMul_487]\n",
      "Ignore MatMul due to non constant B: /[MatMul_492]\n",
      "Ignore MatMul due to non constant B: /[MatMul_545]\n",
      "Ignore MatMul due to non constant B: /[MatMul_550]\n",
      "Ignore MatMul due to non constant B: /[MatMul_603]\n",
      "Ignore MatMul due to non constant B: /[MatMul_608]\n",
      "Ignore MatMul due to non constant B: /[MatMul_661]\n",
      "Ignore MatMul due to non constant B: /[MatMul_666]\n",
      "Ignore MatMul due to non constant B: /[MatMul_719]\n",
      "Ignore MatMul due to non constant B: /[MatMul_724]\n"
     ]
    }
   ],
   "source": [
    "# inputs = {\"input_ids\": batch[0][None,:].to('cuda'),\n",
    "#           \"attention_mask\": batch[1][None,:].to('cuda'),\n",
    "#           'token_type_ids':batch[2][None,:].to('cuda')}\n",
    "\n",
    "\n",
    "inputs = {\"input_ids\": pred_inputs['input_ids'][0][None,:],\n",
    "          \"attention_mask\": pred_inputs['attention_mask'][1][None,:],\n",
    "          'token_type_ids':pred_inputs['token_type_ids'][2][None,:]}\n",
    "\n",
    "dummy_input = inputs\n",
    "torch.onnx.export(model.bert,               # model being run\n",
    "                dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                \"bert.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=11,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['input_ids','attention_mask','token_type_ids'],   # the model's input names\n",
    "                output_names = ['sequence_output','pooled_output'], ##['all_logits','other'],\n",
    "                )\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'bert.onnx'\n",
    "model_quant = './quantized_models/bert.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# out = model.bert(**inputs)\n",
    "# print(out[0].shape)\n",
    "# print(out[1].shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# slot_classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_input = out[0]\n",
    "torch.onnx.export(model.slot_classifier,               # model being run\n",
    "                dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                \"slot_classifier.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=11,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['sequence_output'],   # the model's input names\n",
    "                output_names = ['slot_logits'], ##['all_logits','other'],\n",
    "                )\n",
    "\n",
    "# ['slot_logits', 'intent_token_logits', 'referee_token_logits','all_referee_token_logits']\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'slot_classifier.onnx'\n",
    "model_quant = './quantized_models/slot_classifier.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# intent_token_classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_input = out[0]\n",
    "torch.onnx.export(model.intent_token_classifier,               # model being run\n",
    "                dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                \"intent_token_classifier.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=11,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['sequence_output'],   # the model's input names\n",
    "                output_names = ['intent_token_logits'], ##['all_logits','other'],\n",
    "                )\n",
    "\n",
    "# ['slot_logits', 'intent_token_logits', 'referee_token_logits','all_referee_token_logits']\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'intent_token_classifier.onnx'\n",
    "model_quant = './quantized_models/intent_token_classifier.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pro_classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inputs = {\"input_ids\": batch[0],\n",
    "#           \"attention_mask\": batch[1],\n",
    "#           \"pro_labels_ids\": batch[4],\n",
    "#           'token_type_ids':batch[2]}\n",
    "\n",
    "pro_labels_ids = batch[4][None,:]\n",
    "\n",
    "sequence_output = out[0]\n",
    "if 1 in pro_labels_ids:  # if use pro, and the batch contain pronouns\n",
    "    # 1. concate pronoun to each word in the sequence\n",
    "    pro_token_mask = pro_labels_ids > 0\n",
    "    pro_sample_mask = torch.max(pro_token_mask.long(),dim = 1)[0] > 0\n",
    "    print(pro_sample_mask)\n",
    "    pro_vec = sequence_output[pro_token_mask]\n",
    "    pro_sequence_output = sequence_output[pro_sample_mask]\n",
    "    pro_vec = pro_vec[:, None, :]  # add new dimention\n",
    "    repeat_pro = pro_vec.repeat(1, 32, 1)#self.args.max_seq_len\n",
    "    concated_input = torch.cat((pro_sequence_output, repeat_pro), dim=2)\n",
    "\n",
    "\n",
    "print(concated_input.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dummy_input = concated_input\n",
    "torch.onnx.export(model.pro_classifier,               # model being run\n",
    "                dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                \"pro_classifier.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=11,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['concated_input'],   # the model's input names\n",
    "                output_names = ['referee_token_logits'], ##['all_logits','other'],\n",
    "                )\n",
    "\n",
    "# ['slot_logits', 'intent_token_logits', 'referee_token_logits','all_referee_token_logits']\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'pro_classifier.onnx'\n",
    "model_quant = './quantized_models/pro_classifier.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Try Onnx Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import time\n",
    "\n",
    "def initONNX(path):\n",
    "    start = time.time()\n",
    "    sess_options = onnxruntime.SessionOptions()\n",
    "    #sess_options.enable_profiling = True\n",
    "\n",
    "    sess_options.intra_op_num_threads = 1#4\n",
    "    sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL\n",
    "    sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    ort_session  = onnxruntime.InferenceSession(path, sess_options)\n",
    "    print(\"Loading time ONNX: \", time.time() - start)\n",
    "    return ort_session\n",
    "\n",
    "\n",
    "\n",
    "bert_ort_session = initONNX('./quantized_models/bert.quant.onnx')\n",
    "slot_classifier_ort_session = initONNX('./quantized_models/slot_classifier.quant.onnx')\n",
    "intent_token_classifier_ort_session = initONNX('./quantized_models/intent_token_classifier.quant.onnx')\n",
    "pro_classifier_ort_session = initONNX('./quantized_models/pro_classifier.quant.onnx')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## bert"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_inputs = {\"input_ids\": np.array(batch[0][None,:]),\n",
    "            \"attention_mask\": np.array(batch[1][None,:]),\n",
    "             \"token_type_ids\": np.array(batch[2][None,:])}\n",
    "sequence_output,pooled_output = bert_ort_session.run(None, bert_inputs)\n",
    "sequence_output.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## intent token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intent_token_logits = slot_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "print(len(intent_token_logits),len(intent_token_logits[0]),len(intent_token_logits[0][0]),len(intent_token_logits[0][0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## slot token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "slot_logits = intent_token_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "print(len(slot_logits),len(slot_logits[0]),len(slot_logits[0][0]),len(slot_logits[0][0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## pro"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pro_labels_ids = batch[4][None,:]\n",
    "\n",
    "sequence_output = out[0]\n",
    "if 1 in pro_labels_ids:  # if use pro, and the batch contain pronouns\n",
    "    # 1. concate pronoun to each word in the sequence\n",
    "    pro_token_mask = pro_labels_ids > 0\n",
    "    pro_sample_mask = torch.max(pro_token_mask.long(),dim = 1)[0] > 0\n",
    "    print(pro_sample_mask)\n",
    "    pro_vec = sequence_output[pro_token_mask]\n",
    "    pro_sequence_output = sequence_output[pro_sample_mask]\n",
    "    pro_vec = pro_vec[:, None, :]  # add new dimention\n",
    "    repeat_pro = pro_vec.repeat(1, 32, 1)#self.args.max_seq_len\n",
    "    concated_input = torch.cat((pro_sequence_output, repeat_pro), dim=2)\n",
    "\n",
    "concated_input = concated_input.detach().numpy()\n",
    "print(concated_input.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "referee_token_logits = pro_classifier_ort_session.run(None, {'concated_input':concated_input})\n",
    "print(len(referee_token_logits),len(referee_token_logits[0]),len(referee_token_logits[0][0]),len(referee_token_logits[0][0][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(np.array(referee_token_logits).shape)\n",
    "print(np.squeeze(np.array(referee_token_logits)).shape)\n",
    "a = np.squeeze(np.array(referee_token_logits))\n",
    "np.argmax(a,axis =1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mobile BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crossing/miniconda3/envs/jointbert/lib/python3.9/site-packages/transformers/models/mobilebert/modeling_mobilebert.py:548: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(1000),\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n",
      "Warning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_196]\n",
      "Ignore MatMul due to non constant B: /[MatMul_201]\n",
      "Ignore MatMul due to non constant B: /[MatMul_266]\n",
      "Ignore MatMul due to non constant B: /[MatMul_271]\n",
      "Ignore MatMul due to non constant B: /[MatMul_336]\n",
      "Ignore MatMul due to non constant B: /[MatMul_341]\n",
      "Ignore MatMul due to non constant B: /[MatMul_406]\n",
      "Ignore MatMul due to non constant B: /[MatMul_411]\n",
      "Ignore MatMul due to non constant B: /[MatMul_476]\n",
      "Ignore MatMul due to non constant B: /[MatMul_481]\n",
      "Ignore MatMul due to non constant B: /[MatMul_546]\n",
      "Ignore MatMul due to non constant B: /[MatMul_551]\n",
      "Ignore MatMul due to non constant B: /[MatMul_616]\n",
      "Ignore MatMul due to non constant B: /[MatMul_621]\n",
      "Ignore MatMul due to non constant B: /[MatMul_686]\n",
      "Ignore MatMul due to non constant B: /[MatMul_691]\n",
      "Ignore MatMul due to non constant B: /[MatMul_756]\n",
      "Ignore MatMul due to non constant B: /[MatMul_761]\n",
      "Ignore MatMul due to non constant B: /[MatMul_826]\n",
      "Ignore MatMul due to non constant B: /[MatMul_831]\n",
      "Ignore MatMul due to non constant B: /[MatMul_896]\n",
      "Ignore MatMul due to non constant B: /[MatMul_901]\n",
      "Ignore MatMul due to non constant B: /[MatMul_966]\n",
      "Ignore MatMul due to non constant B: /[MatMul_971]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1036]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1041]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1106]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1111]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1176]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1181]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1246]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1251]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1316]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1321]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1386]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1391]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1456]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1461]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1526]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1531]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1596]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1601]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1666]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1671]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1736]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1741]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1806]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1811]\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input_ids\": batch[0][None,:],\n",
    "          \"attention_mask\": batch[1][None,:],\n",
    "          'token_type_ids':batch[2][None,:]}\n",
    "\n",
    "dummy_input = inputs\n",
    "torch.onnx.export(model.mobilebert,               # model being run\n",
    "                dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                \"mobile_bert.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=11,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['input_ids','attention_mask','token_type_ids'],   # the model's input names\n",
    "                output_names = ['sequence_output','pooled_output'], ##['all_logits','other'],\n",
    "                )\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'mobile_bert.onnx'\n",
    "model_quant = './quantized_models/mobile_bert.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DistilBERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_56]\n",
      "Ignore MatMul due to non constant B: /[MatMul_66]\n",
      "Ignore MatMul due to non constant B: /[MatMul_121]\n",
      "Ignore MatMul due to non constant B: /[MatMul_131]\n",
      "Ignore MatMul due to non constant B: /[MatMul_186]\n",
      "Ignore MatMul due to non constant B: /[MatMul_196]\n",
      "Ignore MatMul due to non constant B: /[MatMul_251]\n",
      "Ignore MatMul due to non constant B: /[MatMul_261]\n",
      "Ignore MatMul due to non constant B: /[MatMul_316]\n",
      "Ignore MatMul due to non constant B: /[MatMul_326]\n",
      "Ignore MatMul due to non constant B: /[MatMul_381]\n",
      "Ignore MatMul due to non constant B: /[MatMul_391]\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"input_ids\": batch[0][None,:],\n",
    "          \"attention_mask\": batch[1][None,:]}\n",
    "dummy_input = inputs\n",
    "torch.onnx.export(model.distilbert,               # model being run\n",
    "                dummy_input,               # model input (or a tuple for multiple inputs)\n",
    "                \"distil_bert.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=11,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['input_ids','attention_mask'],   # the model's input names\n",
    "                output_names = ['sequence_output'], ##['all_logits','other'],\n",
    "                )\n",
    "\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_fp32 = 'distil_bert.onnx'\n",
    "model_quant = './quantized_models/distil_bert.quant.onnx'\n",
    "quantized_model = quantize_dynamic(model_fp32, model_quant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ! pip install onnx\n",
    "# ! pip install onnxruntime\n",
    "import onnx\n",
    "model = onnx.load('./quantized_models/bert.quant.onnx')\n",
    "output =[node.name for node in model.graph.output]\n",
    "\n",
    "input_all = [node.name for node in model.graph.input]\n",
    "input_initializer =  [node.name for node in model.graph.initializer]\n",
    "net_feed_input = list(set(input_all)  - set(input_initializer))\n",
    "\n",
    "print('Inputs: ', net_feed_input)\n",
    "print('Outputs: ', output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.graph.input\n",
    "model.graph.output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numpy Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "JointBERTMultiIntent(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (multi_intent_classifier): MultiIntentClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear): Linear(in_features=768, out_features=20, bias=True)\n    (sigmoid): Sigmoid()\n  )\n  (slot_classifier): SlotClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (activation): ReLU()\n    (linear): Linear(in_features=768, out_features=12, bias=True)\n  )\n  (intent_token_classifier): IntentTokenClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear): Linear(in_features=768, out_features=20, bias=True)\n  )\n  (pro_classifier): ProClassifier(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (linear1): Linear(in_features=1536, out_features=768, bias=True)\n    (linear2): Linear(in_features=768, out_features=3, bias=True)\n  )\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.slot_classifier.linear.weight.shape\n",
    "model.slot_classifier._modules['dropout'].p\n",
    "model.slot_classifier._modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnx\n",
    "from numpy import save\n",
    "from numpy import load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "\n",
    "model_type = 'bert'\n",
    "\n",
    "slot_classifier_w = model.slot_classifier._modules['linear'].weight.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/slot_classifier/weights.npy', slot_classifier_w)\n",
    "slot_classifier_b = model.slot_classifier._modules['linear'].bias.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/slot_classifier/bias.npy', slot_classifier_b)\n",
    "\n",
    "\n",
    "intent_token_classifier_w = model.intent_token_classifier._modules['linear'].weight.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/intent_token_classifier/weights.npy', intent_token_classifier_w)\n",
    "intent_token_classifier_b = model.intent_token_classifier._modules['linear'].bias.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/intent_token_classifier/bias.npy', intent_token_classifier_b)\n",
    "\n",
    "\n",
    "\n",
    "pro_classifier_w1 = model.pro_classifier._modules['linear1'].weight.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/pro_classifier/weights1.npy', pro_classifier_w1)\n",
    "pro_classifier_b1 = model.pro_classifier._modules['linear1'].bias.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/pro_classifier/bias1.npy', pro_classifier_b1)\n",
    "\n",
    "pro_classifier_w2 = model.pro_classifier._modules['linear2'].weight.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/pro_classifier/weights2.npy', pro_classifier_w2)\n",
    "pro_classifier_b2 = model.pro_classifier._modules['linear2'].bias.detach().numpy()\n",
    "save(f'./numpy_para/{model_type}/pro_classifier/bias2.npy', pro_classifier_b2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.slot_classifier_np at 0x7fc09fccbdf0>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class slot_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights = load(dir+'/weights.npy')\n",
    "        self.linear_bias = load(dir+'/bias.npy')\n",
    "    # def ReLU(self,x):\n",
    "    #     return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        # x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights) + self.linear_bias\n",
    "        return x\n",
    "\n",
    "s =  slot_classifier_np(f'./numpy_para/{model_type}/slot_classifier')\n",
    "s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.intent_token_classifier_np at 0x7f48c6d260d0>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class intent_token_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights = load(dir + '/weights.npy')\n",
    "        self.linear_bias = load(dir + '/bias.npy')\n",
    "\n",
    "    # def ReLU(self,x):\n",
    "    #     return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        # x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights) + self.linear_bias\n",
    "        return x\n",
    "\n",
    "i =  intent_token_classifier_np('./numpy_para/intent_token_classifier')\n",
    "i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.pro_classifier_np at 0x7f48c65a11f0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class pro_classifier_np():\n",
    "    def __init__(self,dir):\n",
    "        self.linear_weights1 = load(dir + '/weights1.npy')\n",
    "        self.linear_bias1 = load(dir + '/bias1.npy')\n",
    "        self.linear_weights2 = load(dir + '/weights2.npy')\n",
    "        self.linear_bias2 = load(dir + '/bias2.npy')\n",
    "    def ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "    def forward(self,x):\n",
    "        x = np.squeeze(x)\n",
    "        x = x @ np.transpose(self.linear_weights1) + self.linear_bias1\n",
    "        x = self.ReLU(x)\n",
    "        x = x @ np.transpose(self.linear_weights2) + self.linear_bias2\n",
    "        return x\n",
    "\n",
    "p =  pro_classifier_np('./numpy_para/pro_classifier')\n",
    "p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class CommandProcessor():\n",
    "    def __init__(self,session = None):\n",
    "        # self.INTENT_CLASSES = ['PAD','O','B-greet','I-greet','B-guide','I-guide','B-follow','I-follow','B-find','I-find','B-take','I-take','B-go','I-go','B-know','I-know']\n",
    "        # self.SLOT_CLASSES = ['PAD','O','B-obj','B-dest','I-sour','I-obj','I-dest','B-per','B-sour','I-per']\n",
    "\n",
    "        self.INTENT_CLASSES = ['PAD','O', 'B-greet', 'I-greet', 'B-know', 'I-know', 'B-follow', 'I-follow', 'B-take', 'I-take', 'B-tell', 'I-tell', 'B-guide', 'I-guide', 'B-go', 'I-go', 'B-answer', 'I-answer', 'B-find','I-find']\n",
    "        self.SLOT_CLASSES = ['PAD', 'O','I-obj', 'B-sour', 'B-dest','I-sour','B-what','B-obj','I-dest','I-per', 'I-what', 'B-per']\n",
    "\n",
    "        self.PRO_CLASSES = ['PAD','O','B-referee']\n",
    "\n",
    "        self.referee_token_map = {i:label for i,label in enumerate(self.PRO_CLASSES)}\n",
    "        self.intent_token_map = {i:label for i,label in enumerate(self.INTENT_CLASSES)}\n",
    "        self.slot_label_map = {i:label for i,label in enumerate(self.SLOT_CLASSES)}\n",
    "\n",
    "        self.max_seq_len = 32\n",
    "        self.pro_lst = ['him','her','it','its']\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.pad_token_label_id = 0\n",
    "\n",
    "        self.input_text_path = './sample_pred_in.txt'#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        self.output_file = './outputs'\n",
    "\n",
    "        self.bert_ort_session = self.initONNX('./quantized_models/bert.quant.onnx')\n",
    "        # self.slot_classifier_ort_session = self.initONNX('./quantized_models/slot_classifier.quant.onnx')\n",
    "        # self.intent_token_classifier_ort_session = self.initONNX('./quantized_models/intent_token_classifier.quant.onnx')\n",
    "        # self.pro_classifier_ort_session = self.initONNX('./quantized_models/pro_classifier.quant.onnx')\n",
    "\n",
    "    def initONNX(self,path):\n",
    "        start = time.time()\n",
    "        sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "        sess_options.intra_op_num_threads = 1#4\n",
    "        sess_options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL\n",
    "        sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "        ort_session  = onnxruntime.InferenceSession(path, sess_options)\n",
    "        print(\"Loading time ONNX: \", time.time() - start)\n",
    "        return ort_session\n",
    "\n",
    "    def read_input_file(self):\n",
    "        with open(self.input_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words = f.readline().strip().split()\n",
    "            # for line in f:\n",
    "            #     line = line.strip()\n",
    "            #     words = line.split()\n",
    "            #     break # I should delete precessed commands!!!!!!!!!!!!!\n",
    "\n",
    "        return words\n",
    "\n",
    "    def convert_input_file_to_dataloader(self,words,\n",
    "                                         cls_token_segment_id=0,\n",
    "                                         pad_token_segment_id=0,\n",
    "                                         sequence_a_segment_id=0,\n",
    "                                         mask_padding_with_zero=True):\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        # Setting based on the current model type\n",
    "        cls_token = tokenizer.cls_token\n",
    "        sep_token = tokenizer.sep_token\n",
    "        unk_token = tokenizer.unk_token\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "        tokens = []\n",
    "        slot_label_mask = []\n",
    "        pro_labels_ids = []\n",
    "        for word in words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
    "            if word in self.pro_lst:\n",
    "                pro_label = 1\n",
    "            else:\n",
    "                pro_label = 0\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            slot_label_mask.extend([self.pad_token_label_id + 1] + [self.pad_token_label_id] * (len(word_tokens) - 1))\n",
    "            pro_labels_ids.extend([pro_label] + [self.pad_token_label_id] * (len(word_tokens) - 1)) #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "        # Account for [CLS] and [SEP]\n",
    "        special_tokens_count = 2\n",
    "        if len(tokens) > self.max_seq_len - special_tokens_count:\n",
    "            tokens = tokens[: (self.max_seq_len - special_tokens_count)]\n",
    "            slot_label_mask = slot_label_mask[:(self.max_seq_len - special_tokens_count)]\n",
    "            pro_labels_ids = pro_labels_ids[:(self.max_seq_len - special_tokens_count)] #!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [SEP] token\n",
    "        tokens += [sep_token]\n",
    "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        slot_label_mask += [self.pad_token_label_id]\n",
    "        pro_labels_ids += [self.pad_token_label_id]#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "        # Add [CLS] token\n",
    "        tokens = [cls_token] + tokens\n",
    "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
    "        slot_label_mask = [self.pad_token_label_id] + slot_label_mask\n",
    "        pro_labels_ids = [self.pad_token_label_id] + pro_labels_ids#!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = self.max_seq_len - len(input_ids)\n",
    "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
    "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
    "        slot_label_mask = slot_label_mask + ([self.pad_token_label_id] * padding_length)\n",
    "        pro_labels_ids = pro_labels_ids + ([self.pad_token_label_id] * padding_length)\n",
    "\n",
    "\n",
    "        input_ids = np.array(input_ids)\n",
    "        attention_mask = np.array(attention_mask)\n",
    "        token_type_ids = np.array(token_type_ids)\n",
    "        pro_labels_ids = np.array(pro_labels_ids)\n",
    "        # print('input_ids shape: ',input_ids.shape)\n",
    "        # print('attention_mask shape: ',attention_mask.shape)\n",
    "        # print('token_type_ids shape: ',token_type_ids.shape)\n",
    "        # print('pro_labels_ids shape: ',pro_labels_ids.shape)\n",
    "        sample = {'input_ids':input_ids[None,:], 'attention_mask':attention_mask[None,:], 'token_type_ids': token_type_ids[None,:]}\n",
    "\n",
    "        return sample,slot_label_mask,pro_labels_ids\n",
    "\n",
    "    def predict(self):\n",
    "        lines = self.read_input_file()\n",
    "        sample,slot_label_mask,pro_labels_ids = self.convert_input_file_to_dataloader(lines)\n",
    "        start = time.time()\n",
    "\n",
    "        sequence_output, _ = self.bert_ort_session.run(None, sample)\n",
    "        # ============================= Slot prediction ==============================\n",
    "        slot_classifier = slot_classifier_np('./numpy_para/slot_classifier')\n",
    "        slot_logits = slot_classifier.forward(sequence_output)\n",
    "        # slot_logits = self.slot_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "        slot_preds = np.squeeze(np.array(slot_logits))\n",
    "        slot_preds = np.argmax(slot_preds, axis=1)\n",
    "\n",
    "        # ============================== Intent Token Seq =============================\n",
    "        intent_token_classifier = intent_token_classifier_np('./numpy_para/intent_token_classifier')\n",
    "        intent_token_logits = intent_token_classifier.forward(sequence_output)\n",
    "        # intent_token_logits = self.intent_token_classifier_ort_session.run(None, {'sequence_output':sequence_output})\n",
    "        intent_token_preds = np.squeeze(np.array(intent_token_logits))\n",
    "        intent_token_preds = np.argmax(intent_token_preds, axis=1)\n",
    "\n",
    "        # ============================= Pronoun referee prediction ==============================\n",
    "        if any(pro_labels_ids):\n",
    "            sq_sequence_output = np.squeeze(sequence_output)\n",
    "            pro_token = sq_sequence_output[pro_labels_ids == 1]\n",
    "            # gpsr has 2 pronouns referred to the same referral, we only need to encode one pronoun\n",
    "            if pro_token.shape[0] != 1:\n",
    "                pro_token = pro_token[0, :]\n",
    "            repeat_pro = np.tile(pro_token,(self.max_seq_len,1))\n",
    "            concated_input = np.concatenate((sq_sequence_output,repeat_pro),axis = 1)[None,:]\n",
    "\n",
    "            pro_classifier = pro_classifier_np('./numpy_para/pro_classifier')\n",
    "            referee_token_logits = pro_classifier.forward(concated_input)\n",
    "            # referee_token_logits = self.pro_classifier_ort_session.run(None, {'concated_input':concated_input})\n",
    "            referee_preds = np.squeeze(np.array(referee_token_logits))\n",
    "            referee_preds = np.argmax(referee_preds, axis=1)\n",
    "\n",
    "        else:\n",
    "            referee_preds = np.ones(self.max_seq_len)\n",
    "\n",
    "\n",
    "        print('------------------------------------------------------')\n",
    "        print(\"Total inference time: \", time.time() - start)\n",
    "\n",
    "\n",
    "        slot_preds_list = []\n",
    "        intent_token_preds_list = []\n",
    "        referee_preds_list = []\n",
    "\n",
    "        for token_idx in range(len(slot_label_mask)):\n",
    "            if slot_label_mask[token_idx] != self.pad_token_label_id:\n",
    "                referee_preds_list.append(self.referee_token_map[referee_preds[token_idx]])\n",
    "                intent_token_preds_list.append(self.intent_token_map[intent_token_preds[token_idx]])\n",
    "                slot_preds_list.append(self.slot_label_map[slot_preds[token_idx]])\n",
    "\n",
    "\n",
    "        self.write_readable_outputs(slot_preds_list,intent_token_preds_list,referee_preds_list)\n",
    "\n",
    "        return\n",
    "\n",
    "    def write_readable_outputs(self,slot_preds_list,intent_token_preds_list,referee_preds_list):\n",
    "        words = self.read_input_file()\n",
    "        line = ''\n",
    "        for token_idx,(word, i_pred, s_pred, r_pred) in enumerate(zip(words, intent_token_preds_list, slot_preds_list,referee_preds_list)):\n",
    "            if s_pred == 'O' and i_pred == 'O' and r_pred == 'O':\n",
    "                line = line + word + \" \"\n",
    "            elif i_pred != 'O':\n",
    "                if word not in self.pro_lst:\n",
    "                    line = line + \"[{}:{}:{}] \".format(word, i_pred,s_pred)\n",
    "\n",
    "                    if r_pred == 'B-referee':\n",
    "                        r_idx = token_idx\n",
    "\n",
    "                else:\n",
    "                    line = line + \"[{}({}):{}:{}] \".format(word,words[r_idx], i_pred,s_pred)\n",
    "\n",
    "        with open(self.output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            if 'B-referee' in referee_preds_list:\n",
    "                f.write('\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n')\n",
    "                f.write('* Pro Case: \\n')\n",
    "                f.write(line.strip()+'\\n')\n",
    "                f.write('---------------------------------------------------------------------\\n \\n')\n",
    "            else:\n",
    "                f.write(line.strip()+'\\n')\n",
    "            print(line)\n",
    "            print('=====================================')\n",
    "        return\n",
    "\n",
    "inference = CommandProcessor()\n",
    "inference.predict()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jointbert",
   "language": "python",
   "display_name": "jointbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
